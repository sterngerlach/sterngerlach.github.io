<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="SternGerlach" />
  <title>ç‚¹ç¾¤å‡¦ç†ã®FPGAé«˜é€ŸåŒ–</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">ç‚¹ç¾¤å‡¦ç†ã®FPGAé«˜é€ŸåŒ–</h1>
<p class="author">SternGerlach</p>
</header>
<!--
 pandoc -s -f markdown -t html5 --mathjax --css style.css point-cloud-classification.md -o point-cloud-classification.html
-->
<p><a href="./index.html">ãƒ›ãƒ¼ãƒ ã«æˆ»ã‚‹</a></p>
<h1 id="ã“ã®ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦">ã“ã®ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦</h1>
<p>ã“ã®ãƒšãƒ¼ã‚¸ã¯ã€<a
href="https://adventar.org/calendars/7773">æ…¶æ‡‰ç†å·¥ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼2022</a>ã®22æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚
å»å¹´ã®è¨˜äº‹ã¯<a
href="./scan-matching-branch-and-bound.html">ã“ã¡ã‚‰</a>ã¨<a
href="./scan-matching-branch-and-bound-impl.html">ã“ã¡ã‚‰</a>ã§ã™ã€‚</p>
<p>æ—©é€Ÿä½™è«‡ã§ã™ãŒã€1983å¹´12æœˆ22æ—¥ã¯ã€Yellow Magic Orchestra (YMO)
ãŒè¡Œã£ãŸæœ€å¾Œã®å›½å†…ãƒ„ã‚¢ãƒ¼ã®æœ€çµ‚æ—¥ã§ã€é–‹å‚¬å ´æ‰€ã¯æ—¥æœ¬æ­¦é“é¤¨ã§ã—ãŸã€‚
ä»Šæ—¥ã¯ã€ãã®æ•£é–‹ãƒ„ã‚¢ãƒ¼ã‹ã‚‰ã¡ã‚‡ã†ã©39å¹´ç›®ã®è¨˜å¿µã™ã¹ãæ—¥ã§ã™ã€‚
1984å¹´2æœˆ22æ—¥ç™ºå£²ã®ã€Œã‚¢ãƒ•ã‚¿ãƒ¼ãƒ»ã‚µãƒ¼ãƒ´ã‚£ã‚¹ã€ã‚„ã€1992å¹´11æœˆ21æ—¥ç™ºå£²ã®ã€Œã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆãƒ»ã‚µãƒ¼ãƒ´ã‚£ã‚¹ã€ã«éŸ³æºãŒåéŒ²ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã¿ãªã•ã‚“æ˜¯éè´ã„ã¦ã¿ã¦ãã ã•ã„ã€‚
ã¾ãŸä½™è«‡ã§ã™ãŒã€æ™®æ®µã¯(ç ”ç©¶ãã£ã¡ã®ã‘ã§)CDã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚
70å¹´ä»£ã‹ã‚‰80å¹´ä»£ã«ã‹ã‘ã¦ã®ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãŒå¥½ãã§ã™ã€‚
æœ€è¿‘ã¯ã€å°‚ã‚‰ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã‚’è´ã„ã¦ã„ã¾ã™ã€‚
ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã®æ—§è¦æ ¼ç›¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯<a
href="./off-course-ca35-series.html">ã“ã¡ã‚‰</a>ã«ã‚ã‚Šã¾ã™ã€‚
ã¾ãŸã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯<a href="./cds.html">ã“ã¡ã‚‰</a>ã¨<a
href="./toshiba-emi.html">ã“ã¡ã‚‰</a>ã«ã¾ã¨ã‚ã¦ã‚ã‚Šã¾ã™ã€‚
æš‡ãªã¨ãã«ã”è¦§ãã ã•ã„ã€‚</p>
<p>ã‚‚ã†ä¸€ã¤ä½™è«‡ã€‚
ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã®åˆæœŸã®5ä½œå“ã‹ã‚‰ã€è‡ªåˆ†ã®å¥½ããªã‚‚ã®ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚</p>
<ol type="1">
<li>1stã‚¢ãƒ«ãƒãƒ ã€Œåƒ•ã®è´ˆã‚Šã‚‚ã®ã€(1973å¹´):
ğŸ¥‡ã€Œã‚ˆã¿ãŒãˆã‚‹ã²ã¨ã¨ãã€ğŸ¥ˆã€Œè²¼ã‚Šå¿˜ã‚ŒãŸå†™çœŸã€ğŸ¥‰ã€Œå½¼ã®ã»ã»ãˆã¿ã€</li>
<li>2ndã‚¢ãƒ«ãƒãƒ ã€Œã“ã®é“ã‚’ã‚†ã‘ã° ã‚ªãƒ•ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»ãƒ©ã‚¦ãƒ³ãƒ‰2ã€(1974å¹´):
ğŸ¥‡ã€Œã¯ãŸã¡ã®é ƒã€ğŸ¥ˆã€Œåˆ¥ã‚Œã®æƒ…æ™¯(1)ã€ğŸ¥‰ã€Œé¦–è¼ªã®ãªã„çŠ¬ã€ã€Œã‚ã®è§’ã‚’ã¾ãŒã‚Œã°ã€ã€Œæ—¥æ›œæ—¥ã®ãŸã„ãã¤ã€</li>
<li>3rdã‚¢ãƒ«ãƒãƒ ã€Œãƒ¯ã‚¤ãƒ³ã®åŒ‚ã„ã€(1975å¹´):
ğŸ¥‡ã€Œå¹»æƒ³ã€ğŸ¥ˆã€Œè€äººã®ã¤ã¶ã‚„ãã€ğŸ¥‰ã€Œæ†‚ãä¸–ã«ã€ã€Œé›¨ã‚ˆæ¿€ã—ãã€ã€Œå€–ã›ãªã‚“ã¦ã€</li>
<li>4thã‚¢ãƒ«ãƒãƒ ã€ŒSong Is Loveã€(1976å¹´):
ğŸ¥‡ã€Œå†¬ãŒæ¥ã‚‹ã¾ãˆã«ã€ğŸ¥ˆã€Œé’ç©ºã¨äººç”Ÿã¨ã€ğŸ¥‰ã€Œæ­Œã‚’æ§ã’ã¦ã€ã€Œé’æ˜¥ã€ã€Œã²ã¨ã‚Šã§ç”Ÿãã¦ã‚†ã‘ã‚Œã°ã€</li>
<li>5thã‚¢ãƒ«ãƒãƒ ã€ŒJunktionã€(1977å¹´):
ğŸ¥‡ã€Œå¤‰ã‚ã£ã¦ã‚†ãå¥³ã€ğŸ¥ˆã€ŒInvitationã€ğŸ¥‰ã€Œæ„›ã®ãã–ã—ã€</li>
</ol>
<p>ä»Šå¹´è´ã„ãŸãƒ™ã‚¹ãƒˆã‚¢ãƒ«ãƒãƒ  (å„ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã«ã¤ã2ä½œã¾ã§)ã€‚</p>
<ol type="1">
<li>ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒHaloã€(1983å¹´ / VICL-62399 / 2007å¹´ç›¤)</li>
<li>ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€Œã“ã®é“ã‚’ã‚†ã‘ã° ã‚ªãƒ•ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»ãƒ©ã‚¦ãƒ³ãƒ‰2ã€(1974å¹´ /
CA35-1033 / 1983å¹´ç›¤)</li>
<li>ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€Œãƒ¯ã‚¤ãƒ³ã®åŒ‚ã„ã€(1975å¹´ / CA35-1032 / 1983å¹´ç›¤)</li>
<li>ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒNew Tuneã€(1985å¹´ / 35FD-1005 / 1985å¹´ç›¤)</li>
<li>å¤§æ»è© ä¸€ã€ŒEach Timeã€(1984å¹´ / 35DH 78 / 1984å¹´ç›¤)</li>
<li>éº—ç¾ã€Œâ€œRâ€ã€(1984å¹´ / 35C31-7250 / 1984å¹´ç›¤)</li>
<li>ãƒã‚¤ãƒ»ãƒ•ã‚¡ã‚¤ãƒ»ã‚»ãƒƒãƒˆã€ŒSweet Locomotionã€(1986å¹´ / 32DH 393 /
1986å¹´ç›¤)</li>
<li>å’Œä¹…äº•æ˜ è¦‹ã€ŒFloraã€(1990å¹´ / PSCR-1006 / 1990å¹´ç›¤))</li>
<li>éˆ´æœ¨åº·åšã€ŒSincerelyã€(1983å¹´ / CA35-1043 / 1983å¹´ç›¤)</li>
<li>å²¡ç”°æœ‰å¸Œå­ã€Œãƒ´ã‚£ãƒ¼ãƒŠã‚¹èª•ç”Ÿã€(1986å¹´ / D32A0164 / 1986å¹´ç›¤)</li>
</ol>
<p>ã‚¤ãƒ³ãƒˆãƒ­ãŒè‰¯ã„æ›²ã€‚</p>
<ol type="1">
<li>ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒShooting Starã€(1981å¹´)</li>
<li>äº•ä¸Šé‘‘ã€ŒKarsavina ï½ãƒ‹ã‚¸ãƒ³ã‚¹ã‚­ãƒ¼ã®ç¿¼ã€(1983å¹´)</li>
<li>äº•ä¸Šé‘‘ã€ŒRunning Fence -Ode A Christoã€(1982å¹´)</li>
</ol>
<p>ä»Šå¹´ã¯ã€ç‚¹ç¾¤å‡¦ç† (ç‚¹ç¾¤åˆ†é¡ã‚¿ã‚¹ã‚¯)
å‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®FPGAé«˜é€ŸåŒ–ã‚’è©¦ã—ã¦ã¿ã¾ã™ã€‚
LeNetã‚„ResNetãªã©ã€ç”»åƒå‡¦ç†å‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®FPGAé«˜é€ŸåŒ–ã‚‚é¢ç™½ã„ã®ã§ã™ãŒã€æ—¢ã«ãŸãã•ã‚“ã®ç´ æ™´ã‚‰ã—ã„è¨˜äº‹ãŒå‡ºã¦ã„ã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸã€‚
éŸ³æ¥½ã®è©±ã‚‚ã€èª°ã«ã‚‚é€šã˜ãªã„ã—ã€ã‚¦ã‚±ãªã„ã¨æ€ã£ãŸã®ã§ã‚„ã‚ã¾ã—ãŸã€‚
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§é–²è¦§ã•ã‚Œã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚</p>
<h1 id="ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æº–å‚™">ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æº–å‚™</h1>
<p>ç‚¹ç¾¤ã®åˆ†é¡ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã—ãŸä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€2017å¹´ã«CVPRã§ç™ºè¡¨ã•ã‚ŒãŸPointNetãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
PointNetã¯ã€MLPã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã‹ã‚‰ãªã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
åˆ†é¡ã‚¿ã‚¹ã‚¯å‘ã‘ã®PointNetã®æ§‹é€ ã‚’ã€ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-layers.svg"><img src="point-cloud-classification-images/pointnet-layers.svg" width="100%" /></a></p>
<p>ãƒ¢ãƒ‡ãƒ«ã¯ã€ç‚¹ç¾¤ã‹ã‚‰ã®ç‰¹å¾´æŠ½å‡ºã¨ã€ç‰¹å¾´ã«åŸºã¥ãåˆ†é¡ã®ã€2ã¤ã®éƒ¨åˆ†ã«åˆ†ã‘ã‚‰ã‚Œã¾ã™
(å›³ã®Feature extractionã¨Classification)ã€‚</p>
<p>å›³ã®å·¦ç«¯ã«ç¤ºã™ã‚ˆã†ã«ã€<span
class="math inline">\(N\)</span>å€‹ã®ç‚¹ã‚’å«ã‚€ã€3æ¬¡å…ƒã®ç‚¹ç¾¤<span
class="math inline">\(\mathcal{P} = \left\{ \boldsymbol{p}_1, \ldots,
\boldsymbol{p}_N \right\} \in \mathbb{R}^{N \times
3}\)</span>ãŒå…¥åŠ›ã§ã™ã€‚ MLPã‚’ç”¨ã„ã¦ã€å„ç‚¹<span
class="math inline">\(\boldsymbol{p}_i \in
\mathbb{R}^3\)</span>ã«å¯¾ã—ã¦ã€1024æ¬¡å…ƒã®ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´<span
class="math inline">\(\boldsymbol{\psi}_i \in
\mathbb{R}^{1024}\)</span>ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
å…¨ã¦ã®ç‚¹ã«å¯¾ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\Psi} = \left\{ \boldsymbol{\psi}_1,
\ldots, \boldsymbol{\psi}_N \right\} \in \mathbb{R}^{N \times
1024}\)</span>ã‚’è¨ˆç®—ã—ãŸã‚‰ã€ãã‚Œã‚‰ã‚’Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã‚ˆã‚Šé›†ç´„ã—ã¦ã€ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi} \in
\mathbb{R}^{1024}\)</span>ã‚’å¾—ã¾ã™ (<span
class="math inline">\(\boldsymbol{\phi} \gets \max(\boldsymbol{\psi}_1,
\ldots, \boldsymbol{\psi}_N)\)</span>)ã€‚</p>
<p>åˆ†é¡ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ã“ã®ç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’å…¥åŠ›ã¨ã—ã¦ã€å„ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ
(ã‚¹ã‚³ã‚¢)ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹æ•°ã‚’<span
class="math inline">\(K\)</span>ã¨ã™ã‚Œã°ã€å‡ºåŠ›ã¯<span
class="math inline">\(K\)</span>æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ãªã‚Šã¾ã™ã€‚</p>
<p>å›³ã®Input TransformãŠã‚ˆã³Feature
Transformã¯ã€ç‚¹ç¾¤ã®ç‰¹å¾´ã«å¯¾ã—ã¦ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã‚’æ–½ã—ã€å‰›ä½“å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªç‰¹å¾´é‡ã‚’å¾—ã‚‹ãŸã‚ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ãŒã€å®Ÿè£…ãŒé¢å€’ãªã®ã§å–ã‚Šé™¤ãã¾ã™(<strong>æœ€é©åŒ–ãã®1:
ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–</strong>)ã€‚
å¾“ã£ã¦ã€ä»Šå›FPGAä¸Šã«å®Ÿè£…ã™ã‚‹PointNetã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<p>ç”»åƒèªè­˜å‘ã‘ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€ç•³ã¿è¾¼ã¿å±¤ãŒã‚ã‚Šã¾ã›ã‚“ã€‚
ã¾ãŸã€MLPã¯ã€å…¨çµåˆå±¤ã€ReLUæ´»æ€§åŒ–å±¤ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã¨ã—ã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-layers2.svg"><img src="point-cloud-classification-images/pointnet-layers2.svg" width="80%" /></a></p>
<p>PyTorchã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™
(<code>net/model.py</code>)ã€‚ ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰å…¨ä½“ã¯<a
href="https://github.com/sterngerlach/advent_2022_point_cloud_classification">ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒª</a>ã«ç½®ã‹ã‚Œã¦ã„ã‚‹ã®ã§ã€é©å®œã”å‚ç…§ãã ã•ã„ã€‚</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PointNetFeat(torch.nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv4 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv5 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">128</span>, <span class="dv">1024</span>, <span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn4 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn5 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">1024</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, N, 3]</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 3, N]</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024, N]</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn2(<span class="va">self</span>.conv2(x)))</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn3(<span class="va">self</span>.conv3(x)))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn4(<span class="va">self</span>.conv4(x)))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn5(<span class="va">self</span>.conv5(x)))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024]</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.<span class="bu">max</span>(x, dim<span class="op">=</span><span class="dv">2</span>)[<span class="dv">0</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PointNetCls(torch.nn.Module):</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes: <span class="bu">int</span>):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feat <span class="op">=</span> PointNetFeat()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification network</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> torch.nn.Linear(<span class="dv">1024</span>, <span class="dv">512</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> torch.nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> torch.nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">512</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">256</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, N, 3]</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024]</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.feat(x)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, `num_classes`]</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.fc1(x)))</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn2(<span class="va">self</span>.fc2(x)))</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
<p>ã•ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾å®Ÿè£…ã™ã‚‹å ´åˆã€æ¬¡ã®ã‚ˆã†ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºéƒ¨åˆ† (å›³ã®Feature extraction)ã«æ³¨ç›®ã—ã¾ã™ã€‚
å›³ä¸­ã®ç°è‰²ã®å››è§’ã«ç¤ºã™ã‚ˆã†ã«ã€<span
class="math inline">\(N\)</span>å€‹å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚„ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\Psi}\)</span>ã‚’ã€ã©ã“ã‹ã«ä¿æŒã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
å¤§å®¹é‡ã®ãƒ¡ãƒ¢ãƒªã‚’æ­è¼‰ã—ãŸGPUã§ã‚ã‚Œã°ã€ã“ã‚Œã§ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ãŒã€FPGAå†…éƒ¨ã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª
(BlockRAM)
ã¯éå¸¸ã«å®¹é‡ãŒå°‘ãªã„ã®ã§ã€å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚’ä¿æŒã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªãŒã‚ã£ã¨ã„ã†é–“ã«æ¯æ¸‡ã™ã‚‹ã§ã—ã‚‡ã†ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€æ­è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®å®¹é‡ã«ã‚ˆã£ã¦ã€ç‚¹ã®å€‹æ•°<span
class="math inline">\(N\)</span>ãŒåˆ¶é™ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚
ã“ã‚Œã¯é¿ã‘ãŸã„ã‚‚ã®ã§ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®ä»£ã‚ã‚Šã«ã€å®¹é‡ã®å¤§ããªDRAMä¸Šã«ç½®ãã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã¯é•·ããªã‚Šã¾ã™ã€‚
å…¨ã¦ã®å±¤ã®ä¸­é–“çµæœã‚’DRAMã«ç½®ãã¨ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¢—åŠ ã—ã¦ã€æ€§èƒ½ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã—ã¾ã™ã€‚
å±¤ã®ä¸­é–“çµæœã¯ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«ç½®ããŸã„ã‚‚ã®ã§ã™ã€‚</p>
<p>ãã“ã§ã€å…¨ã¦ã®ç‚¹<span
class="math inline">\(\mathcal{P}\)</span>ã«å¯¾ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\Psi}\)</span>ã‚’ä¸€æ°—ã«è¨ˆç®—ã™ã‚‹ã®ã§ã¯ãªãã€1ã¤ãšã¤ã®ç‚¹<span
class="math inline">\(\boldsymbol{p}\)</span>ã«å¯¾ã—ã¦é †ã«ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\psi}\)</span>ã‚’è¨ˆç®—ã—ã¾ã—ã‚‡ã†ã€‚
ä¸€æ°—ã«è¨ˆç®—ã™ã‚‹ã®ã¨æ¯”ã¹ã¦è¨ˆç®—åŠ¹ç‡ã¯è½ã¡ã¾ã™ãŒã€1ã¤ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚„ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ã ã‘ã‚’ä¿æŒã™ã‚Œã°ã‚ˆã„ã®ã§ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®æ¶ˆè²»ã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ã€‚</p>
<p>ä»¥å‰ã¯
(PyTorchãªã©ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†å ´åˆã¯)ã€ç‰¹å¾´æŠ½å‡ºã¯æ¬¡ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã¦ã„ã¾ã—ãŸã€‚</p>
<ol type="1">
<li>å…¨ã¦ã®ç‚¹<span
class="math inline">\(\mathcal{P}\)</span>ã«å¯¾ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ã‚’<span
class="math inline">\(\boldsymbol{\Psi}\)</span>ã‚’ã¾ã¨ã‚ã¦è¨ˆç®—ã™ã‚‹
(<span class="math inline">\((N, 64)\)</span>ã‚„<span
class="math inline">\((N, 1024)\)</span>ã®ãƒãƒƒãƒ•ã‚¡ãŒå¿…è¦)ã€‚</li>
<li>Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã‚ˆã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\Psi}\)</span>ã‚’é›†ç´„ã—ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’å¾—ã‚‹ (<span
class="math inline">\(\boldsymbol{\phi} \gets \max(\boldsymbol{\psi}_1,
\ldots, \boldsymbol{\psi}_N)\)</span>)ã€‚</li>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’MLPã«å…¥åŠ›ã—ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ(<span
class="math inline">\(K\)</span>æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«)ã‚’å¾—ã‚‹ã€‚</li>
</ol>
<p>ã“ã‚Œã‚’ã€æ¬¡ã®ã‚ˆã†ã«å¤‰æ›´ã—ã¾ã™(<strong>æœ€é©åŒ–ãã®2:
è¨ˆç®—é †åºã®å¤‰æ›´</strong>)ã€‚</p>
<ol type="1">
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’ã€<span
class="math inline">\(\boldsymbol{0}\)</span>ã§åˆæœŸåŒ–ã™ã‚‹ã€‚</li>
<li>å„ç‚¹<span class="math inline">\(\boldsymbol{p}_i \ (i = 1, \ldots,
N)\)</span>ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã†ã€‚
<ol type="1">
<li>MLPã®é †ä¼æ’­ã«ã‚ˆã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\psi}_i\)</span>ã‚’å¾—ã‚‹ (<span
class="math inline">\((1, 64)\)</span>ã‚„<span class="math inline">\((1,
1024)\)</span>ã®ãƒãƒƒãƒ•ã‚¡ãŒã‚ã‚Œã°ã‚ˆã„)ã€‚</li>
<li><span class="math inline">\(\boldsymbol{\phi}\)</span>ã¨<span
class="math inline">\(\boldsymbol{\psi}_i\)</span>ã¨ã®ã€è¦ç´ ã”ã¨ã®<span
class="math inline">\(\max\)</span>ã‚’ã¨ã‚‹ã“ã¨ã§ã€<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’æ›´æ–°ã™ã‚‹ (<span
class="math inline">\(\boldsymbol{\phi} \gets \max(\boldsymbol{\phi},
\boldsymbol{\psi}_i)\)</span>)ã€‚</li>
</ol></li>
<li>ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’MLPã«å…¥åŠ›ã—ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ(<span
class="math inline">\(K\)</span>æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«)ã‚’å¾—ã‚‹ã€‚</li>
</ol>
<p>å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\Psi}\)</span>ã‚’é›†ç´„ã™ã‚‹ã®ã§ã¯ãªãã€å„ç‚¹<span
class="math inline">\(\boldsymbol{p}_i\)</span>ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\psi}_i\)</span>ã‚’ä½¿ã£ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi}\)</span>ã‚’é€æ¬¡çš„ã«æ›´æ–°ã—ã¦ã„ãã¾ã™ã€‚
ã“ã‚Œã¯è¿‘ä¼¼ã§ã¯ãªã„ã®ã§ã€å…¨ãåŒã˜çµæœã¨ãªã‚Šã¾ã™ã€‚</p>
<p>æœ€çµ‚çš„ã«ã€ä»Šå›FPGAä¸Šã«å®Ÿè£…ã™ã‚‹PointNetã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-layers3.svg"><img src="point-cloud-classification-images/pointnet-layers3.svg" width="80%" /></a></p>
<h1 id="é«˜ä½åˆæˆã«ã‚ˆã‚‹å®Ÿè£…">é«˜ä½åˆæˆã«ã‚ˆã‚‹å®Ÿè£…</h1>
<p>ä»Šå›ã¯ã€é«˜ä½åˆæˆ (HLS: High-Level
Synthesis)ã‚’ç”¨ã„ã¦ã€ä¸Šè¨˜ã«ç¤ºã™PointNetã®å°‚ç”¨å›è·¯
(<strong>IPã‚³ã‚¢</strong>) ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æ¨è«–ã‚’å®Ÿç¾ã™ã‚‹åˆ¥ã®æ‰‹æ®µã¨ã—ã¦ã€è¡Œåˆ—æ¼”ç®—ã‚„ç•³ã¿è¾¼ã¿æ¼”ç®—ç”¨ã®ã€å·¨å¤§ã‹ã¤æ±ç”¨çš„ãªæ¼”ç®—å›è·¯ã‚’FPGAä¸Šã«å®Ÿè£…ã—ã€ãã‚Œã«ç¹°ã‚Šè¿”ã—ãƒ‡ãƒ¼ã‚¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚</p>
<p>é«˜ä½åˆæˆã¯ã€C/C++ã«ã‚ˆã‚‹å‹•ä½œãƒ¬ãƒ™ãƒ« (Behavior Level)
ã®å›è·¯è¨˜è¿°ã‚’ã€Verilog HDLã‚„SystemVerilogã«ã‚ˆã‚‹ãƒ¬ã‚¸ã‚¹ã‚¿è»¢é€ãƒ¬ãƒ™ãƒ« (RTL:
Register Transfer Level) ã®å›è·¯è¨˜è¿°ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚ Verilog
HDLã‚’ç›´æ¥è¨˜è¿°ã™ã‚‹ã®ã«æ¯”ã¹ã¦ã€é¥ã‹ã«æ¥½ã§ã€ã‚¹ãƒˆãƒ¬ã‚¹ãŒå°‘ãªãã€ç”Ÿç”£æ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚
ä½†ã—ã€C/C++ã§è¨˜è¿°ã™ã‚‹ã¨ã¯ã„ã£ã¦ã‚‚ã€é€šå¸¸ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã¨ã¯å…¨ãæ§˜ç›¸ãŒç•°ãªã‚Šã¾ã™ã€‚
<code>malloc()</code>ã‚„<code>new</code>ã¯ã‚‚ã¡ã‚ã‚“ã®ã“ã¨ã€ã“ã‚Œã‚‰ã«ä¾å­˜ã™ã‚‹<code>std::vector</code>ãªã©ã®ä¾¿åˆ©ãªãƒ‡ãƒ¼ã‚¿å‹ã‚‚ä½¿ãˆãªã„ã®ã§ã€å›ºå®šé•·ã®é…åˆ—ã«ç½®ãæ›ãˆã¦ã©ã†ã«ã‹ã—ã¾ã™ã€‚
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã‚µã‚¤ã‚ºãŒå›ºå®šã§ã€ä¸€èˆ¬ã«ã¯æ±ºã¾ã£ãŸå‹•ä½œã‚’ã™ã‚‹ã®ã§ã€FPGAä¸Šã«å®Ÿè£…ã—ã‚„ã™ã„ã§ã™ã€‚</p>
<p>é«˜ä½åˆæˆç”¨ã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã€Xilinxç¤¾ã®Vitis HLS 2022.1ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ã¾ãŸå®Ÿè£…å¯¾è±¡ã®FPGAã¨ã—ã¦ã€Xilinx ZCU104 Evaluation Board
(XCZU7EV-2FFVC1156)ã‚’ä½¿ã„ã¾ã™ã€‚ Xilinx
ZCU104ã«ã¯ã€FPGAã®ã»ã‹ã«ã€ã‚¯ã‚¢ãƒƒãƒ‰ã‚³ã‚¢ ARM Cortex-A53 CPU
(1.2GHz)ã¨2GBã®DRAMã‚‚æ­è¼‰ã•ã‚Œã¦ãŠã‚Šã€LinuxãŒå‹•ä½œã—ã¾ã™ã€‚</p>
<p>æ—©é€Ÿã€PointNetã®IPã‚³ã‚¢ã‚’ç¤ºã—ã¾ã™
(é©å®œGitHubã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã”è¦§ãã ã•ã„)ã€‚
é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒGCC
6.2ã§ã™ã®ã§ã€C++14ã‚„C++17ã®ä¸€éƒ¨æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã™ã€‚
ä½†ã—ã€ãƒ„ãƒ¼ãƒ«ã®ãƒã‚°ã‚’è¸ã‚€ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ã‚ã¾ã‚Šå‡ã£ãŸæ©Ÿèƒ½ã¯ä½¿ã‚ãªã„ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Size of the PointNet classification network</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Refer to net/model.py for details</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">// Size of the feature extraction network</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims0 <span class="op">=</span> <span class="dv">3</span><span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims1 <span class="op">=</span> <span class="dv">64</span><span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims2 <span class="op">=</span> <span class="dv">64</span><span class="op">;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims3 <span class="op">=</span> <span class="dv">64</span><span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims4 <span class="op">=</span> <span class="dv">128</span><span class="op">;</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kFeatDims5 <span class="op">=</span> <span class="dv">1024</span><span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">// Size of the classification network</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">// ModelNet40 has 40 object classes</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kClsDims0 <span class="op">=</span> kFeatDims5<span class="op">;</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kClsDims1 <span class="op">=</span> <span class="dv">512</span><span class="op">;</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kClsDims2 <span class="op">=</span> <span class="dv">256</span><span class="op">;</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> kClsDims3 <span class="op">=</span> <span class="dv">40</span><span class="op">;</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">// Top function</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>                    <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params1<span class="op">,</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params2<span class="op">,</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params3<span class="op">,</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params4<span class="op">,</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params5<span class="op">,</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params1<span class="op">,</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params2<span class="op">,</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params3<span class="op">)</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params2 offset=slave bundle=gmem0</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params3 offset=slave bundle=gmem0</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params4 offset=slave bundle=gmem0</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params5 offset=slave bundle=gmem0</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params1 offset=slave bundle=gmem0</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params2 offset=slave bundle=gmem0</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=op_mode bundle=control</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=num_points bundle=control</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=out_logits bundle=control</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params2 bundle=control</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params3 bundle=control</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params4 bundle=control</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params5 bundle=control</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params1 bundle=control</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params2 bundle=control</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=return bundle=control</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Parameters for feature extraction</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;</span> feat_conv1<span class="op">;</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;</span> feat_conv2<span class="op">;</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;</span> feat_conv3<span class="op">;</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;</span> feat_conv4<span class="op">;</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;</span> feat_conv5<span class="op">;</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims1<span class="op">&gt;</span> feat_bn1<span class="op">;</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims2<span class="op">&gt;</span> feat_bn2<span class="op">;</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims3<span class="op">&gt;</span> feat_bn3<span class="op">;</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims4<span class="op">&gt;</span> feat_bn4<span class="op">;</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims5<span class="op">&gt;</span> feat_bn5<span class="op">;</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Parameters for classification network</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>  <span class="co">// LinearParams&lt;param_t, kClsDims0, kClsDims1&gt; cls_fc1;</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>  <span class="co">// LinearParams&lt;param_t, kClsDims1, kClsDims2&gt; cls_fc2;</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;</span> cls_fc3<span class="op">;</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims1<span class="op">&gt;</span> cls_bn1<span class="op">;</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims2<span class="op">&gt;</span> cls_bn2<span class="op">;</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Extracted feature</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>  <span class="dt">value_t</span> feature<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>op_mode <span class="op">==</span> kModeInitWeights<span class="op">)</span> <span class="op">{</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Initialize the PointNet feature extraction network</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>    InitializeFeatNaive<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_conv1<span class="op">,</span> <span class="op">&amp;</span>feat_conv2<span class="op">,</span> <span class="op">&amp;</span>feat_conv3<span class="op">,</span> <span class="op">&amp;</span>feat_conv4<span class="op">,</span> <span class="op">&amp;</span>feat_conv5<span class="op">,</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_bn1<span class="op">,</span> <span class="op">&amp;</span>feat_bn2<span class="op">,</span> <span class="op">&amp;</span>feat_bn3<span class="op">,</span> <span class="op">&amp;</span>feat_bn4<span class="op">,</span> <span class="op">&amp;</span>feat_bn5<span class="op">,</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>      feat_params1<span class="op">,</span> feat_params2<span class="op">,</span> feat_params3<span class="op">,</span> feat_params4<span class="op">,</span> feat_params5<span class="op">);</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Initialize the classification network</span></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    InitializeClsNaive<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>cls_fc3<span class="op">,</span> <span class="op">&amp;</span>cls_bn1<span class="op">,</span> <span class="op">&amp;</span>cls_bn2<span class="op">,</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>      cls_params1<span class="op">,</span> cls_params2<span class="op">,</span> cls_params3<span class="op">);</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>op_mode <span class="op">==</span> kModeInference<span class="op">)</span> <span class="op">{</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Run the PointNet feature extraction</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>    InferenceFeatNaive<span class="op">&lt;</span><span class="dt">value_t</span><span class="op">,</span> <span class="dt">param_t</span><span class="op">,</span> <span class="dv">1024</span><span class="op">&gt;(</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>      point_cloud<span class="op">,</span> num_points<span class="op">,</span> feature<span class="op">,</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_conv1<span class="op">,</span> <span class="op">&amp;</span>feat_conv2<span class="op">,</span> <span class="op">&amp;</span>feat_conv3<span class="op">,</span> <span class="op">&amp;</span>feat_conv4<span class="op">,</span> <span class="op">&amp;</span>feat_conv5<span class="op">,</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_bn1<span class="op">,</span> <span class="op">&amp;</span>feat_bn2<span class="op">,</span> <span class="op">&amp;</span>feat_bn3<span class="op">,</span> <span class="op">&amp;</span>feat_bn4<span class="op">,</span> <span class="op">&amp;</span>feat_bn5<span class="op">);</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Run the classification</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>    InferenceClsNaive<span class="op">&lt;</span><span class="dt">value_t</span><span class="op">,</span> <span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>      feature<span class="op">,</span> out_logits<span class="op">,</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>cls_fc3<span class="op">,</span> <span class="op">&amp;</span>cls_bn1<span class="op">,</span> <span class="op">&amp;</span>cls_bn2<span class="op">,</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>      cls_params1<span class="op">,</span> cls_params2<span class="op">,</span> cls_params3<span class="op">);</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>ä¸Šè¨˜ã‚’é«˜ä½åˆæˆã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãªIPã‚³ã‚¢ãŒä½œã‚‰ã‚Œã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-ip-core.svg"><img src="point-cloud-classification-images/pointnet-ip-core.svg" width="50%" /></a></p>
<p>ã“ã®IPã‚³ã‚¢ã‚’åˆ¥ã®IPã‚³ã‚¢ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§
(å¾Œè¿°)ã€æ¬¡ã®ã‚ˆã†ãªãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã§ãã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/board-design.svg"><img src="point-cloud-classification-images/board-design.svg" width="100%" /></a></p>
<p>ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ã«å¯¾ã—ã¦ã€è«–ç†åˆæˆãŠã‚ˆã³é…ç½®é…ç·šã™ã‚‹ã“ã¨ã§ã€å›è·¯æƒ…å ±ã‚’è¡¨ã™ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
(Bitstream) ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’FPGAã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã€PointNetã®å°‚ç”¨å›è·¯ãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<h2 id="å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆ">å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆ</h2>
<p><code>PointNetClsTop</code>ãŒã€IPã‚³ã‚¢ã‚’è¡¨ã™æœ€ä¸Šä½ã®é–¢æ•°ã§ã™ã€‚
ãƒˆãƒƒãƒ—é–¢æ•° (Top function) ã¨ã‚ˆã°ã‚Œã¾ã™ã€‚
é–¢æ•°ã®å¼•æ•°ã¯ã€IPã‚³ã‚¢ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã¨ãªã‚Šã€åˆ¥ã®IPã‚³ã‚¢ã«æ¥ç¶šã•ã‚Œã¾ã™
(ä¸Šã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ã”è¦§ãã ã•ã„)ã€‚ HLSã§ã¯ã€é–¢æ•°ãã®ã‚‚ã®ãŒå›è·¯
(Verilog HDLã«ãŠã‘ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«) ã«ãªã‚Šã¾ã™ã€‚
é–¢æ•°ã®å†å¸°å‘¼ã³å‡ºã—ã¯ã§ãã¾ã›ã‚“ã€‚</p>
<p>ç‰¹å¾´æŠ½å‡ºç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯5ã¤ã®MLPã€ã¾ãŸã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯3ã¤ã®MLPãŒå«ã¾ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰æ“ä½œã§ãã‚‹ã‚ˆã†ã«ã€DRAMä¸Šã®ãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚Œã¾ã™ã€‚
ã¾ãŸã€ç‚¹ç¾¤<span
class="math inline">\(\mathcal{P}\)</span>ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›(ãƒ­ã‚¸ãƒƒãƒˆ)ã‚‚åŒæ§˜ã«ã€DRAMãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚Œã¾ã™ã€‚</p>
<p><code>feat_params1</code>ã‹ã‚‰<code>feat_params5</code>ã¾ã§ã¨ã€<code>cls_params1</code>ã‹ã‚‰<code>cls_params3</code>ã¾ã§ã®8ã¤ã®ãƒãƒ¼ãƒˆã¯ã€DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€IPã‚³ã‚¢å´ã‹ã‚‰èª­ã¿å–ã‚‹ãŸã‚ã«ä½¿ã„ã¾ã™ã€‚
<code>point_cloud</code>ã¯ç‚¹ç¾¤ã®èª­ã¿å‡ºã—ã€<code>out_logits</code>ã¯ãƒ­ã‚¸ãƒƒãƒˆã®æ›¸ãè¾¼ã¿ã®ãŸã‚ã«ä½¿ã„ã¾ã™ã€‚
<code>op_mode</code>ã¯å›è·¯ã®å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã€<code>num_points</code>ã¯ç‚¹ã®å€‹æ•°<span
class="math inline">\(N\)</span>ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã§ã™ã€‚</p>
<p><code>#pragma HLS</code>ã‹ã‚‰å§‹ã¾ã‚‹è¡Œã¯ã€é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã«å¯¾ã—ã¦ã€C/C++ã‹ã‚‰RTLã«å¤‰æ›ã™ã‚‹éš›ã®ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆã¾ã™
(å¿…ãšã—ã‚‚å®ˆã£ã¦ãã‚Œã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“)ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãªã©ã¯C/C++ã§ã¯è¨˜è¿°ã§ãã¾ã›ã‚“ãŒã€ã“ã®ã‚ˆã†ãª<strong>HLSãƒ—ãƒ©ã‚°ãƒ</strong>ã‚’é©åˆ‡ãªå ´æ‰€ã«ç½®ãã“ã¨ã§ã€é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ãŒè‡ªå‹•çš„ã«ã“ã‚Œã‚‰ã®æœ€é©åŒ–ã‚’æ–½ã—ã¦ãã‚Œã¾ã™ã€‚</p>
<p><code>#pragma HLS INLINE off</code>ã¨ã™ã‚‹ã¨ã€ãã®é–¢æ•°ãŒã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã•ã‚Œãªããªã‚Šã¾ã™
(å¿…ãšã€1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ä½œã‚‰ã‚Œã‚‹)ã€‚
å¤§ããªé–¢æ•°ã§ã‚ã‚Œã°ã€è‡ªå‹•çš„ã«ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿µã®ãŸã‚ä»˜ä¸ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚ˆã†ãªçŠ¶æ³ã§ã¯ã€é–¢æ•°<code>B</code>ã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã—ãªã„æ–¹ãŒã„ã„ã¨æ€ã„ã¾ã™ã€‚
åŒæ™‚ã«ä½¿ã‚ã‚Œãªã„ã®ã«ã‚‚é–¢ã‚ã‚‰ãšã€é–¢æ•°<code>A</code>ã®å†…éƒ¨ã«<code>B</code>ã®ã‚³ãƒ”ãƒ¼ãŒ3ã¤ä½œã‚‰ã‚Œã¦ã€ãƒªã‚½ãƒ¼ã‚¹ã®ç„¡é§„é£ã„ã¨ãªã‚Šã¾ã™ã€‚
é–¢æ•°<code>B</code>ã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–ã‚’æŠ‘åˆ¶ã—ã¦ã€<code>B</code>ã‚’1ã¤ã ã‘ä½œã‚Šã€ãã‚Œã‚’ä½¿ã„å›ã—ãŸæ–¹ãŒã„ã„ã§ã—ã‚‡ã†ã€‚</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> B<span class="op">(</span><span class="at">const</span> <span class="dt">float</span> x_in<span class="op">[</span><span class="dv">10</span><span class="op">],</span> <span class="dt">float</span> y_out<span class="op">[</span><span class="dv">10</span><span class="op">])</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ä½•ã‚‰ã‹ã®å‡¦ç†</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> A<span class="op">(</span><span class="at">const</span> <span class="dt">float</span> x_in<span class="op">[</span><span class="dv">10</span><span class="op">],</span> <span class="dt">float</span> y_out<span class="op">[</span><span class="dv">10</span><span class="op">])</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">float</span> x0<span class="op">[</span><span class="dv">10</span><span class="op">];</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">float</span> x1<span class="op">[</span><span class="dv">10</span><span class="op">];</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  B<span class="op">(</span>x_in<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  B<span class="op">(</span>x0<span class="op">,</span> x1<span class="op">);</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  B<span class="op">(</span>x1<span class="op">,</span> y_out<span class="op">);</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>#pragma HLS INTERFACE m_axi</code>ã¨ã€<code>#pragma HLS INTERFACE s_axilite</code>ã®è¨˜è¿°ãŒç›®ç«‹ã¡ã¾ã™ãŒã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆ
(ä¾‹ãˆã°<code>feat_params1</code>)
ã«å¯¾ã—ã¦ã“ã®2ã¤ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’è¨˜è¿°ã™ã‚‹ã¨ã€IPã‚³ã‚¢å´ã‹ã‚‰DRAMãƒãƒƒãƒ•ã‚¡ã‚’èª­ã¿æ›¸ãã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
èª­ã¿æ›¸ãã®éš›ã«ã¯ã€AXIã¨ã‚ˆã°ã‚Œã‚‹ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€<code>#pragma HLS INTERFACE m_axi</code>ã«ã‚ˆã£ã¦ãã‚Œã‚’æŒ‡å®šã§ãã¾ã™
(IPã‚³ã‚¢å´ãŒãƒã‚¹ã‚¿ãƒ¼ã«ãªã‚Šã¾ã™)ã€‚</p>
<p>ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã¯ã€å„ãƒãƒ¼ãƒˆã«å¯¾ã—ã¦ã€ãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¦ã€ãƒãƒ¼ãƒˆã¨ãƒãƒƒãƒ•ã‚¡ã‚’ç´ã¥ã‘ã¾ã™ã€‚
å„ãƒãƒ¼ãƒˆã«ã¯ã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€<code>#pragma HLS INTERFACE s_axilite</code>ã«ã‚ˆã£ã¦ãã‚Œã‚’å®Ÿç¾ã§ãã¾ã™
(IPã‚³ã‚¢å´ã‹ã‚‰ã¿ã‚‹ã¨ã‚¹ãƒ¬ãƒ¼ãƒ–ã§ã™)ã€‚
<code>op_mode</code>ã€<code>num_points</code>ã«å¯¾ã—ã¦ã‚‚ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚
<code>port=return</code>ã¨ã—ã¦ã„ã‚‹è¡Œã¯ã€IPã‚³ã‚¢ç”¨ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã—ã€CPUå´ã‹ã‚‰IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ãŸã‚Šã€çŠ¶æ…‹
(ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ãªã®ã‹å‹•ä½œä¸­ã‹) ã‚’èª­ã¿å–ã£ãŸã‚Šã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ¬ã‚¸ã‚¹ã‚¿ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/OãŠã‚ˆã³AXI-Liteãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã‚ˆã£ã¦èª­ã¿æ›¸ãã•ã‚Œã¾ã™ã€‚</p>
<p>å„å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã‹ã‚‰ã¯ã€PyTorchã®ãƒ¢ãƒ‡ãƒ«ã§å®šç¾©ã—ãŸã€å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒèª­ã¿å‡ºã•ã‚Œã¾ã™
(ä¸€æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦ã€å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé€£çµã•ã‚Œã¾ã™)ã€‚</p>
<ul>
<li><code>feat_params1</code>: <code>PointNetFeat::conv1</code> +
<code>PointNetFeat::bn1</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>feat_params2</code>: <code>PointNetFeat::conv2</code> +
<code>PointNetFeat::bn2</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>feat_params3</code>: <code>PointNetFeat::conv3</code> +
<code>PointNetFeat::bn3</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>feat_params4</code>: <code>PointNetFeat::conv4</code> +
<code>PointNetFeat::bn4</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>feat_params5</code>: <code>PointNetFeat::conv5</code> +
<code>PointNetFeat::bn5</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>cls_params1</code>: <code>PointNetCls::fc1</code> +
<code>PointNetCls::bn1</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>cls_params2</code>: <code>PointNetCls::fc2</code> +
<code>PointNetCls::bn2</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
<li><code>cls_params3</code>:
<code>PointNetCls::fc3</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿</li>
</ul>
<div class="sourceCode" id="cb4"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params1<span class="op">,</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params2<span class="op">,</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params3<span class="op">,</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params4<span class="op">,</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params5<span class="op">,</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params1<span class="op">,</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params2<span class="op">,</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params3<span class="op">)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<h2 id="å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‡¦ç†">å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‡¦ç†</h2>
<p><code>torch.nn.Conv1d</code>ãŠã‚ˆã³<code>torch.nn.Linear</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ã¯ã€é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
<code>Conv1d</code>ã¨ã‚ã‚Šã¾ã™ãŒã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã¯1ãªã®ã§ã€<code>Linear</code>ã¨å‹•ä½œãŒåŒã˜ã«ãªã‚Šã¾ã™ã€‚
ä»¥å¾Œã€<code>Conv1d</code>ã¨<code>Linear</code>ã‚’åŒä¸€è¦–ã—ã¾ã™ã€‚
å…¥åŠ›ã¨å‡ºåŠ›ã®æ¬¡å…ƒæ•°ã‚’<span
class="math inline">\(\mathrm{InDims}\)</span>ã€<span
class="math inline">\(\mathrm{OutDims}\)</span>ã¨ã™ã‚‹ã¨ã€é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®ã‚µã‚¤ã‚ºã¯<span
class="math inline">\((\mathrm{OutDims},
\mathrm{InDims})\)</span>ã€<span
class="math inline">\((\mathrm{OutDims})\)</span>ã¨ãªã‚Šã¾ã™ã€‚ å…¥åŠ›<span
class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^{\mathrm{InDims}}\)</span>ã€é‡ã¿<span
class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{\mathrm{OutDims}
\times \mathrm{InDims}}\)</span>ã€ãƒã‚¤ã‚¢ã‚¹<span
class="math inline">\(\boldsymbol{b} \in
\mathbb{R}^{\mathrm{OutDims}}\)</span>ãŒã‚ã‚‹ã¨ãã€å‡ºåŠ›<span
class="math inline">\(\boldsymbol{y} \in
\mathbb{R}^{\mathrm{OutDims}}\)</span>ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚ <span
class="math display">\[
  \boldsymbol{y} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}
\]</span></p>
<p><code>torch.nn.BatchNorm1d</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ã¯ã€å¹³å‡ã€æ¨™æº–åå·®ã€é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã®4ã¤ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
å…¥å‡ºåŠ›ã®æ¬¡å…ƒã‚’<span
class="math inline">\(\mathrm{Dims}\)</span>ã¨ã™ã‚‹ã¨ã€ã“ã‚Œã‚‰4ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¯<span
class="math inline">\((\mathrm{Dims})\)</span>ã§ã™ã€‚
å¹³å‡ã€æ¨™æº–åå·®ã€é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹<span
class="math inline">\(\boldsymbol{\mu}, \boldsymbol{\sigma},
\boldsymbol{w}, \boldsymbol{b} \in
\mathbb{R}^{\mathrm{Dims}}\)</span>ãŒã‚ã‚‹ã¨ãã€å…¥åŠ›<span
class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^{\mathrm{Dims}}\)</span>ã«å¯¾ã—ã¦å‡ºåŠ›<span
class="math inline">\(\boldsymbol{y} \in
\mathbb{R}^{\mathrm{Dims}}\)</span>ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚ <span
class="math display">\[
  y_i = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \varepsilon}} \cdot w_i +
b_i \quad (i = 1, \ldots, \mathrm{Dims})
\]</span> <span
class="math inline">\(\varepsilon\)</span>ã¯ã€ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ããŸã‚ã®å°ã•ãªæ­£ã®å€¤ã§ã™ã€‚
<span class="math inline">\(x_i\)</span>ã¯ã€<span
class="math inline">\(\boldsymbol{x}\)</span>ã®ç¬¬<span
class="math inline">\(i\)</span>è¦ç´ ã§ã™ (ä»–ã‚‚åŒæ§˜)ã€‚
ä¸Šè¨˜ã‚’ã¿ã‚‹ã¨ã€<span class="math inline">\(w_i / \sqrt{\sigma_i^2 +
\varepsilon}\)</span>ã®éƒ¨åˆ†ã‚’å…ˆã«è¨ˆç®—ã§ãã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ <span
class="math inline">\(\boldsymbol{w}\)</span>ã¨<span
class="math inline">\(\boldsymbol{\sigma}\)</span>ã®ä¸¡æ–¹ã‚’ä½¿ã†å ´åˆã¨æ¯”ã¹ã¦ã€é™¤ç®—ãŠã‚ˆã³å¹³æ–¹æ ¹ã®è¨ˆç®—ã‚’çœç•¥ã§ãã¾ã™ã€‚
ã¾ãŸã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚
ç´°ã‹ã„è©±ã«ã¿ãˆã¾ã™ãŒã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®å¤§ããªFPGAä¸Šã«å®Ÿè£…ã™ã‚‹å ´åˆã¯é‡è¦ã§ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨ˆç®—ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚ <span class="math display">\[
  y_i = \left( x_i - \mu_i \right) \cdot s_i + b_i \quad (i = 1, \ldots,
\mathrm{Dims})
\]</span> ä¸Šè¨˜ã®<span
class="math inline">\(s_i\)</span>ã‚’ã€ã“ã“ã§ã¯<strong>ã‚¹ã‚±ãƒ¼ãƒ«</strong>ã¨å‘¼ã¶ã“ã¨ã«ã—ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€å¹³å‡<span
class="math inline">\(\boldsymbol{\mu}\)</span>ã€ãƒã‚¤ã‚¢ã‚¹<span
class="math inline">\(\boldsymbol{b}\)</span>ã€ã‚¹ã‚±ãƒ¼ãƒ«<span
class="math inline">\(\boldsymbol{s} \in
\mathbb{R}^{\mathrm{Dims}}\)</span>ã®3ã¤ã«ãªã‚Šã¾ã™ã€‚ <span
class="math inline">\(\boldsymbol{s}\)</span>ã®è¨ˆç®—ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–æ™‚ã«ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ä¸Šã§è¡Œã†ã“ã¨ã«ã—ã¾ã™ã€‚</p>
<p>ãƒãƒƒãƒæ­£è¦åŒ–ã®å¾Œã«ReLUæ´»æ€§åŒ–ãŒè¨ˆç®—ã•ã‚Œã¾ã™ã€‚
å„å±¤ã‚’åˆ¥ã€…ã«å®Ÿè£…ã™ã‚‹ã‚ˆã‚Šã‚‚ã€ã¾ã¨ã‚ã¦ã—ã¾ã£ãŸæ–¹ãŒåŠ¹ç‡ãŒã‚ˆã„ã®ã§ã€ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã‚’æ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã¾ã™
(<strong>æœ€é©åŒ–ãã®3: è¨ˆç®—ã®ç°¡ç•¥åŒ–</strong>)ã€‚ <span
class="math display">\[
  y_i = \max \left( 0, \left( x_i - \mu_i \right) \cdot s_i + b_i
\right) \quad (i = 1, \ldots, \mathrm{Dims})
\]</span></p>
<p>æœ€å¾Œã«Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã§ã™ãŒã€å…ˆè¿°ã®é€šã‚Šã€å„ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\psi}_i \in
\mathbb{R}^{1024}\)</span>ã¨ã€ç¾åœ¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡<span
class="math inline">\(\boldsymbol{\phi} \in
\mathbb{R}^{1024}\)</span>ã¨ã®ã€è¦ç´ ã”ã¨ã®<span
class="math inline">\(\max\)</span>ã«ç½®ãæ›ãˆã¾ã—ãŸã€‚
Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®è¨ˆç®—ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ <span
class="math display">\[
  \phi_i = \max \left( \phi_i, \psi_i \right) \quad (i = 1, \ldots,
1024)
\]</span></p>
<p>ã•ã¦ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®<code>LinearParams&lt;T, InDims_, OutDims_&gt;</code>æ§‹é€ ä½“ã¨ã€<code>BatchNorm1dParams&lt;T, Dims_&gt;</code>æ§‹é€ ä½“ã¯ã€å…¨çµåˆå±¤
(<code>Conv1d</code>ãŠã‚ˆã³<code>Linear</code>) ã¨ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤
(<code>BatchNorm1d</code>) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã‚Œãã‚Œã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parameters for fully-connected layers</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> InDims_<span class="op">,</span> <span class="dt">int</span> OutDims_<span class="op">&gt;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> LinearParams</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">enum</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    InDims <span class="op">=</span> InDims_<span class="op">,</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    OutDims <span class="op">=</span> OutDims_<span class="op">,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  T weight<span class="op">[</span>OutDims<span class="op">][</span>InDims<span class="op">];</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  T bias<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Parameters for 1D batch normalization layers</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> Dims_<span class="op">&gt;</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> BatchNorm1dParams</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">enum</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    Dims <span class="op">=</span> Dims_<span class="op">,</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `scale` is obtained by multiplying weights and reciprocal of the</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">// square root of the standard deviation (to reduce the computational cost)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  T scale<span class="op">[</span>Dims<span class="op">];</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  T bias<span class="op">[</span>Dims<span class="op">];</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  T mean<span class="op">[</span>Dims<span class="op">];</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div>
<p><code>PointNetClsTop</code>å†…ã§ã¯ã€PyTorchã§å®šç¾©ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å„å±¤ã«å¯¾å¿œã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå®£è¨€ã•ã‚Œã¾ã™ã€‚</p>
<ul>
<li><code>feat_conv1</code>:
<code>PointNetFeat::conv1</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>feat_conv2</code>:
<code>PointNetFeat::conv2</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>feat_conv3</code>:
<code>PointNetFeat::conv3</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>feat_conv4</code>:
<code>PointNetFeat::conv4</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>feat_conv5</code>:
<code>PointNetFeat::conv5</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>feat_bn1</code>:
<code>PointNetFeat::bn1</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>feat_bn2</code>:
<code>PointNetFeat::bn2</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>feat_bn3</code>:
<code>PointNetFeat::bn3</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>feat_bn4</code>:
<code>PointNetFeat::bn4</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>feat_bn5</code>:
<code>PointNetFeat::bn5</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>cls_fc3</code>:
<code>PointNetCls::fc3</code>ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹</li>
<li><code>cls_bn1</code>:
<code>PointNetCls::bn1</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
<li><code>cls_bn2</code>:
<code>PointNetCls::bn2</code>ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«</li>
</ul>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ã¦ã®å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€æ¨è«–ã‚’é–‹å§‹ã™ã‚‹å‰ã«äºˆã‚ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ç½®ã„ã¦ãŠãã¾ã™ã€‚
ä¸€æ–¹ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨çµåˆå±¤2ã¤
(<code>PointNetCls::fc1</code>ã€<code>PointNetCls::fc2</code>)
ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ã¯ç½®ã‹ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ããã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹ãŸã‚ã§ã™ã€‚
ã“ã‚Œã‚‰ã®å±¤ã«ã¤ã„ã¦ã¯ã€æ¨è«–æ™‚ã«DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å‡ºã—ã¾ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å–ã‚Šå‡ºã—ã¦ã€å‡ºåŠ›ã®ä¸€éƒ¨ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚
ä¸€éƒ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«ã€å°ã•ãªã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã‚’ç”¨æ„ã™ã‚Œã°ã‚ˆããªã‚Šã¾ã™ã€‚</p>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯ã€<span
class="math inline">\(N\)</span>å€‹å…¨ã¦ã®ç‚¹ã«å¯¾ã—ã¦ç‰¹å¾´æŠ½å‡ºã‚’è¡Œã†ãŸã‚ã«ã€<span
class="math inline">\(N\)</span>å›ã®é †ä¼æ’­ãŒèµ·ã“ã‚Šã¾ã™ã€‚
æ¨è«–æ™‚é–“ã®ãªã‹ã§å ã‚ã‚‹å‰²åˆãŒå¤§ãã„ã®ã§ã€1å›ã®é †ä¼æ’­ã«è¦ã™ã‚‹è¨ˆç®—æ™‚é–“ã‚’ã†ã¾ãçŸ­ç¸®ã§ãã‚Œã°ã€å…¨ä½“ã®æ¨è«–æ™‚é–“ã®å¤§å¹…ãªçŸ­ç¸®ã«ã¤ãªãŒã‚Šã¾ã™
(<strong>ã‚¢ãƒ ãƒ€ãƒ¼ãƒ«ã®æ³•å‰‡</strong>)ã€‚
ä¸€æ–¹ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é †ä¼æ’­ã¯1åº¦ã ã‘ã§ã€æ¨è«–æ™‚é–“ã®ãªã‹ã§ã¯ãã‚Œã»ã©é‡è¦ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã«äº‹å‰ã«æ ¼ç´ã™ã‚‹ã®ã¨æ¯”ã¹ã¦ã€æ¨è«–æ™‚ã«DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å‡ºã™ã¨ã€å±¤ã®è¨ˆç®—æ™‚é–“ã¯ä¼¸ã³ã¦ã—ã¾ã„ã¾ã™ãŒã€æ¨è«–æ™‚é–“ã«ä¸ãˆã‚‹å½±éŸ¿ã¯ãã‚Œã»ã©å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚</p>
<h2 id="ãƒ‡ãƒ¼ã‚¿å‹">ãƒ‡ãƒ¼ã‚¿å‹</h2>
<p>Vitis
HLSã§ã¯ã€ä»»æ„ç²¾åº¦ã®<strong>å›ºå®š</strong>å°æ•°ç‚¹æ•°å‹<code>ap_fixed</code>ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
å˜ç²¾åº¦æµ®å‹•å°æ•°ç‚¹æ•°<code>float</code>ã‚„ã€åŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹æ•°<code>half</code>ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚
ã“ã“ã§ã¯ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«ã€å›ºå®šå°æ•°ç‚¹æ•°ã‚’ä½¿ã„ã¾ã™ã€‚</p>
<p>ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰ (<code>ap_o_mode::AP_WRAP</code>)
ã§ã¯ã€å€¤ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã—ãŸã¨ãã«æŠ˜ã‚Šè¿”ã—ã¾ã™ã€‚
ã“ã‚Œã ã¨ã€æœ€å¤§å€¤ã‹ã‚‰æ€¥ã«æœ€å°å€¤ã«ãªã£ãŸã‚Šã—ã¦å±ãªã£ã‹ã—ã„ã®ã§ã€æœ€å¤§å€¤ã‚ã‚‹ã„ã¯æœ€å°å€¤ã«ç•™ã¾ã‚Šç¶šã‘ã‚‹ã‚ˆã†ã«ã€é£½å’Œãƒ¢ãƒ¼ãƒ‰
(<code>ap_o_mode::AP_SAT</code>) ã«å¤‰æ›´ã—ã¦ã„ã¾ã™ã€‚
é£½å’Œãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã†å›ºå®šå°æ•°ç‚¹æ•°å‹ã‚’ã€<code>ap_fixed_sat</code>ã¨ã—ã¦å®šç¾©ã—ã¾ã—ãŸã€‚</p>
<p>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å…¥å‡ºåŠ›ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã§ãƒ“ãƒƒãƒˆå¹…ã‚’å¤‰ãˆã‚‹ãŸã‚ã«ã€å…¥å‡ºåŠ›ç”¨ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã«åˆ¥ã€…ã®å‹ã‚’ç”¨æ„ã—ã¾ã—ãŸ
(<code>param_t</code>ãŠã‚ˆã³<code>value_t</code>)ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤åŸŸã«åˆã‚ã›ã¦ã€ãƒ“ãƒƒãƒˆå¹…ã‚’å‰Šæ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
ãƒ“ãƒƒãƒˆå¹…ã®å‰Šæ¸›ã‚„é‡å­åŒ–ã€å°æ•°ç‚¹å‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãªã©ã¯ã€ãã‚Œè‡ªä½“ãŒç«‹æ´¾ãªç ”ç©¶åˆ†é‡ã¨ãªã£ã¦ã„ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Value types</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="dt">int</span> _AP_W<span class="op">,</span> <span class="dt">int</span> _AP_I<span class="op">&gt;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">using</span> ap_fixed_sat <span class="op">=</span> ap_fixed<span class="op">&lt;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  _AP_W<span class="op">,</span> _AP_I<span class="op">,</span> ap_q_mode<span class="op">::</span>AP_TRN<span class="op">,</span> ap_o_mode<span class="op">::</span>AP_SAT<span class="op">,</span> <span class="dv">0</span><span class="op">&gt;;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Data type for values (layer inputs, outputs, and intermediate results)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">using</span> <span class="dt">value_t</span> <span class="op">=</span> ap_fixed_sat<span class="op">&lt;</span>kValueBitWidth<span class="op">,</span> kValueIntWidth<span class="op">&gt;;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">// Data type for network parameters</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">using</span> <span class="dt">param_t</span> <span class="op">=</span> ap_fixed_sat<span class="op">&lt;</span>kParamBitWidth<span class="op">,</span> kParamIntWidth<span class="op">&gt;;</span></span></code></pre></div>
<h2 id="å‹•ä½œãƒ¢ãƒ¼ãƒ‰">å‹•ä½œãƒ¢ãƒ¼ãƒ‰</h2>
<p>ã•ã¦ã€ã“ã“ã§ç¤ºã™IPã‚³ã‚¢ã«ã¯ã€2ã¤ã®<strong>å‹•ä½œãƒ¢ãƒ¼ãƒ‰</strong>
(Operation mode) ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
<ul>
<li>é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ (<code>kModeInitWeights</code>):
é‡ã¿ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã£ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã™ã‚‹ã€‚</li>
<li>æ¨è«–ãƒ¢ãƒ¼ãƒ‰ (<code>kModeInference</code>):
å…¥åŠ›ç‚¹ç¾¤ã‹ã‚‰ã€å„ã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹ã€‚</li>
</ul>
<p>ã“ã‚Œã‚‰ã‚’é †ã«èª¬æ˜ã—ã¾ã™ã€‚</p>
<h3 id="é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰">é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰</h3>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã£ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã—ã¾ã™ã€‚
ä»¥ä¸‹ã«ç¤ºã™ã€<code>InitializeFeatNaive</code>ãŠã‚ˆã³<code>InitializeClsNaive</code>ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãã‚Œãã‚Œã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãŸã‚ã®é–¢æ•°ã§ã™ã€‚</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the parameter initialization</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InitializeFeatNaive<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> conv1<span class="op">,</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                         LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> conv2<span class="op">,</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                         LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> conv3<span class="op">,</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                         LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> conv4<span class="op">,</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                         LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> conv5<span class="op">,</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> bn3<span class="op">,</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> bn4<span class="op">,</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> bn5<span class="op">,</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params1<span class="op">,</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params2<span class="op">,</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params3<span class="op">,</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params4<span class="op">,</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params5<span class="op">)</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;(</span>conv1<span class="op">,</span> bn1<span class="op">,</span> params1<span class="op">);</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;(</span>conv2<span class="op">,</span> bn2<span class="op">,</span> params2<span class="op">);</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;(</span>conv3<span class="op">,</span> bn3<span class="op">,</span> params3<span class="op">);</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;(</span>conv4<span class="op">,</span> bn4<span class="op">,</span> params4<span class="op">);</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>conv5<span class="op">,</span> bn5<span class="op">,</span> params5<span class="op">);</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the parameter initialization</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InitializeClsNaive<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;*</span> fc3<span class="op">,</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params1<span class="op">,</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params2<span class="op">,</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params3<span class="op">)</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>  ReadBatchNorm1dParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kClsDims1<span class="op">&gt;(</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    bn1<span class="op">,</span> params1<span class="op">,</span> kClsDims0 <span class="op">*</span> kClsDims1 <span class="op">+</span> kClsDims1<span class="op">);</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>  ReadBatchNorm1dParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">&gt;(</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    bn2<span class="op">,</span> params2<span class="op">,</span> kClsDims1 <span class="op">*</span> kClsDims2 <span class="op">+</span> kClsDims2<span class="op">);</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>  ReadLinearParamsNaive<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;(</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    fc3<span class="op">,</span> params3<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>ã“ã‚Œã‚‰ã®é–¢æ•°ã®ãªã‹ã§ã¯ã€<code>ReadBlockParamsNaive</code>ã€<code>ReadLinearParamsNaive</code>ã€ãã—ã¦<code>ReadBatchNorm1dParamsNaive</code>ã®3ã¤ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™ã€‚
å„é–¢æ•°ã¯æ¬¡ã®ã‚ˆã†ãªå‹•ä½œã§ã™ (è©³ç´°ã¯ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ã”å‚ç…§ãã ã•ã„)ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã«ã¯<code>float</code>å‹ã§ç½®ã‹ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚’å›ºå®šå°æ•°ç‚¹æ•°å‹ã«ç›´ã™å‡¦ç†ã‚‚å«ã¾ã‚Œã¾ã™ã€‚</p>
<ul>
<li><code>ReadLinearParamsNaive&lt;T, InDims, OutDims&gt;</code>:
DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€å…¨çµåˆå±¤
(<code>Conv1d</code>ãŠã‚ˆã³<code>Linear</code>)
ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’èª­ã¿å–ã‚‹ã€‚
é‡ã¿ã®ã‚µã‚¤ã‚ºã¯<code>(OutDims, InDims)</code>ã€ãƒã‚¤ã‚¢ã‚¹ã®ã‚µã‚¤ã‚ºã¯<code>(OutDims)</code>ã§ã‚ã‚‹ã€‚
2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹
(é…åˆ—ã®ã‚µã‚¤ã‚ºã¯<code>OutDims * InDims + OutDims</code>)ã€‚</li>
<li><code>ReadBatchNorm1dParamsNaive&lt;T, Dims&gt;</code>:
DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ (<code>BatchNorm1d</code>)
ã®ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã‚’èª­ã¿å–ã‚‹ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¯<code>(Dims)</code>ã§ã‚ã‚‹ã€‚
3ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹
(é…åˆ—ã®ã‚µã‚¤ã‚ºã¯<code>3 * Dims</code>)ã€‚</li>
<li><code>ReadBlockParamsNaive&lt;T, InDims, OutDims</code>:
DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€å…¨çµåˆå±¤ãŠã‚ˆã³ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿5ã¤ã‚’èª­ã¿å–ã‚‹ã€‚
5ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹
(é…åˆ—ã®ã‚µã‚¤ã‚ºã¯<code>OutDims * InDims + 4 * OutDims</code>)ã€‚</li>
</ul>
<h3 id="æ¨è«–ãƒ¢ãƒ¼ãƒ‰">æ¨è«–ãƒ¢ãƒ¼ãƒ‰</h3>
<p>å…¥åŠ›ç‚¹ç¾¤ã‹ã‚‰ã€å„ã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ä»¥ä¸‹ã«ç¤ºã™ã€<code>InferenceFeatNaive</code>ãŠã‚ˆã³<code>InferenceClsNaive</code>ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãã‚Œãã‚Œã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡¦ç†ã§ã™ã€‚</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the PointNet feature extraction</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for layer input, output, and intermediate results</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">// `U` is the type for parameters</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `N` is the expected number of input points (e.g., 1024)</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">,</span> <span class="dt">int</span> N<span class="op">&gt;</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceFeatNaive<span class="op">(</span><span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                        T feature<span class="op">[</span>kFeatDims5<span class="op">],</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> conv1<span class="op">,</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> conv2<span class="op">,</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> conv3<span class="op">,</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> conv4<span class="op">,</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> conv5<span class="op">,</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> bn3<span class="op">,</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> bn4<span class="op">,</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> bn5<span class="op">)</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Zero-initialize the output feature</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>  VectorNdSetZero<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>feature<span class="op">);</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute the feature</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> num_points<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_FLATTEN off</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Input, output, and intermediate results</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    T x0<span class="op">[</span>kFeatDims0<span class="op">];</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    T x1<span class="op">[</span>kFeatDims1<span class="op">];</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    T x2<span class="op">[</span>kFeatDims1<span class="op">];</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    T x3<span class="op">[</span>kFeatDims2<span class="op">];</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    T x4<span class="op">[</span>kFeatDims2<span class="op">];</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    T x5<span class="op">[</span>kFeatDims3<span class="op">];</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    T x6<span class="op">[</span>kFeatDims3<span class="op">];</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    T x7<span class="op">[</span>kFeatDims4<span class="op">];</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    T x8<span class="op">[</span>kFeatDims4<span class="op">];</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    T x9<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    T x10<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Read a point from a DDR memory</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    ReadPointNaive<span class="op">&lt;</span>T<span class="op">&gt;(</span>point_cloud<span class="op">,</span> i<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute a point feature</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>      x0<span class="op">,</span> x1<span class="op">,</span> conv1<span class="op">-&gt;</span>weight<span class="op">,</span> conv1<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">&gt;(</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>      x1<span class="op">,</span> x2<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>      x2<span class="op">,</span> x3<span class="op">,</span> conv2<span class="op">-&gt;</span>weight<span class="op">,</span> conv2<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">&gt;(</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>      x3<span class="op">,</span> x4<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>      x4<span class="op">,</span> x5<span class="op">,</span> conv3<span class="op">-&gt;</span>weight<span class="op">,</span> conv3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">&gt;(</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>      x5<span class="op">,</span> x6<span class="op">,</span> bn3<span class="op">-&gt;</span>scale<span class="op">,</span> bn3<span class="op">-&gt;</span>bias<span class="op">,</span> bn3<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>      x6<span class="op">,</span> x7<span class="op">,</span> conv4<span class="op">-&gt;</span>weight<span class="op">,</span> conv4<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">&gt;(</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>      x7<span class="op">,</span> x8<span class="op">,</span> bn4<span class="op">-&gt;</span>scale<span class="op">,</span> bn4<span class="op">-&gt;</span>bias<span class="op">,</span> bn4<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>    LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>      x8<span class="op">,</span> x9<span class="op">,</span> conv5<span class="op">-&gt;</span>weight<span class="op">,</span> conv5<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>      x9<span class="op">,</span> x10<span class="op">,</span> bn5<span class="op">-&gt;</span>scale<span class="op">,</span> bn5<span class="op">-&gt;</span>bias<span class="op">,</span> bn5<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Update the output feature</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    MaxPool1dNaive<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>x10<span class="op">,</span> feature<span class="op">);</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the classification network</span></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for layer input, output, and intermediate results</span></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a><span class="co">// `U` is the type for parameters</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">&gt;</span></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceClsNaive<span class="op">(</span><span class="at">const</span> T feature<span class="op">[</span>kFeatDims5<span class="op">],</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>                       <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;*</span> fc3<span class="op">,</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params1<span class="op">,</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params2<span class="op">,</span></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params3<span class="op">)</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>kFeatDims5 <span class="op">==</span> kClsDims0<span class="op">,</span></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;Feature dimension should be equal to the input dimension&quot;</span><span class="op">);</span></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Input, output, and intermediate results</span></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>  T x0<span class="op">[</span>kClsDims1<span class="op">];</span></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>  T x1<span class="op">[</span>kClsDims1<span class="op">];</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>  T x2<span class="op">[</span>kClsDims2<span class="op">];</span></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>  T x3<span class="op">[</span>kClsDims2<span class="op">];</span></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>  T x4<span class="op">[</span>kClsDims3<span class="op">];</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute logits</span></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>  LinearNaiveDDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims0<span class="op">,</span> kClsDims1<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>    feature<span class="op">,</span> x0<span class="op">,</span> params1<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">&gt;(</span></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">,</span> x1<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>  LinearNaiveDDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">,</span> kClsDims2<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>    x1<span class="op">,</span> x2<span class="op">,</span> params2<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">&gt;(</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">,</span> x3<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>  LinearNaive<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">,</span> <span class="kw">false</span><span class="op">&gt;(</span></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>    x3<span class="op">,</span> x4<span class="op">,</span> fc3<span class="op">-&gt;</span>weight<span class="op">,</span> fc3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Write the result</span></span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>  WriteTensor1dNaive<span class="op">&lt;</span>T<span class="op">,</span> kClsDims3<span class="op">&gt;(</span>out_logits<span class="op">,</span> x4<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>InferenceFeatNaive</code>ã§ã¯ã€DRAMã«ç½®ã‹ã‚ŒãŸç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿
(<code>point_cloud</code>) ã‹ã‚‰ã€1ã¤ãšã¤ç‚¹ã‚’èª­ã¿å–ã‚Šã¾ã™ã€‚ å„ç‚¹
(<code>x0</code>) ã«å¯¾ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ (<code>x10</code>)
ã‚’è¨ˆç®—ã—ã€ç¾åœ¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡ (<code>feature</code>)
ã‚’æ›´æ–°ã™ã‚‹å‡¦ç†ã‚’ã€ç‚¹ã®å€‹æ•° (<code>num_points</code>) ã ã‘ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚
<code>InferenceClsNaive</code>ã¯ã€ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡
(<code>feature</code>) ã‚’å—ã‘å–ã£ã¦ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ
(<code>x4</code>) ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’DRAMãƒãƒƒãƒ•ã‚¡ (<code>out_logits</code>)
ã«æ›¸ãæˆ»ã—ã¾ã™ã€‚</p>
<p><code>ReadPointNaive</code>ã¯ã€<span
class="math inline">\(i\)</span>ç•ªç›®ã®ç‚¹<span
class="math inline">\(\boldsymbol{p}_i\)</span>ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã‚‹ã‚‚ã®ã§ã™ã€‚
<code>LinearNaive</code>ã€<code>BatchNorm1dReLUNaive</code>ã€<code>MaxPool1dNaive</code>ã¯ã€åå‰ã®é€šã‚Šã€å…¨çµåˆå±¤
(<code>Conv1d</code>)ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨ReLUæ´»æ€§åŒ–ã€Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«å¯¾å¿œã—ã¾ã™
(å…ˆç¨‹ã®è¨ˆç®—å¼ã‚’å‚ç…§)ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å‡ºã—ã¦ã€å±¤ã®å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
<code>LinearNaiveDDR</code>ã‚‚å…¨çµåˆå±¤ã®é–¢æ•°ã§ã™ãŒã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°‘ã—ãšã¤å–ã‚Šå‡ºã—ã¤ã¤ã€å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®é–¢æ•°ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚
HLSãƒ—ãƒ©ã‚°ãƒã‚’é™¤ã‘ã°ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å®Ÿè£…ã¨å¤§ä½“åŒã˜ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
è¡Œæ•°ã¯å¤šã„ã§ã™ãŒã€å‡¦ç†å†…å®¹ã¯å˜ç´”ã§ã™ã€‚</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the fully-connected layer</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">// `TParam` is the type for weight and bias</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">// `ApplyReLU` is the flag to apply ReLU activation</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>          <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">,</span> <span class="dt">bool</span> ApplyReLU<span class="op">&gt;</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> LinearNaive<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>InDims<span class="op">],</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                 T y<span class="op">[</span>OutDims<span class="op">],</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">const</span> TParam weight<span class="op">[</span>OutDims<span class="op">][</span>InDims<span class="op">],</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">const</span> TParam bias<span class="op">[</span>OutDims<span class="op">])</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> OutDims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE off</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    T val <span class="op">=</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>      val <span class="op">+=</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i<span class="op">][</span>j<span class="op">];</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> val <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val<span class="op">;</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the fully-connected layer</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co">// Weight and bias parameters are stored on the DDR memory</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>          <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">,</span> <span class="dt">bool</span> ApplyReLU<span class="op">&gt;</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> LinearNaiveDDR<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>InDims<span class="op">],</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>                    T y<span class="op">[</span>OutDims<span class="op">],</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params<span class="op">,</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `params` contains weight parameters of size (`OutDims`, `InDims`) and</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bias parameters of size (`OutDims`) in a contiguous buffer</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> OffsetToBias <span class="op">=</span> OutDims <span class="op">*</span> InDims<span class="op">;</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>  TParam bias<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Copy the bias parameters in advance</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> OutDims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>params<span class="op">[</span>offset <span class="op">+</span> OffsetToBias <span class="op">+</span> i<span class="op">]);</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> OutDims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE off</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    T val <span class="op">=</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>    TParam weight<span class="op">[</span>InDims<span class="op">];</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>      weight<span class="op">[</span>j<span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>params<span class="op">[</span>offset <span class="op">+</span> i <span class="op">*</span> InDims <span class="op">+</span> j<span class="op">]);</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>      val <span class="op">+=</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>j<span class="op">];</span></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> val <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val<span class="op">;</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the 1D batch normalization and ReLU activation</span></span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a><span class="co">// `TParam` is the type for parameters</span></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a><span class="co">// `Dims` is the number of input and output dimensions</span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span> <span class="dt">int</span> Dims<span class="op">&gt;</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> BatchNorm1dReLUNaive<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>                          T y<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>                          <span class="at">const</span> TParam scale<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>                          <span class="at">const</span> TParam bias<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>                          <span class="at">const</span> TParam mean<span class="op">[</span>Dims<span class="op">])</span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> Dims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Batch normalization with the learned parameters</span></span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>    T val <span class="op">=</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">[</span>i<span class="op">])</span> <span class="op">*</span> scale<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ReLU activation</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>    y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> val <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a><span class="co">// Naive implementation of the 1D max-pooling layer</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a><span class="co">// `Dims` is the number of input and output dimensions</span></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a><span class="co">// `y` must be properly initialized</span></span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> Dims<span class="op">&gt;</span></span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> MaxPool1dNaive<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>Dims<span class="op">],</span> T y<span class="op">[</span>Dims<span class="op">])</span></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `x` is of size (1, `Dims`)</span></span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `y` is of size (1, `Dims`)</span></span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> Dims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>    y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">&gt;</span> y<span class="op">[</span>i<span class="op">]</span> <span class="op">?</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">:</span> y<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>LinearNaiveDDR</code>ã§ã¯ã€å…¨çµåˆå±¤ã®ãƒã‚¤ã‚¢ã‚¹é …
<code>bias</code>ã¨ã€å‡ºåŠ›1è¦ç´ åˆ†ã®è¨ˆç®—ã«å¿…è¦ãªé‡ã¿
<code>weight</code>ã ã‘ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ä¿æŒã—ã¾ã™ã€‚
å…¥å‡ºåŠ›ã®æ¬¡å…ƒã‚’<span class="math inline">\(\mathrm{InDims},
\mathrm{OutDims}\)</span>ã¨ã™ã‚Œã°ã€<code>bias</code>ã®ã‚µã‚¤ã‚ºã¯<span
class="math inline">\(\mathrm{OutDims}\)</span>ã€<code>weight</code>ã®ã‚µã‚¤ã‚ºã¯<span
class="math inline">\(\mathrm{InDims}\)</span>ã¨ãªã‚Šã¾ã™ã€‚</p>
<p>ä¸Šè¨˜ã®é–¢æ•°ã®ãƒ«ãƒ¼ãƒ—ã«ã¯<code>#pragma HLS PIPELINE</code>ãŒä»˜åŠ ã•ã‚Œã¦ãŠã‚Šã€ãƒ«ãƒ¼ãƒ—å†…éƒ¨ã®å‡¦ç†ãŒè‡ªå‹•çš„ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã•ã‚Œã¾ã™
(<strong>æœ€é©åŒ–ãã®4: ãƒ«ãƒ¼ãƒ—ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–</strong>)ã€‚
<code>#pragma HLS PIPELINE off</code>ã¨ã™ã‚‹ã¨ã€ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ãŒæŠ‘åˆ¶ã•ã‚Œã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’ã€ä»¥ä¸‹ã®å›³ã«ç¤ºã—ã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pipelined-execution.svg"><img src="point-cloud-classification-images/pipelined-execution.svg" width="70%" /></a></p>
<p>ãƒ«ãƒ¼ãƒ—ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ãªã„å ´åˆã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é †ã«å®Ÿè¡Œã—ã¾ã™
(å›³ã®ä¸Šéƒ¨)ã€‚ ä¸€æ–¹ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã§ã¯ã€ãƒ«ãƒ¼ãƒ—å†…éƒ¨ã®å‡¦ç†ã‚’åˆ†å‰²
(å›³ã®å ´åˆã¯4åˆ†å‰²) ã—ã€ãã‚Œãã‚Œã®å‡¦ç†ã‚’æ™‚é–“çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¾ã™
(å›³ã®ä¸‹éƒ¨)ã€‚
è¤‡æ•°ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ã®ã§ã€ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã¯ã€æœ€ã‚‚æ™‚é–“ã®æ›ã‹ã‚‹å‡¦ç† (å›³ã®å ´åˆã¯å‡¦ç†3)
ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã¾ã™ã€‚
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å‡¦ç†ã‚’ã€ãªã‚‹ã¹ãå‡ç­‰ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã®åŠ¹æœãŒå¢—ã—ã¾ã™ã€‚
ä¸Šè¨˜ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«ã€æœ€å†…ãƒ«ãƒ¼ãƒ—ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã™ã‚‹ã¨ã€å‡¦ç†æ™‚é–“ã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ã€‚
2é‡ãƒ«ãƒ¼ãƒ—ã®ã†ã¡å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã™ã‚‹ã¨ã€å†…å´ã®ãƒ«ãƒ¼ãƒ—ã¯å…¨ã¦å±•é–‹ã•ã‚Œã¦ã€1é‡ãƒ«ãƒ¼ãƒ—ã«ç›´ã•ã‚Œã‚‹ã®ã§ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ãŒå¤§å¹…ã«å¢—ãˆã¦ã—ã¾ã„ã¾ã™ã€‚
å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã«ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã—ãªã„æ–¹ãŒã„ã„ã¨æ€ã„ã¾ã™ã€‚</p>
<p>ä¸Šè¨˜ã®IPã‚³ã‚¢ã¯ã€<code>hls/src/top_naive.cpp</code>ã«ã‚ã‚Šã¾ã™ã€‚</p>
<h2 id="ä¸¦åˆ—åŒ–-ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã®æ´»ç”¨">ä¸¦åˆ—åŒ– (ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã®æ´»ç”¨)</h2>
<p>ã“ã®IPã‚³ã‚¢ã‚‚æ­£ã—ãå‹•ä½œã™ã‚‹ã®ã§ã™ãŒã€æ˜ã‚‰ã‹ã«ãƒŠã‚¤ãƒ¼ãƒ–ãª
(å…¨ãå·¥å¤«ã—ã¦ã„ãªã„ç´ æœ´ãª) å®Ÿè£…ã§ã™ã€‚ ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (Data parallelism)
ã‚’æ´»ã‹ã—ã¦ã€å„å±¤ã®è¨ˆç®—ã‚’ä¸¦åˆ—åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã† (<strong>æœ€é©åŒ–ãã®5:
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§</strong>)ã€‚</p>
<p>å…¨çµåˆå±¤ã®è¨ˆç®—ã‚’ã‚‚ã†ä¸€åº¦ã¿ã¦ã¿ã¾ã™ã€‚ <span class="math display">\[
  \boldsymbol{y} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}
\]</span> å‡ºåŠ›<span
class="math inline">\(\boldsymbol{y}\)</span>ã®å„è¦ç´ <span
class="math inline">\(y_i\)</span>ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚ <span
class="math display">\[
  y_i = \sum_j W_{i, j} x_j + b_i
\]</span> <span class="math inline">\(B\)</span>å€‹ã®å‡ºåŠ›è¦ç´ <span
class="math inline">\(y_i, y_{i + 1}, \ldots, y_{i + B -
1}\)</span>ã®é–“ã«ã¯ä¾å­˜ãŒãªã„ã®ã§
(ãã‚Œãã‚Œã®è¦ç´ ã¯äº’ã„ã«ä¾å­˜ã›ãšç‹¬ç«‹ã«è¨ˆç®—ã§ãã‚‹ã®ã§)ã€ä¸¦åˆ—ã«è¨ˆç®—ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
<span class="math display">\[
  \begin{eqnarray}
    y_i &amp;=&amp; \sum_j W_{i, j} x_j + b_i \\
    y_{i + 1} &amp;=&amp; \sum_j W_{i + 1, j} x_j + b_{i + 1} \\
    &amp;\vdots&amp; \\
    y_{i + B - 1} &amp;=&amp; \sum_j W_{i + B - 1, j} x_j + b_{i + B -
1}
  \end{eqnarray}
\]</span> <span class="math inline">\(W_{i, j} x_j, W_{i + 1, j} x_j,
\ldots, W_{i + B - 1, j} x_j\)</span>ã®<span
class="math inline">\(B\)</span>å€‹ã®ç©ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã‚ã‘ã§ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€<span class="math inline">\(j\)</span> (å…¥åŠ›æ¬¡å…ƒ)
ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã¯ãã®ã¾ã¾ã«ã—ã¦ã€<span class="math inline">\(i\)</span>
(å‡ºåŠ›æ¬¡å…ƒ) ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ <span
class="math inline">\(B\)</span>å€‹ã®å‡ºåŠ›ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ã®ã§ã€<span
class="math inline">\(B\)</span>å€ã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™
(ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚‚<span class="math inline">\(B\)</span>å€ã«ãªã‚Šã¾ã™)ã€‚</p>
<p>ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã«ã€è¤‡æ•°ã®å‡ºåŠ›è¦ç´ <span
class="math inline">\(y_i, y_{i + 1}, \ldots, y_{i + B -
1}\)</span>ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã—ã¾ã™ã€‚ <span class="math display">\[
  \begin{eqnarray}
    y_i &amp;=&amp; \max \left( 0, \left( x_i - \mu_i \right) \cdot s_i
+ b_i \right) \\
    y_{i + 1} &amp;=&amp; \max \left( 0, \left( x_{i + 1} - \mu_{i + 1}
\right) \cdot s_{i + 1} + b_{i + 1} \right) \\
    &amp;\vdots&amp; \\
    y_{i + B - 1} &amp;=&amp; \max \left( 0, \left( x_{i + B - 1} -
\mu_{i + B - 1} \right) \cdot s_{i + B - 1} + b_{i + B - 1} \right)
  \end{eqnarray}
\]</span></p>
<p>Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã¤ã„ã¦ã‚‚å…¨ãåŒã˜ã§ã€è¤‡æ•°ã®å‡ºåŠ›è¦ç´ <span
class="math inline">\(\phi_i, \phi_{i + 1}, \ldots, \phi_{i + B -
1}\)</span>ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã—ã¾ã™ã€‚ <span class="math display">\[
  \begin{eqnarray}
    \phi_i &amp;=&amp; \max \left( \phi_i, \psi_i \right) \\
    \phi_{i + 1} &amp;=&amp; \max \left( \phi_{i + 1}, \psi_{i + 1}
\right) \\
    &amp;\vdots&amp; \\
    \phi_{i + B - 1} &amp;=&amp; \max \left( \phi_{i + B - 1}, \psi_{i +
B - 1} \right)
  \end{eqnarray}
\]</span></p>
<p><code>LinearNaive</code>ã€<code>LinearNaiveDDR</code>ã€<code>BatchNorm1dReLUNaive</code>ã€<code>MaxPool1dNaive</code>ãŒã€å„å±¤ã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ã—ãŸã€‚
ä¸¦åˆ—åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
<code>LinearOpt1</code>ã€<code>LinearOpt1DDR</code>ã€<code>BatchNorm1dReLUOpt1</code>ã€<code>MaxPool1dOpt1</code>ã«ç½®ãæ›ãˆã¾ã™
(åå‰ã‚’<code>Naive</code>ã‹ã‚‰<code>Opt1</code>ã«ã—ã¾ã™)ã€‚
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¼•æ•°ã¨ã—ã¦<code>B</code>ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™
(<code>B</code>ä¸¦åˆ—)ã€‚</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the fully-connected layer</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Matrix-vector multiplication is parallelized along the output dimension</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `TParam` is the type for weight and bias</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">// `ApplyReLU` is the flag to apply ReLU activation</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">// `B` is the block size for the output dimension</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>          <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">,</span> <span class="dt">bool</span> ApplyReLU<span class="op">,</span> <span class="dt">int</span> B<span class="op">&gt;</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> LinearOpt1<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>InDims<span class="op">],</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>                T y<span class="op">[</span>OutDims<span class="op">],</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> TParam weight<span class="op">[</span>OutDims<span class="op">][</span>InDims<span class="op">],</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>                <span class="at">const</span> TParam bias<span class="op">[</span>OutDims<span class="op">])</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `OutDims` must be a multiple of `B`</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> B <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of `B`&quot;</span><span class="op">);</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i0 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i0 <span class="op">&lt;</span> OutDims<span class="op">;</span> i0 <span class="op">+=</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE off</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    T vals<span class="op">[</span>B<span class="op">];</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        T last <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i<span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">=</span> last <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i<span class="op">][</span>j<span class="op">];</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the fully-connected layer</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="co">// Weight and bias parameters are stored on the DDR memory</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="co">// Matrix-vector multiplication is parallelized along the output dimension</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>          <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">,</span> <span class="dt">bool</span> ApplyReLU<span class="op">,</span> <span class="dt">int</span> B<span class="op">&gt;</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> LinearOpt1DDR<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>InDims<span class="op">],</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>                   T y<span class="op">[</span>OutDims<span class="op">],</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>                   <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params<span class="op">,</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>                   <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `params` contains weight parameters of size (`OutDims`, `InDims`) and</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bias parameters of size (`OutDims`) in a contiguous buffer</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `OutDims` must be a multiple of `B`</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> B <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of `B`&quot;</span><span class="op">);</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `B` must be larger than 1</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>B <span class="op">&gt;</span> <span class="dv">1</span><span class="op">,</span> <span class="st">&quot;`B` must be larger than 1&quot;</span><span class="op">);</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> BHalf <span class="op">=</span> B <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> OffsetToBias <span class="op">=</span> OutDims <span class="op">*</span> InDims<span class="op">;</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>  TParam bias<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1</span></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Copy the bias parameters in advance</span></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> OutDims<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>params<span class="op">[</span>offset <span class="op">+</span> OffsetToBias <span class="op">+</span> i<span class="op">]);</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i0 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i0 <span class="op">&lt;</span> OutDims<span class="op">;</span> i0 <span class="op">+=</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE off</span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>    T vals<span class="op">[</span>B<span class="op">];</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    TParam weight<span class="op">[</span>B<span class="op">][</span>InDims<span class="op">];</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1</span></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Copy the weight parameters for `B` outputs</span></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> offset0 <span class="op">=</span> offset <span class="op">+</span> i0 <span class="op">*</span> InDims<span class="op">;</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>        weight<span class="op">[</span>i1<span class="op">][</span>j<span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>params<span class="op">[</span>offset0 <span class="op">+</span> i1 <span class="op">*</span> InDims <span class="op">+</span> j<span class="op">]);</span></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> OutDims<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>          T last <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i<span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>          vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">=</span> last <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i1<span class="op">][</span>j<span class="op">];</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> OutDims<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>          y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>          y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the 1D batch normalization and ReLU activation</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a><span class="co">// `TParam` is the type for parameters</span></span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a><span class="co">// `Dims` is the number of input and output dimensions</span></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a><span class="co">// `B` is the block size for the output dimension</span></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span> <span class="dt">int</span> Dims<span class="op">,</span> <span class="dt">int</span> B<span class="op">&gt;</span></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> BatchNorm1dReLUOpt1<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>                         T y<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> TParam scale<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> TParam bias<span class="op">[</span>Dims<span class="op">],</span></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> TParam mean<span class="op">[</span>Dims<span class="op">])</span></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `scale` is the multiplication of the weight and reciprocal of the</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>  <span class="co">// standard deviation (to reduce the on-chip memory consumption)</span></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>Dims <span class="op">%</span> B <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`Dims` must be a multiple of `B`&quot;</span><span class="op">);</span></span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i0 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i0 <span class="op">&lt;</span> Dims<span class="op">;</span> i0 <span class="op">+=</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>      <span class="co">// Batch normalization with the learned parameters</span></span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>      T val <span class="op">=</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">[</span>i<span class="op">])</span> <span class="op">*</span> scale<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>      <span class="co">// ReLU activation</span></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> val <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> val <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the 1D max-pooling layer</span></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a><span class="co">// `Dims` is the number of input and output dimensions</span></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a><span class="co">// `B` is the block size for the output dimension</span></span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a><span class="co">// `y` must be properly initialized</span></span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> Dims<span class="op">,</span> <span class="dt">int</span> B<span class="op">&gt;</span></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> MaxPool1dOpt1<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>Dims<span class="op">],</span> T y<span class="op">[</span>Dims<span class="op">])</span></span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>Dims <span class="op">%</span> B <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`Dims` must be a multiple of `B`&quot;</span><span class="op">);</span></span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i0 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i0 <span class="op">&lt;</span> Dims<span class="op">;</span> i0 <span class="op">+=</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">&gt;</span> y<span class="op">[</span>i<span class="op">]</span> <span class="op">?</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">:</span> y<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>LinearOpt1</code>ã¨<code>LinearNaive</code>ã‚’æ¯”ã¹ã¦ã¿ã‚‹ã¨ã€<code>j</code>
(å…¥åŠ›æ¬¡å…ƒ) ã®ãƒ«ãƒ¼ãƒ—ã¯ãã®ã¾ã¾ã§ã€<code>i</code> (å‡ºåŠ›æ¬¡å…ƒ)
ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ãŒã€<code>i0</code>ã¨<code>i1</code>ã®2ã¤ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚
<code>i0</code>ã¯<code>B</code>åˆ»ã¿ã€<code>i1</code>ã¯<code>i0</code>ã‹ã‚‰<code>i0 + B - 1</code>ã¾ã§1ã¤ãšã¤å¢—ãˆã¦ã‚†ãã¾ã™ã€‚
<code>i1</code>ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°
(<code>#pragma HLS UNROLL</code>)
ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ«ãƒ¼ãƒ—ã®ä¸­èº«ãŒå®Œå…¨ã«å±•é–‹ã•ã‚Œã¾ã™ã€‚
<code>i1</code>ã®ãƒ«ãƒ¼ãƒ—è‡ªä½“ã¯ç„¡ããªã£ã¦ã€<code>i0</code>ã‹ã‚‰<code>i0 + B - 1</code>ã¾ã§ã®å‡¦ç†ãŒä¸¦åˆ—ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
æœ€åˆã®ãƒ«ãƒ¼ãƒ—ã«æ³¨ç›®ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        T last <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i<span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">=</span> last <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i<span class="op">][</span>j<span class="op">];</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>      T last0 <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">0</span><span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span><span class="dv">0</span><span class="op">];</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>      T last1 <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">1</span><span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>      <span class="co">// ...</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>      T lastB1 <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i0 <span class="op">+</span> B <span class="op">-</span> <span class="dv">1</span><span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span>B <span class="op">-</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>      vals<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> last0 <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">0</span><span class="op">][</span>j<span class="op">];</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>      vals<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> last1 <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">1</span><span class="op">][</span>j<span class="op">];</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>      <span class="co">// ...</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>      vals<span class="op">[</span>B <span class="op">-</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> lastB1 <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i0 <span class="op">+</span> B <span class="op">-</span> <span class="dv">1</span><span class="op">][</span>j<span class="op">];</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã«ã€<code>vals</code>ã¨ã„ã†ã€ã‚µã‚¤ã‚º<code>B</code>ã®ä¸€æ™‚é…åˆ—ã‚’æ–°ãŸã«ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚
ã“ã®é…åˆ—ã«ã¯ã€å‡ºåŠ›<code>y[i0]</code>ã‹ã‚‰<code>y[i0 + B - 1]</code>ã¾ã§ã®è¨ˆç®—çµæœã‚’ä¿æŒã—ã¾ã™ã€‚
<code>vals</code>ã®å„è¦ç´ ã¯ã€ãƒã‚¤ã‚¢ã‚¹é …<code>bias[i0]</code>ã‹ã‚‰<code>bias[i0 + B - 1]</code>ã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚
ãã®å¾Œã€<code>j</code>ã®ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã£ã¦ã€<code>x[j] * weight[i0][j]</code>ã‹ã‚‰<code>x[j] * weight[i0 + B - 1][j]</code>ãŒã€<code>vals</code>ã®å„è¦ç´ ã«é †ã«åŠ ç®—ã•ã‚Œã¾ã™ã€‚
ä¸Šè¨˜ã®è¨ˆç®—å¼ã¨å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚</p>
<p>ãƒ«ãƒ¼ãƒ—ã‚’å±•é–‹ã™ã‚‹ã¨ã€<code>vals[0]</code>ã‹ã‚‰<code>vals[B - 1]</code>ã¾ã§ã®å…¨è¦ç´ ã€ãã‚Œã‹ã‚‰<code>bias[i0]</code>ã‹ã‚‰<code>bias[i0 + B - 1]</code>ã¾ã§ã€ãã—ã¦<code>weight[i0][j]</code>ã‹ã‚‰<code>weight[i0 + B - 1][j]</code>ã¾ã§ã®<code>B</code>å€‹ã®è¦ç´ ã«ã€1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ã€é…åˆ—<code>bias</code>ã€<code>vals</code>ã€<code>weight</code>ã®ãƒãƒ¼ãƒˆæ•°ã‚’<code>B</code>ä»¥ä¸Šã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p><code>vals</code>ã«ã¤ã„ã¦ã¯ã€<code>#pragma HLS ARRAY_PARTITION type=complete</code>ã‚’ä½¿ã£ã¦ã€é…åˆ—ã‚’å€‹ã€…ã®è¦ç´ ã«å®Œå…¨ã«åˆ†è§£ã—ã¦ã„ã¾ã™ã€‚
åˆ†å‰²ã—ãªã„å ´åˆã¯ãƒãƒ¼ãƒˆãŒ2ã¤ã—ã‹ãªã„ã®ã§ã€åŒæ™‚ã«2ã¤ã®è¦ç´ ã‚’èª­ã¿å‡ºã™
(ã‚ã‚‹ã„ã¯1è¦ç´ ã‚’èª­ã¿å‡ºã—ã¦ã€åˆ¥ã®1è¦ç´ ã¸æ›¸ãè¾¼ã‚€) ã“ã¨ã—ã‹ã§ãã¾ã›ã‚“ã€‚
å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€é…åˆ—ã®å…¨ã¦ã®è¦ç´ ã‚’åŒæ™‚ã«èª­ã¿æ›¸ãã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ãªãŠã€å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª (BlockRAM)
ã§ã¯ãªãã€ãƒ•ãƒªãƒƒãƒ—ãƒ•ãƒ­ãƒƒãƒ— (FF) ã‚’ä½¿ã£ã¦é…åˆ—ãŒå®Ÿè£…ã•ã‚Œã¾ã™ã€‚</p>
<p><code>B</code>å€‹ã®è¦ç´ ã‚’ã‚‚ã¤é…åˆ—<code>vals</code>ã‚’ã€å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/complete-partition.svg"><img src="point-cloud-classification-images/complete-partition.svg" width="50%" /></a></p>
<p><code>LinearOpt1</code>å†…ã«ã¯è¨˜è¿°ã•ã‚Œã¦ã„ã¾ã›ã‚“ãŒã€<code>weight</code>ã¨<code>bias</code>ã«ã¤ã„ã¦ã¯ã€åˆ¥ã®å ´æ‰€ã§ã€<code>vals</code>ã¨åŒæ§˜ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
<code>weight</code>ã¨<code>bias</code>ã‹ã‚‰ã€1ã‚µã‚¤ã‚¯ãƒ«ã§<code>B</code>å€‹ã®<strong>é€£ç¶šã—ãŸ</strong>è¦ç´ 
(<code>bias[i0]</code>ã‹ã‚‰<code>bias[i0 + B - 1]</code>ã¾ã§ã€ãã—ã¦<code>weight[i0][j]</code>ã‹ã‚‰<code>weight[i0 + B - 1][j]</code>ã¾ã§)
ã‚’èª­ã¿å‡ºã™ãŸã‚ã«ã¯ã€æ¬¡ã®ã‚ˆã†ã«<strong>ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²</strong>ã—ã¾ã™ã€‚
<code>weight</code>ã¯2æ¬¡å…ƒé…åˆ—ã§ã™ãŒã€æœ€åˆã®æ¬¡å…ƒã«å¯¾ã—ã¦åˆ†å‰²ã—ãŸã„ã®ã§ã€<code>dim=1</code>ã‚’æŒ‡å®šã—ã¾ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª (BlockRAM)
1ã¤ã«ã¤ããƒãƒ¼ãƒˆãŒ2ã¤ä»˜ã„ã¦ãŠã‚Šã€1ã‚µã‚¤ã‚¯ãƒ«ã§2è¦ç´ ã®èª­ã¿å‡ºã—
(ã‚ã‚‹ã„ã¯1ã¤ã®æ›¸ãå‡ºã—ã¨1ã¤ã®èª­ã¿å‡ºã—) ãŒã§ãã¾ã™ã€‚
<code>B</code>å€‹ã®è¦ç´ ã‚’1ã‚µã‚¤ã‚¯ãƒ«ã§èª­ã¿å‡ºã™ãŸã‚ã«ã¯ã€é…åˆ—ã‚’<code>BHalf = B / 2</code>å€‹ã«åˆ†å‰²ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> BHalf <span class="op">=</span> B <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  TParam weight<span class="op">[</span>OutDims<span class="op">][</span>InDims<span class="op">];</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  TParam bias<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1</span></span></code></pre></div>
<p>ç°¡å˜ãªä¾‹ã¨ã—ã¦ã€2æ¬¡å…ƒé…åˆ—<code>w[8][4]</code>ã‚’ã€æœ€åˆã®æ¬¡å…ƒã§4ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²
(<code>factor=4 dim=1</code>) ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
4åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒ8ã¤ã«å¢—ãˆã‚‹ã®ã§ã€8ã¤ã®é€£ç¶šã—ãŸè¦ç´ 
(ä¾‹ãˆã°<code>w[0][j]</code>ã‹ã‚‰<code>w[7][j]</code>ã¾ã§)
ã‚’ã¾ã¨ã‚ã¦èª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<p>ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã§ã¯ã€åˆ†å‰²ã•ã‚ŒãŸãã‚Œãã‚Œã®é…åˆ—ã«å¯¾ã—ã¦é †ã«ã€å…ˆé ­ã®è¦ç´ ã‹ã‚‰
(<code>w[0][0]</code>ã€<code>w[1][0]</code>ã€<code>w[2][0]</code>ã®é †ã«)
è©°ã‚ã¦ã„ãã¾ã™ã€‚
å…¨ã¦ã®é…åˆ—ã«è¦ç´ ãŒå…¥ã£ãŸã‚‰ã€ã¾ãŸæœ€åˆã®é…åˆ—ã«æˆ»ã£ã¦ã€è¦ç´ ã‚’é †ã«è©°ã‚ã¦ã„ãã¾ã™ã€‚
ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨å›³ã®ã‚ˆã†ãªé…ç½®ã«ãªã‚Šã¾ã™ã€‚ é€£ç¶šã™ã‚‹è¦ç´ 
(<code>w[0][0]</code>ã€<code>w[1][0]</code>ã€<code>w[2][0]</code>ã€<code>w[3][0]</code>ãªã©)
ãŒåˆ¥ã€…ã®é…åˆ—ã«æ ¼ç´ã•ã‚Œã‚‹ã®ã§ã€ã“ã‚Œã‚‰ã‚’ä¸€åº¦ã«å–ã‚Šå‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨ã€é…åˆ—ã®ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€é…åˆ—ã®é€£ç¶šã™ã‚‹è¦ç´ ã«å¯¾ã™ã‚‹ä¸¦åˆ—å‡¦ç†ã‚’ã€å®¹æ˜“ã«å®Ÿç¾ã§ãã¾ã™ã€‚
ã“ã®ã“ã¨ã‹ã‚‰ã€<code>#pragma HLS UNROLL</code>ã¨<code>#pragma HLS ARRAY_PARTITION</code>ã¯ã€ã‚»ãƒƒãƒˆã§ä½¿ã†å ´é¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã¨ã€é…åˆ—ã®åˆ†å‰²æ•°ã¯æƒãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ä¿‚æ•°<code>B</code>ã§ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‚‰ã€é…åˆ—ã¯<code>B / 2</code>å€‹
(<code>B</code>å€‹ã§ã‚‚ã‚ˆã„)
ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã—ãªã„ã¨ã€<code>B</code>ä¸¦åˆ—ã«ãªã‚Šã¾ã›ã‚“ã€‚
ã¾ãŸã€ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã®ã«ã€é…åˆ—ã‚’ä¸€åˆ‡åˆ†å‰²ã—ãªã‘ã‚Œã°ã€ä¸¦åˆ—å‡¦ç†ã«ãªã‚Šã¾ã›ã‚“ã€‚</p>
<p><a
href="point-cloud-classification-images/cyclic-partition.svg"><img src="point-cloud-classification-images/cyclic-partition.svg" width="60%" /></a></p>
<p>æœ€åˆã®æ¬¡å…ƒã§2ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰² (<code>factor=2 dim=1</code>)
ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
2åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒ4ã¤ã«å¢—ãˆã‚‹ã®ã§ã€4ã¤ã®é€£ç¶šã—ãŸè¦ç´ 
(ä¾‹ãˆã°<code>w[0][j]</code>ã‹ã‚‰<code>w[3][j]</code>ã€ã‚ã‚‹ã„ã¯<code>w[4][j]</code>ã‹ã‚‰<code>w[7][j]</code>ã¾ã§)
ã‚’ã¾ã¨ã‚ã¦èª­ã¿å‡ºã›ã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/cyclic-partition3.svg"><img src="point-cloud-classification-images/cyclic-partition3.svg" width="60%" /></a></p>
<p>2ç•ªç›®ã®æ¬¡å…ƒã§2ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰² (<code>factor=2 dim=2</code>)
ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ä»Šåº¦ã¯ã€2ç•ªç›®ã®æ¬¡å…ƒã«ã¤ã„ã¦ã€4ã¤ã®é€£ç¶šã—ãŸè¦ç´ 
(ä¾‹ãˆã°<code>w[i][0]</code>ã‹ã‚‰<code>w[i][3]</code>ã¾ã§)
ã«1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/cyclic-partition2.svg"><img src="point-cloud-classification-images/cyclic-partition2.svg" width="50%" /></a></p>
<p>ã“ã‚Œã‚‰ã‚’è€ƒãˆã‚‹ã¨ã€<code>weight</code>ã¨<code>bias</code>ã«ã¤ã„ã¦ã¯ä¸Šè¨˜ã®ãƒ—ãƒ©ã‚°ãƒã‚’ä½¿ãˆã°ã‚ˆã„ã¨åˆ†ã‹ã‚Šã¾ã™ã€‚</p>
<p>ã•ã¦ã€2ã¤ç›®ã®ãƒ«ãƒ¼ãƒ—ã«æ³¨ç›®ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
1ã¤ç›®ã®ãƒ«ãƒ¼ãƒ—ã§è¨ˆç®—ã•ã‚ŒãŸ<code>B</code>å€‹ã®è¦ç´ ã‚’ã€å‡ºåŠ›<code>y</code>ã«æ›¸ãè¾¼ã‚€éƒ¨åˆ†ã§ã™ã€‚</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>ã“ã®ãƒ«ãƒ¼ãƒ—ã‚‚ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span> <span class="op">{</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>      <span class="co">// ...</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> B <span class="op">-</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>B <span class="op">-</span> <span class="dv">1</span><span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span>B <span class="op">-</span> <span class="dv">1</span><span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span><span class="dv">0</span><span class="op">];</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>      <span class="co">// ...</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>      y<span class="op">[</span>i0 <span class="op">+</span> B <span class="op">-</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>B <span class="op">-</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>å‡ºåŠ›<code>y[i0]</code>ã‹ã‚‰<code>y[i0 + B - 1]</code>ã¾ã§ã®ã€é€£ç¶šã™ã‚‹<code>B</code>å€‹ã®è¦ç´ ã«1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
<code>LinearOpt1</code>å†…ã«ã¯è¨˜è¼‰ã•ã‚Œã¾ã›ã‚“ãŒã€é…åˆ—<code>y</code>ã‚‚ã€æ¬¡ã®ã‚ˆã†ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> BHalf <span class="op">=</span> B <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  T y<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=y type=cyclic factor=BHalf dim=1</span></span></code></pre></div>
<p>ãªãŠã€å…¥åŠ›<code>x</code>ã«ã¤ã„ã¦ã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§1ã¤ã®è¦ç´ ã«ã—ã‹ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ãŸã‚ã€åˆ†å‰²ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
<code>LinearOpt1</code>ã‚’ä½¿ã£ã¦ã€å…¨çµåˆå±¤ã®å‡¦ç†ã‚’<code>B</code>ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã«ã¯ã€å¼•æ•°ã§ã‚ã‚‹é‡ã¿<code>weight</code>ã€ãƒã‚¤ã‚¢ã‚¹<code>bias</code>ã€å‡ºåŠ›<code>y</code>ã‚’ã€å‡ºåŠ›ã®æ¬¡å…ƒã§<code>B / 2</code>å€‹ã«åˆ†å‰²ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“
(<code>B</code>ãŒ2ã§ã‚ã‚Œã°åˆ†å‰²ã®å¿…è¦ã¯ãªã„)ã€‚</p>
<p>ä»¥ä¸ŠãŒ<code>LinearOpt1</code>ã®ä¸»ãªå¤‰æ›´ç‚¹ã§ã™ã€‚
<code>LinearOpt1DDR</code>ã«ã¤ã„ã¦ã‚‚ã€<code>B</code>å€‹ã®å‡ºåŠ›ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€åŒæ§˜ã®å¤‰æ›´ãŒãªã•ã‚Œã¦ã„ã¾ã™ã€‚
å…¨çµåˆå±¤ã®ãƒã‚¤ã‚¢ã‚¹é …<code>bias</code>ã¨ã€å‡ºåŠ›ã®<code>B</code>è¦ç´ åˆ†ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«å¿…è¦ãªé‡ã¿<code>weight</code>ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ä¸Šã«è»¢é€ã—ã¦ã„ã¾ã™ã€‚
<code>LinearNaiveDDR</code>ã¨ã¯ç•°ãªã‚Šã€é‡ã¿ã‚’ä¿æŒã™ã‚‹ãƒãƒƒãƒ•ã‚¡<code>weight</code>ã¯ã€2æ¬¡å…ƒé…åˆ—ã¨ãªã£ã¦ã„ã¾ã™ã€‚
<code>B</code>å€‹ã®å¿…è¦ãªè¦ç´ ã‚’å–ã‚Šå‡ºã™ãŸã‚ã«ã€<code>bias</code>ã¨<code>weight</code>ã¯<code>BHalf = B / 2</code>å€‹ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
<p><code>BatchNorm1dReLUOpt1</code>ã¨<code>MaxPool1dOpt1</code>ã«ã¤ã„ã¦ã‚‚ã€<code>i</code>
(å‡ºåŠ›æ¬¡å…ƒ)
ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ãŒã€<code>i0</code>ã¨<code>i1</code>ã®2ã¤ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚
<code>i1</code>ã®ãƒ«ãƒ¼ãƒ—ã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã€<code>B</code>å€‹ã®å‡ºåŠ›ãŒä¸¦åˆ—ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
<code>BatchNorm1dReLUOpt1</code>ã‚’ä½¿ã£ã¦ã€ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã‚’<code>B</code>ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã«ã¯ã€é–¢æ•°ã®å…¥åŠ›<code>x</code>ã€å‡ºåŠ›<code>y</code>ã¨ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
(ã‚¹ã‚±ãƒ¼ãƒ«<code>scale</code>ã€ãƒã‚¤ã‚¢ã‚¹<code>bias</code>ã€å¹³å‡<code>mean</code>)
ã‚’<code>B / 2</code>å€‹ã«åˆ†å‰²ã—ã¾ã™ã€‚
<code>MaxPool1dOpt1</code>ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã§ã€<code>B</code>ä¸¦åˆ—ã§Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã«ã€é–¢æ•°ã®å…¥åŠ›<code>x</code>ã¨<code>y</code>ã‚’<code>B / 2</code>å€‹ã«åˆ†å‰²ã—ã¾ã™
(<code>x</code>ã¯å„ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´é‡ã§ã€<code>y</code>ã¯ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡)ã€‚</p>
<p>å„å±¤ã‚’<code>B</code>ä¸¦åˆ—ã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ã€é…åˆ—ã®åˆ†å‰²ã®ãƒ«ãƒ¼ãƒ«ã‚’æ¬¡ã«ã¾ã¨ã‚ã¾ã™ã€‚
2ä¸¦åˆ—ã®å ´åˆã¯ã€åˆ†å‰²ã®å¿…è¦ãŒãªã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚</p>
<ul>
<li><code>LinearOpt1</code>:
é‡ã¿<code>weight</code>ã€ãƒã‚¤ã‚¢ã‚¹<code>bias</code>ã€å‡ºåŠ›<code>y</code>ã‚’ã€å‡ºåŠ›ã®æ¬¡å…ƒã§<code>B / 2</code>å€‹ã«åˆ†å‰²
(å…¥åŠ›<code>x</code>ã¯åˆ†å‰²ã®å¿…è¦ãªã—)</li>
<li><code>LinearOpt1DDR</code>:
å‡ºåŠ›<code>y</code>ã‚’<code>B / 2</code>å€‹ã«åˆ†å‰²
(å…¥åŠ›<code>x</code>ã¯åˆ†å‰²ã®å¿…è¦ãªã—)</li>
<li><code>BatchNorm1dReLUOpt1</code>:
å…¥åŠ›<code>x</code>ã¨å‡ºåŠ›<code>y</code>ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
(ã‚¹ã‚±ãƒ¼ãƒ«<code>scale</code>ã€ãƒã‚¤ã‚¢ã‚¹<code>bias</code>ã€å¹³å‡<code>mean</code>)
ã‚’ã€<code>B / 2</code>å€‹ã«åˆ†å‰²</li>
<li><code>MaxPool1dOpt1</code>:
å…¥åŠ›<code>x</code>ã¨å‡ºåŠ›<code>y</code>ã‚’ã€<code>B / 2</code>å€‹ã«åˆ†å‰²</li>
</ul>
<p>ã“ã‚Œã‚‰ã®ä¸¦åˆ—åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–å‡¦ç†ã‚’æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã¾ã™ã€‚
<code>InferenceFeatNaive</code>ã¨<code>InferenceClsNaive</code>ã‹ã‚‰ã€ãã‚Œãã‚Œ<code>InferenceFeatOpt1</code>ã¨<code>InferenceClsOpt1</code>ã«ãªã‚Šã¾ã™ã€‚
é–¢æ•°ã®å¼•æ•°ã¯å¤‰æ›´ã—ã¾ã›ã‚“ã€‚
ãªãŠã€<code>InitializeFeatNaive</code>ã¨<code>InitializeClsNaive</code>
(é‡ã¿ã®åˆæœŸåŒ–é–¢æ•°) ã¯ã€ãã®ã¾ã¾ä½¿ã†ã“ã¨ã«ã—ã¾ã™
(é–¢æ•°åã ã‘ã€<code>InitializeFeatOpt1</code>ã€<code>InitializeClsOpt1</code>ã¨ã—ã¾ã—ãŸ)ã€‚</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the PointNet feature extraction</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for layer input, output, and intermediate results</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">// `U` is the type for parameters</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `N` is the expected number of input points (e.g., 1024)</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">,</span> <span class="dt">int</span> N<span class="op">&gt;</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceFeatOpt1<span class="op">(</span><span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                       T feature<span class="op">[</span>kFeatDims5<span class="op">],</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> conv1<span class="op">,</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> conv2<span class="op">,</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> conv3<span class="op">,</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> conv4<span class="op">,</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> conv5<span class="op">,</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> bn3<span class="op">,</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> bn4<span class="op">,</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> bn5<span class="op">)</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Zero-initialize the output feature</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>  VectorNdSetZero<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>feature<span class="op">);</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute the feature</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> num_points<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_FLATTEN off</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Input, output, and intermediate results</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    T x0<span class="op">[</span>kFeatDims0<span class="op">];</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    T x1<span class="op">[</span>kFeatDims1<span class="op">];</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    T x2<span class="op">[</span>kFeatDims1<span class="op">];</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    T x3<span class="op">[</span>kFeatDims2<span class="op">];</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    T x4<span class="op">[</span>kFeatDims2<span class="op">];</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    T x5<span class="op">[</span>kFeatDims3<span class="op">];</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    T x6<span class="op">[</span>kFeatDims3<span class="op">];</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    T x7<span class="op">[</span>kFeatDims4<span class="op">];</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    T x8<span class="op">[</span>kFeatDims4<span class="op">];</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    T x9<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    T x10<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x3 type=cyclic factor=4 dim=1</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x5 type=cyclic factor=4 dim=1</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x7 type=cyclic factor=8 dim=1</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x9 type=cyclic factor=64 dim=1</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Read a point from a DDR memory</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>    ReadPointNaive<span class="op">&lt;</span>T<span class="op">&gt;(</span>point_cloud<span class="op">,</span> i<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute a point feature</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>      x0<span class="op">,</span> x1<span class="op">,</span> conv1<span class="op">-&gt;</span>weight<span class="op">,</span> conv1<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>      x1<span class="op">,</span> x2<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>      x2<span class="op">,</span> x3<span class="op">,</span> conv2<span class="op">-&gt;</span>weight<span class="op">,</span> conv2<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>      x3<span class="op">,</span> x4<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>      x4<span class="op">,</span> x5<span class="op">,</span> conv3<span class="op">-&gt;</span>weight<span class="op">,</span> conv3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>      x5<span class="op">,</span> x6<span class="op">,</span> bn3<span class="op">-&gt;</span>scale<span class="op">,</span> bn3<span class="op">-&gt;</span>bias<span class="op">,</span> bn3<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">16</span><span class="op">&gt;(</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>      x6<span class="op">,</span> x7<span class="op">,</span> conv4<span class="op">-&gt;</span>weight<span class="op">,</span> conv4<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>      x7<span class="op">,</span> x8<span class="op">,</span> bn4<span class="op">-&gt;</span>scale<span class="op">,</span> bn4<span class="op">-&gt;</span>bias<span class="op">,</span> bn4<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">128</span><span class="op">&gt;(</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>      x8<span class="op">,</span> x9<span class="op">,</span> conv5<span class="op">-&gt;</span>weight<span class="op">,</span> conv5<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>      x9<span class="op">,</span> x10<span class="op">,</span> bn5<span class="op">-&gt;</span>scale<span class="op">,</span> bn5<span class="op">-&gt;</span>bias<span class="op">,</span> bn5<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Update the output feature</span></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>    MaxPool1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span>x10<span class="op">,</span> feature<span class="op">);</span></span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the classification network</span></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for layer input, output, and intermediate results</span></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="co">// `U` is the type for parameters</span></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">&gt;</span></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceClsOpt1<span class="op">(</span><span class="at">const</span> T feature<span class="op">[</span>kFeatDims5<span class="op">],</span></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>                      <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> LinearParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;*</span> fc3<span class="op">,</span></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> BatchNorm1dParams<span class="op">&lt;</span>U<span class="op">,</span> kClsDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params1<span class="op">,</span></span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params2<span class="op">,</span></span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> params3<span class="op">)</span></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>kFeatDims5 <span class="op">==</span> kClsDims0<span class="op">,</span></span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;Feature dimension should be equal to the input dimension&quot;</span><span class="op">);</span></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Input, output, and intermediate results</span></span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>  T x0<span class="op">[</span>kClsDims1<span class="op">];</span></span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a>  T x1<span class="op">[</span>kClsDims1<span class="op">];</span></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a>  T x2<span class="op">[</span>kClsDims2<span class="op">];</span></span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a>  T x3<span class="op">[</span>kClsDims2<span class="op">];</span></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>  T x4<span class="op">[</span>kClsDims3<span class="op">];</span></span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x0 type=cyclic factor=8 dim=1</span></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=x2 type=cyclic factor=4 dim=1</span></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute logits</span></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a>  LinearOpt1DDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims0<span class="op">,</span> kClsDims1<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">16</span><span class="op">&gt;(</span></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a>    feature<span class="op">,</span> x0<span class="op">,</span> params1<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">,</span> x1<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a>  LinearOpt1DDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">,</span> kClsDims2<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a>    x1<span class="op">,</span> x2<span class="op">,</span> params2<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">,</span> x3<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a>  LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a>    x3<span class="op">,</span> x4<span class="op">,</span> fc3<span class="op">-&gt;</span>weight<span class="op">,</span> fc3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Write the result</span></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a>  WriteTensor1dNaive<span class="op">&lt;</span>T<span class="op">,</span> kClsDims3<span class="op">&gt;(</span>out_logits<span class="op">,</span> x4<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>å„å±¤ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™éš›ã«ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¼•æ•°ã«ä¸¦åˆ—åŒ–åº¦ã‚‚æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®4ç•ªç›®ã®å…¨çµåˆå±¤
(PyTorchã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹<code>PointNetFeat::conv4</code>)
ã¯16ä¸¦åˆ—ã€æœ€å¾Œã®å…¨çµåˆå±¤ (<code>PointNetFeat::conv5</code>)
ã¯128ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
ä¸€æ–¹ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¯ã€2ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚
å„å±¤ã®ä¸¦åˆ—åº¦ã‚’ã©ã®ã‚ˆã†ã«æ±ºå®šã—ãŸã®ã‹ã«ã¤ã„ã¦ã¯ã€å¾Œè¿°ã—ã¾ã™ã€‚</p>
<p>ç¶šã„ã¦ã€IPã‚³ã‚¢ã®æœ€ä¸Šä½é–¢æ•°<code>PointNetClsTop</code>ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params1<span class="op">,</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params2<span class="op">,</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params3<span class="op">,</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params4<span class="op">,</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params5<span class="op">,</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params1<span class="op">,</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params2<span class="op">,</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params3<span class="op">)</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params2 offset=slave bundle=gmem0</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params3 offset=slave bundle=gmem0</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params4 offset=slave bundle=gmem0</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params5 offset=slave bundle=gmem0</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params1 offset=slave bundle=gmem0</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params2 offset=slave bundle=gmem0</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=op_mode bundle=control</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=num_points bundle=control</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=out_logits bundle=control</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params2 bundle=control</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params3 bundle=control</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params4 bundle=control</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params5 bundle=control</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params1 bundle=control</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params2 bundle=control</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=return bundle=control</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Parameters for feature extraction</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;</span> feat_conv1<span class="op">;</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;</span> feat_conv2<span class="op">;</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;</span> feat_conv3<span class="op">;</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;</span> feat_conv4<span class="op">;</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;</span> feat_conv5<span class="op">;</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims1<span class="op">&gt;</span> feat_bn1<span class="op">;</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims2<span class="op">&gt;</span> feat_bn2<span class="op">;</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims3<span class="op">&gt;</span> feat_bn3<span class="op">;</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims4<span class="op">&gt;</span> feat_bn4<span class="op">;</span></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kFeatDims5<span class="op">&gt;</span> feat_bn5<span class="op">;</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv2.weight type=cyclic factor=4 dim=1</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv2.bias type=cyclic factor=4 dim=1</span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv3.weight type=cyclic factor=4 dim=1</span></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv3.bias type=cyclic factor=4 dim=1</span></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv4.weight type=cyclic factor=8 dim=1</span></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv4.bias type=cyclic factor=8 dim=1</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv5.weight type=cyclic factor=64 dim=1</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=feat_conv5.bias type=cyclic factor=64 dim=1</span></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Parameters for classification network</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>  <span class="co">// LinearParams&lt;param_t, kClsDims0, kClsDims1&gt; cls_fc1;</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>  <span class="co">// LinearParams&lt;param_t, kClsDims1, kClsDims2&gt; cls_fc2;</span></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>  LinearParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;</span> cls_fc3<span class="op">;</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims1<span class="op">&gt;</span> cls_bn1<span class="op">;</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dParams<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">,</span> kClsDims2<span class="op">&gt;</span> cls_bn2<span class="op">;</span></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Extracted feature</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>  <span class="dt">value_t</span> feature<span class="op">[</span>kFeatDims5<span class="op">];</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>op_mode <span class="op">==</span> kModeInitWeights<span class="op">)</span> <span class="op">{</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Initialize the PointNet feature extraction network</span></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    InitializeFeatOpt1<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_conv1<span class="op">,</span> <span class="op">&amp;</span>feat_conv2<span class="op">,</span> <span class="op">&amp;</span>feat_conv3<span class="op">,</span> <span class="op">&amp;</span>feat_conv4<span class="op">,</span> <span class="op">&amp;</span>feat_conv5<span class="op">,</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_bn1<span class="op">,</span> <span class="op">&amp;</span>feat_bn2<span class="op">,</span> <span class="op">&amp;</span>feat_bn3<span class="op">,</span> <span class="op">&amp;</span>feat_bn4<span class="op">,</span> <span class="op">&amp;</span>feat_bn5<span class="op">,</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>      feat_params1<span class="op">,</span> feat_params2<span class="op">,</span> feat_params3<span class="op">,</span> feat_params4<span class="op">,</span> feat_params5<span class="op">);</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Initialize the classification network</span></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    InitializeClsOpt1<span class="op">&lt;</span><span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>cls_fc3<span class="op">,</span> <span class="op">&amp;</span>cls_bn1<span class="op">,</span> <span class="op">&amp;</span>cls_bn2<span class="op">,</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>      cls_params1<span class="op">,</span> cls_params2<span class="op">,</span> cls_params3<span class="op">);</span></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>op_mode <span class="op">==</span> kModeInference<span class="op">)</span> <span class="op">{</span></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Run the PointNet feature extraction</span></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>    InferenceFeatOpt1<span class="op">&lt;</span><span class="dt">value_t</span><span class="op">,</span> <span class="dt">param_t</span><span class="op">,</span> <span class="dv">1024</span><span class="op">&gt;(</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>      point_cloud<span class="op">,</span> num_points<span class="op">,</span> feature<span class="op">,</span></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_conv1<span class="op">,</span> <span class="op">&amp;</span>feat_conv2<span class="op">,</span> <span class="op">&amp;</span>feat_conv3<span class="op">,</span> <span class="op">&amp;</span>feat_conv4<span class="op">,</span> <span class="op">&amp;</span>feat_conv5<span class="op">,</span></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>feat_bn1<span class="op">,</span> <span class="op">&amp;</span>feat_bn2<span class="op">,</span> <span class="op">&amp;</span>feat_bn3<span class="op">,</span> <span class="op">&amp;</span>feat_bn4<span class="op">,</span> <span class="op">&amp;</span>feat_bn5<span class="op">);</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Run the classification</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>    InferenceClsOpt1<span class="op">&lt;</span><span class="dt">value_t</span><span class="op">,</span> <span class="dt">param_t</span><span class="op">&gt;(</span></span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>      feature<span class="op">,</span> out_logits<span class="op">,</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>      <span class="op">&amp;</span>cls_fc3<span class="op">,</span> <span class="op">&amp;</span>cls_bn1<span class="op">,</span> <span class="op">&amp;</span>cls_bn2<span class="op">,</span></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>      cls_params1<span class="op">,</span> cls_params2<span class="op">,</span> cls_params3<span class="op">);</span></span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>é–¢æ•°ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã¯å…¨ãåŒä¸€ã§ã™ã€‚
ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€å±¤ã®å…¥å‡ºåŠ›ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒãƒƒãƒ•ã‚¡
(<code>feat_conv5.weight</code>ã€<code>feat_conv5.bias</code>ã€<code>x3</code>ã€<code>x5</code>ãªã©)
ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã«ã€<code>#pragma HLS ARRAY_PARTITION</code>ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
é…åˆ—ã®åˆ†å‰²æ•° (<code>factor</code>)
ã«ã¤ã„ã¦ã¯ã€ä¸Šè¿°ã®ãƒ«ãƒ¼ãƒ«ã«å‰‡ã£ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€<code>InferenceFeatOpt1</code>ã¨<code>PointNetClsTop</code>ã‚’ã¿ã‚‹ã¨ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€å¾Œã®å…¨çµåˆå±¤ã‚’128ä¸¦åˆ—ã§å®Ÿè¡Œã—ãŸã„ã®ã§ã€å‡ºåŠ›ç”¨ã®ãƒãƒƒãƒ•ã‚¡<code>x10</code>ã¨ã€å…¨çµåˆå±¤ã®2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿<code>feat_conv5.weight</code>ã€<code>feat_conv5.bias</code>ã‚’64åˆ†å‰²ã—ã¦ã„ã¾ã™
(è¨˜è¿°ã™ã‚‹å ´æ‰€ãŒæ•£ã‚‰ã°ã£ã¦ã„ã‚‹ã®ãŒé›£ç‚¹ã§ã™)ã€‚
åŒæ§˜ã«ã€<code>InferenceClsOpt1</code>ã¨<code>PointNetClsTop</code>ã‚’ã¿ã‚‹ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€åˆã®å…¨çµåˆå±¤ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§ã€å‡ºåŠ›ç”¨ã®ãƒãƒƒãƒ•ã‚¡<code>x0</code>ã¯8åˆ†å‰²ã—ã¦ã„ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¯2ä¸¦åˆ—ãªã®ã§ã€é…åˆ—ã‚’åˆ†å‰²ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</p>
<p>å…ˆè¿°ã®ã‚ˆã†ã«ã€é…åˆ—ã‚’åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒå¢—ãˆã¦ã€ä¸€åº¦ã«å¤šãã®è¦ç´ ã‚’èª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ãŒã€è²´é‡ãªã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®æ¶ˆè²»ã‚‚å¢—ãˆã¾ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®æ¶ˆè²»ã‚’æŠ‘ãˆã¤ã¤ã€ãªã‚‹ã¹ãä¸¦åˆ—åº¦ã‚’ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
æ¨è«–æ™‚é–“ã®çŸ­ç¸®ã«æœ€ã‚‚åŠ¹æœãŒã‚ã‚‹éƒ¨åˆ†
(ä¾‹ãˆã°ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€å¾Œã®å…¨çµåˆå±¤)
ã®ä¸¦åˆ—åº¦ã‚’ä¸Šã’ã¦ã€åŠ¹æœãŒã‚ã¾ã‚Šãªã„éƒ¨åˆ† (ä¾‹ãˆã°ãƒãƒƒãƒæ­£è¦åŒ–å±¤)
ã®ä¸¦åˆ—åº¦ã¯ä¸‹ã’ã¦ã„ã¾ã™ã€‚</p>
<p>ã“ã“ã§ã€å„å±¤ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã™ (å‹•ä½œå‘¨æ³¢æ•°ã¯150MHz)ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">å±¤</th>
<th style="text-align: left;"><code>InferenceFeatNaive</code></th>
<th style="text-align: left;"><code>InferenceFeatOpt1</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤1
(<code>PointNetFeat::conv1</code>)</td>
<td style="text-align: left;">577 (3.843us)</td>
<td style="text-align: left;">321 (2.138us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetFeat::bn1</code>)</td>
<td style="text-align: left;">68 (0.453us)</td>
<td style="text-align: left;">36 (0.240us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤2
(<code>PointNetFeat::conv2</code>)</td>
<td style="text-align: left;">4,481 (29.84us)</td>
<td style="text-align: left;">569 (3.790us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetFeat::bn2</code>)</td>
<td style="text-align: left;">68 (0.453us)</td>
<td style="text-align: left;">36 (0.240us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤3
(<code>PointNetFeat::conv3</code>)</td>
<td style="text-align: left;">4,481 (29.84us)</td>
<td style="text-align: left;">569 (3.790us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetFeat::bn3</code>)</td>
<td style="text-align: left;">68 (0.453us)</td>
<td style="text-align: left;">36 (0.240us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤4
(<code>PointNetFeat::conv4</code>)</td>
<td style="text-align: left;">8,961 (59.68us)</td>
<td style="text-align: left;">569 (3.790us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetFeat::bn4</code>)</td>
<td style="text-align: left;">132 (0.879us)</td>
<td style="text-align: left;">68 (0.453us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤5
(<code>PointNetFeat::conv5</code>)</td>
<td style="text-align: left;">137,217 (914.0us)</td>
<td style="text-align: left;">1,081 (7.199us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetFeat::bn5</code>)</td>
<td style="text-align: left;">1,028 (6.846us)</td>
<td style="text-align: left;">516 (3.437us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤</td>
<td style="text-align: left;">1,026 (6.833us)</td>
<td style="text-align: left;">514 (3.423us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">å…¨ä½“ (1å›åˆ†)</td>
<td style="text-align: left;">158,149 (1.053ms)</td>
<td style="text-align: left;">4,357 (29.02us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨ä½“ (1024å›åˆ†)</td>
<td style="text-align: left;">161,945,604 (1.079s)</td>
<td style="text-align: left;">4,462,596 (29.72ms)</td>
</tr>
</tbody>
</table>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã€ã‚„ã¯ã‚Šæœ€å¾Œã®å…¨çµåˆå±¤ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ã„ã¾ã™ã€‚
128ä¸¦åˆ—ã«ã™ã‚‹ã“ã¨ã§ã€å®Ÿè¡Œæ™‚é–“ã‚’126.9å€
(137,217ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰1,081ã‚µã‚¤ã‚¯ãƒ«) å‰Šæ¸›ã§ãã¦ã„ã¾ã™ã€‚
4ã¤ç›®ã®å…¨çµåˆå±¤ã«ã¤ã„ã¦ã‚‚ã€16ä¸¦åˆ—ã«ã™ã‚‹ã“ã¨ã§ã€å®Ÿè¡Œæ™‚é–“ãŒ15.75å€
(8,961ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰569ã‚µã‚¤ã‚¯ãƒ«) çŸ­ããªã‚Šã¾ã—ãŸã€‚
å…¨çµåˆå±¤ã‚„ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã€Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã¿ã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ã‹ã—ã¦ã€æ¨è«–æ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã—ãŸã€‚
ã¾ãŸåˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">å±¤</th>
<th style="text-align: left;"><code>InferenceClsNaive</code></th>
<th style="text-align: left;"><code>InferenceClsOpt1</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤1
(<code>PointNetCls::fc1</code>)</td>
<td style="text-align: left;">1,056,279 (7.035ms)</td>
<td style="text-align: left;">558,071 (3.717ms)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetCls::bn1</code>)</td>
<td style="text-align: left;">516 (3.437us)</td>
<td style="text-align: left;">260 (1.732us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤2
(<code>PointNetCls::fc2</code>)</td>
<td style="text-align: left;">266,007 (1.772ms)</td>
<td style="text-align: left;">148,183 (987.0us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU
(<code>PointNetCls::bn2</code>)</td>
<td style="text-align: left;">260 (1.732us)</td>
<td style="text-align: left;">132 (0.879us)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">å…¨çµåˆå±¤3
(<code>PointNetCls::fc3</code>)</td>
<td style="text-align: left;">10,481 (69.80us)</td>
<td style="text-align: left;">5,261 (35.04us)</td>
</tr>
<tr class="even">
<td style="text-align: left;">å…¨ä½“</td>
<td style="text-align: left;">1,333,605 (8.882ms)</td>
<td style="text-align: left;">711,969 (4.742ms)</td>
</tr>
</tbody>
</table>
<p>æœ€åˆã®å…¨çµåˆå±¤ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸãŒã€å®Ÿè¡Œæ™‚é–“ã¯1.89å€
(1,056,279ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰558,071ã‚µã‚¤ã‚¯ãƒ«) ã—ã‹çŸ­ããªã£ã¦ã„ã¾ã›ã‚“ã€‚
å‰è¿°ã®ã‚ˆã†ã«ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€åˆã®å…¨çµåˆå±¤2ã¤ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«ç½®ãã®ã§ã¯ãªãã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’è»¢é€ã—ã¦ã„ã¾ã™ã€‚
è¡Œåˆ—ã®ç©ã‚„åŠ ç®—ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§ã™ãŒã€ãƒ‡ãƒ¼ã‚¿è»¢é€éƒ¨åˆ†ã®å®Ÿè¡Œæ™‚é–“ã¯çŸ­ç¸®ã•ã‚Œãªã„ã®ã§ã€ã“ã®ã‚ˆã†ãªçµæœã«ãªã£ã¦ã„ã¾ã™ã€‚
2ã¤ç›®ã®å…¨çµåˆå±¤ã«é–¢ã—ã¦ã‚‚åŒæ§˜ã«ã€8ä¸¦åˆ—ã‚’æŒ‡å®šã—ãŸã®ã§ã™ãŒã€å®Ÿè¡Œæ™‚é–“ã¯1.80å€
(266,007ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰148,183ã‚µã‚¤ã‚¯ãƒ«) ã®å‰Šæ¸›ã«ç•™ã¾ã£ã¦ã„ã¾ã™ã€‚</p>
<p>ç¾åœ¨ã®å®Ÿè£…ã§ã¯ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹…ã¯32ãƒ“ãƒƒãƒˆã§ã€1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ã<code>float</code>ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ãšã¤è»¢é€ã—ã¦ã„ã¾ã™ã€‚
å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹…ã‚’åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã®å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ã€‚
å¾Œã»ã©ã€ãƒãƒ¼ãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã‹ã‚‰64ãƒ“ãƒƒãƒˆã«åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã§<code>float</code>ã®ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤è»¢é€ã™ã‚‹ã‚ˆã†ã«ã€æ”¹å–„ã—ã¾ã™ã€‚</p>
<p>IPã‚³ã‚¢ã®å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã«ã¯2ã¤ã‚ã‚Šã¾ã™ãŒã€ã“ã®ã†ã¡é‡ã¿ã®åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã«ã¤ã„ã¦ã¯ã€å…¨ãæ‰‹ã‚’åŠ ãˆã¦ã„ã¾ã›ã‚“ã€‚
é‡ã¿ã®åˆæœŸåŒ–ã¯ã€IPã‚³ã‚¢ã®åˆ©ç”¨é–‹å§‹å‰ã«ä¸€åº¦ã ã‘è¡Œã‚ã‚Œã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã¨ã¯å…¨ãé–¢ä¿‚ãªã„ãŸã‚ã§ã™ã€‚</p>
<p>ä»¥ä¸Šã§æ¨è«–ã®ä¸¦åˆ—åŒ–ãŒæ¸ˆã¿ã¾ã—ãŸã€‚
è©³ã—ãã¯<code>hls/src/top_opt1.cpp</code>ã‚’ã”å‚ç…§ãã ã•ã„ã€‚</p>
<h2 id="ä¸¦åˆ—åŒ–ãã®2-ã‚¿ã‚¹ã‚¯ä¸¦åˆ—æ€§ã®æ´»ç”¨">ä¸¦åˆ—åŒ–ãã®2
(ã‚¿ã‚¹ã‚¯ä¸¦åˆ—æ€§ã®æ´»ç”¨)</h2>
<p>å„å±¤ã®è¨ˆç®—ã¯ä¸¦åˆ—åŒ–ã§ãã¾ã—ãŸãŒã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®éƒ¨åˆ†ã«ã¯ã€ã¾ã é«˜é€ŸåŒ–ã®ä½™åœ°ãŒæ®‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–å‡¦ç†ã‚’ã€ã‚‚ã†ä¸€åº¦ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute the feature</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> num_points<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_FLATTEN off</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ...</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Read a point from a DDR memory</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    ReadPointNaive<span class="op">&lt;</span>T<span class="op">&gt;(</span>point_cloud<span class="op">,</span> i<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute a point feature</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>      x0<span class="op">,</span> x1<span class="op">,</span> conv1<span class="op">-&gt;</span>weight<span class="op">,</span> conv1<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>      x1<span class="op">,</span> x2<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>      x2<span class="op">,</span> x3<span class="op">,</span> conv2<span class="op">-&gt;</span>weight<span class="op">,</span> conv2<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>      x3<span class="op">,</span> x4<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>      x4<span class="op">,</span> x5<span class="op">,</span> conv3<span class="op">-&gt;</span>weight<span class="op">,</span> conv3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>      x5<span class="op">,</span> x6<span class="op">,</span> bn3<span class="op">-&gt;</span>scale<span class="op">,</span> bn3<span class="op">-&gt;</span>bias<span class="op">,</span> bn3<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">16</span><span class="op">&gt;(</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>      x6<span class="op">,</span> x7<span class="op">,</span> conv4<span class="op">-&gt;</span>weight<span class="op">,</span> conv4<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>      x7<span class="op">,</span> x8<span class="op">,</span> bn4<span class="op">-&gt;</span>scale<span class="op">,</span> bn4<span class="op">-&gt;</span>bias<span class="op">,</span> bn4<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">128</span><span class="op">&gt;(</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>      x8<span class="op">,</span> x9<span class="op">,</span> conv5<span class="op">-&gt;</span>weight<span class="op">,</span> conv5<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>      x9<span class="op">,</span> x10<span class="op">,</span> bn5<span class="op">-&gt;</span>scale<span class="op">,</span> bn5<span class="op">-&gt;</span>bias<span class="op">,</span> bn5<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Update the output feature</span></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    MaxPool1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span>x10<span class="op">,</span> feature<span class="op">);</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span></code></pre></div>
<p>ãƒ«ãƒ¼ãƒ—ã®å†…éƒ¨ã‚’ã¿ã‚‹ã¨ã€æœ€åˆã«ã€DRAMã«ç½®ã‹ã‚ŒãŸç‚¹ç¾¤<code>point_cloud</code>ã‹ã‚‰<code>i</code>ç•ªç›®ã®ç‚¹ã‚’å–ã£ã¦ãã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡<code>x0</code>ã«æ ¼ç´ã—ã¦ã„ã¾ã™ã€‚
ç¶šã„ã¦ã€ã“ã®<code>x0</code>ãŒãƒã‚±ãƒ„ãƒªãƒ¬ãƒ¼ã®ã‚ˆã†ã«ã€è¤‡æ•°ã®é–¢æ•°ã«æ¸¡ã•ã‚Œã¦ã„ãã¾ã™ã€‚
ä¾‹ãˆã°ã€æœ€åˆã®å…¨çµåˆå±¤ã«ã‚ˆã£ã¦<code>x0</code>ã‹ã‚‰<code>x1</code>ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã«ã‚ˆã£ã¦<code>x1</code>ã‹ã‚‰<code>x2</code>ã€æ¬¡ã®å…¨çµåˆå±¤ã«ã‚ˆã£ã¦<code>x2</code>ã‹ã‚‰<code>x3</code>ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã‚ã‚‹å±¤ã®é–¢æ•° (ä¾‹ãˆã°<code>LinearOpt1(x4, x5)</code>)
ã¯ã€ãã®ä¸€ã¤å‰ã®é–¢æ•°ã®å‡ºåŠ› (<code>x4</code>) ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€å‡ºåŠ›
(<code>x5</code>) ã‚’æ¬¡ã®é–¢æ•°ã«å¼•ãæ¸¡ã—ã¾ã™ã€‚
å…¨ã¦ã®é–¢æ•°ãŒã€å…¥å‡ºåŠ›ã‚’ä»‹ã—ã¦ã€æ•°ç ã¤ãªãã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚
é–¢æ•°ã®å®Ÿè¡Œã®æµã‚Œã‚’å›³ã«ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/dataflow-optimization-before.svg"><img src="point-cloud-classification-images/dataflow-optimization-before.svg" width="80%" /></a></p>
<p>å…ˆç¨‹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¨åŒæ§˜ã«ã€è¤‡æ•°ã®ç‚¹ã«ã¤ã„ã¦å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã§ãã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/dataflow-optimization-after.svg"><img src="point-cloud-classification-images/dataflow-optimization-after.svg" width="90%" /></a></p>
<p>ä¾‹ãˆã°ã€1ã¤ç›®ã®ç‚¹ã«å¯¾ã—ã¦ã€æœ€å¾Œã®å…¨çµåˆå±¤ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹é–“ã«ã€2ã¤ç›®ã®ç‚¹ã«å¯¾ã—ã¦ã€ãã®ä¸€ã¤å‰ã®ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã‚’è¨ˆç®—ã™ã‚‹ã¨ã„ã†ã‚ˆã†ã«ã€è¤‡æ•°ã®ç‚¹ã«å¯¾ã™ã‚‹å‡¦ç†ã‚’æ™‚é–“çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¾ã™ã€‚
ä»¥å‰ã¯ã€ãƒ«ãƒ¼ãƒ—å†…ã®å‡¦ç†ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¦ã€ãƒ«ãƒ¼ãƒ—ã®è¤‡æ•°ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã¾ã—ãŸã€‚
ãã—ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ã€ä¸»ã«ä¹—ç®—ã‚„åŠ ç®—ã§ã—ãŸã€‚
ã“ã“ã§ã¯ã€å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ä¸€ã¤ã®é–¢æ•° (ã‚¿ã‚¹ã‚¯)
ã«å¯¾å¿œã™ã‚‹ã®ã§ã€ã‚ˆã‚Šç²—ç²’åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¨ã„ãˆã¾ã™ã€‚
ã“ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¯ã€Vitis
HLSã§ã¯<strong>ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–</strong> (Dataflow optimization)
ã¨ã‚ˆã°ã‚Œã¦ã„ã¾ã™ (<strong>æœ€é©åŒ–ãã®6: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–</strong>)ã€‚
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ã„ã‚ã„ã‚ãªæ¡ä»¶ãŒã‚ã‚Šã¾ã™ãŒã€ä»Šå›ã®å ´åˆã¯å¤§ä¸ˆå¤«ã§ã™ã€‚</p>
<p>ä»¥å‰è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’ãªã‚‹ã¹ãå‡ç­‰ã«æƒãˆã‚‹ã“ã¨ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŠ¹æœãŒå¢—ã—ã¾ã™ã€‚
å„å±¤ã®è¨ˆç®—æ™‚é–“ã‚’ã€ãªã‚‹ã¹ãå‡ä¸€ã«ã—ãŸã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚
è¨ˆç®—æ™‚é–“ã¯ã€ä¸Šã®è¡¨ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’åˆ©ç”¨ã™ã‚‹å‰ã¯ã€å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•° (ç‰¹ã«å…¨çµåˆå±¤)
ã«ã¯ã€ã‹ãªã‚Šã®ã°ã‚‰ã¤ããŒã‚ã‚Šã¾ã—ãŸã€‚
å…¨çµåˆå±¤5ã¤ã ã‘æŠœãå‡ºã—ã¦ã¿ã‚‹ã¨ã€577ã€4,481ã€4,481ã€8,961ã€137,217ã¨ãªã£ã¦ã„ã¾ã™ã€‚
ãã‚Œãã‚Œã®å±¤ã‚’ã€2ã€8ã€8ã€16ã€128ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã§
(<code>InferenceFeatOpt1</code>ã‚’å‚ç…§)ã€321ã€569ã€569ã€569ã€1,081ã‚µã‚¤ã‚¯ãƒ«ã«å‰Šæ¸›ã•ã‚Œã€ã°ã‚‰ã¤ãã‚‚ã‹ãªã‚ŠæŠ‘ãˆã‚‰ã‚Œã¾ã—ãŸã€‚
æœ€å¾Œã®å…¨çµåˆå±¤ã‚’256ä¸¦åˆ—ã«ã™ã‚Œã°ã€ã•ã‚‰ã«å‡ç­‰ã«ãªã‚Šã¾ã™ãŒã€å›è·¯ãŒè¤‡é›‘ã«ãªã‚Šéãã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸã€‚</p>
<p>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯æœ€ã‚‚æ™‚é–“ã®é•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸ã«ã‚ˆã£ã¦æ€§èƒ½ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚
ä»Šå›ã®å ´åˆã¯ã€æœ€å¾Œã®å…¨çµåˆå±¤ (1,081ã‚µã‚¤ã‚¯ãƒ«) ã«ã‚ˆã£ã¦æ€§èƒ½ãŒæ±ºã¾ã‚Šã¾ã™ã€‚
ä»–ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ã€1,081ã‚µã‚¤ã‚¯ãƒ«ä»¥ä¸‹ã§ã‚ã‚Œã°ã€ä½•ã‚µã‚¤ã‚¯ãƒ«ã§ã‚ã‚ã†ã¨ã‚‚æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚
ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ä»–ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã«é–¢ã—ã¦ã¯ã€1,081ã‚µã‚¤ã‚¯ãƒ«ã‚’è¶…ãˆãªã„ç¯„å›²ã§ã€ãªã‚‹ã¹ãä¸¦åˆ—åº¦ã‚’è½ã¨ã—ã¾ã—ãŸã€‚</p>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã“ã®ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’äºˆã‚è€ƒæ…®ã—ãŸã†ãˆã§ã€å„å±¤ã®ä¸¦åˆ—åº¦ã‚’æŒ‡å®šã—ã¾ã—ãŸã€‚
åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸¦åˆ—åº¦ã¯ã€ä½•ã¨ãªãæ±ºã‚ã¦ã„ã¾ã™ã€‚</p>
<p>ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ–½ã—ãŸå®Ÿè£…ã‚’ã€æ¬¡ã«ç¤ºã—ã¾ã™ã€‚
<code>InferenceFeatOpt1</code>ã‹ã‚‰ã€<code>InferenceFeatOpt2</code>ã¨ã—ã¾ã—ãŸã€‚</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the PointNet feature extraction</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for layer input, output, and intermediate results</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">// `U` is the type for parameters</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `N` is the expected number of input points (e.g., 1024)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">,</span> <span class="dt">int</span> N<span class="op">&gt;</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceFeatOpt2<span class="op">(...)</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Zero-initialize the output feature</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  VectorNdSetZero<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>feature<span class="op">);</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute the feature</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> num_points<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS LOOP_FLATTEN off</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS DATAFLOW</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=point_cloud</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=num_points</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=feature</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=conv1</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=conv2</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=conv3</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=conv4</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=conv5</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=bn1</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=bn2</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=bn3</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=bn4</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS STABLE variable=bn5</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Input, output, and intermediate results</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ...</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Read a point from a DDR memory</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    ReadPointNaive<span class="op">&lt;</span>T<span class="op">&gt;(</span>point_cloud<span class="op">,</span> i<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute a point feature</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>      x0<span class="op">,</span> x1<span class="op">,</span> conv1<span class="op">-&gt;</span>weight<span class="op">,</span> conv1<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>      x1<span class="op">,</span> x2<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>      x2<span class="op">,</span> x3<span class="op">,</span> conv2<span class="op">-&gt;</span>weight<span class="op">,</span> conv2<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>      x3<span class="op">,</span> x4<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>      x4<span class="op">,</span> x5<span class="op">,</span> conv3<span class="op">-&gt;</span>weight<span class="op">,</span> conv3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>      x5<span class="op">,</span> x6<span class="op">,</span> bn3<span class="op">-&gt;</span>scale<span class="op">,</span> bn3<span class="op">-&gt;</span>bias<span class="op">,</span> bn3<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">16</span><span class="op">&gt;(</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>      x6<span class="op">,</span> x7<span class="op">,</span> conv4<span class="op">-&gt;</span>weight<span class="op">,</span> conv4<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>      x7<span class="op">,</span> x8<span class="op">,</span> bn4<span class="op">-&gt;</span>scale<span class="op">,</span> bn4<span class="op">-&gt;</span>bias<span class="op">,</span> bn4<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">128</span><span class="op">&gt;(</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>      x8<span class="op">,</span> x9<span class="op">,</span> conv5<span class="op">-&gt;</span>weight<span class="op">,</span> conv5<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>      x9<span class="op">,</span> x10<span class="op">,</span> bn5<span class="op">-&gt;</span>scale<span class="op">,</span> bn5<span class="op">-&gt;</span>bias<span class="op">,</span> bn5<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Update the output feature</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    MaxPool1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span>x10<span class="op">,</span> feature<span class="op">);</span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>InferenceFeatOpt1</code>ã¨ç•°ãªã‚‹ã®ã¯HLSãƒ—ãƒ©ã‚°ãƒã®éƒ¨åˆ†ã ã‘ã§ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã®å…ˆé ­éƒ¨åˆ†ã«ã¯<code>#pragma HLS DATAFLOW</code>ã®è¨˜è¿°ãŒã‚ã‚Šã€ãƒ«ãƒ¼ãƒ—ã®ä¸­èº«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã—ã¾ã™ã€‚
<code>#pragma HLS STABLE</code>ã®éƒ¨åˆ†ã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã™ã‚‹ã«ã‚ãŸã£ã¦ã€ãã®å¤‰æ•°ã«ã¤ã„ã¦åŒæœŸã‚’ã¨ã‚‹å¿…è¦ãŒãªã„ã€ã¨ã„ã†ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ç‚¹ç¾¤ãªã©ã€ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œä¸­ã¯å¤‰åŒ–ã—ãªã„å¤‰æ•°ã«ä»˜ä¸ã—ã¦ã„ã¾ã™ã€‚
ã“ã®è¨˜è¿°ãŒãªã„ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãŒã†ã¾ãæ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚</p>
<p>ã“ã®2ç¨®é¡ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ¿å…¥ã™ã‚‹ã ã‘ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’ã„ã¨ã‚‚ç°¡å˜ã«å®Ÿç¾ã§ãã¾ã™ã€‚
é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã¯ç´ æ™´ã‚‰ã—ã„ã¨æ€ã„ã¾ã™ã€‚ <code>PointNetClsTop</code>
(ãƒˆãƒƒãƒ—é–¢æ•°) ã‚„åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«– (<code>InferenceClsOpt1</code>)
ã«ã¤ã„ã¦ã¯ä»¥å‰ã¨å…¨ãåŒã˜ã§ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å‰²æ„›ã—ã¾ã™ã€‚</p>
<p>ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’ã¿ã¦ã¿ã¾ã™ã€‚
<code>InferenceFeatOpt1</code>ã§ã¯ã€1ã¤ã®ç‚¹ã«å¯¾ã™ã‚‹é †ä¼æ’­ã«4,357ã‚µã‚¤ã‚¯ãƒ«
(29.02us)
è¦ã—ã¦ã„ã¾ã—ãŸãŒã€<code>InferenceFeatOpt2</code>ã§ã‚‚4,344ã‚µã‚¤ã‚¯ãƒ«
(28.93us) ã§ã€ã»ã¼å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚
ä¸€æ–¹ã€1,024å€‹ã®ç‚¹ã«å¯¾ã™ã‚‹å‡¦ç†æ™‚é–“ã‚’ã¿ã¦ã¿ã‚‹ã¨ã€<code>InferenceFeatOpt1</code>ã§ã¯4,462,596ã‚µã‚¤ã‚¯ãƒ«
(29.72ms) ã§ã—ãŸãŒã€<code>InferenceFeatOpt2</code>ã§ã¯1,112,259ã‚µã‚¤ã‚¯ãƒ«
(7.408ms) ã«å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¦ã‚‚ã€å„å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹è¨ˆç®—æ™‚é–“ (ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·)
ã¯å¤‰åŒ–ã—ã¾ã›ã‚“ãŒã€å˜ä½æ™‚é–“ã‚ãŸã‚Šã«å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ•° (ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ)
ã¯æ”¹å–„ã™ã‚‹ã®ã§ã€ãã‚Œã«ä¼´ã£ã¦å…¨ä½“ã®æ€§èƒ½ã‚‚å‘ä¸Šã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚</p>
<p>ã“ã‚Œã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã¯çµ‚ã‚ã‚Šã§ã™ã€‚
è©³ã—ãã¯<code>hls/src/top_opt2.cpp</code>ã‚’ã”è¦§ãã ã•ã„ã€‚</p>
<h2 id="å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆå¹…ã®æ‹¡å¼µ">å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆå¹…ã®æ‹¡å¼µ</h2>
<p>åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨çµåˆå±¤éƒ¨åˆ†ã§ã¯ã€ç©å’Œæ¼”ç®—ã‚’ä¸¦åˆ—åŒ–ã—ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å…¨ä½“ã®å‡¦ç†æ™‚é–“ã¯ãã‚Œã»ã©çŸ­ç¸®ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚
DRAMã‹ã‚‰ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»¢é€ã®ã‚µã‚¤ã‚¯ãƒ«æ•°ãŒã€å¤‰åŒ–ã—ã¦ã„ãªã„ãŸã‚ã§ã™ã€‚
ãã“ã§æœ€å¾Œã®æœ€é©åŒ–ã¨ã—ã¦ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®ãƒ“ãƒƒãƒˆå¹…ã‚’32ã‹ã‚‰64ã«åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ã2ã¤ã®<code>float</code>ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã§ãã‚‹ã‚ˆã†ã«ã€å®Ÿè£…ã‚’ä¿®æ­£ã—ã¦ã¿ã¾ã—ã‚‡ã†
(<strong>æœ€é©åŒ–ãã®7: ãƒ‡ãƒ¼ã‚¿è»¢é€</strong>)ã€‚</p>
<p>æœ€åˆã«ã€IPã‚³ã‚¢ã®æœ€ä¸Šä½é–¢æ•°<code>PointNetClsTop</code>ã‹ã‚‰ä¿®æ­£ã—ã¾ã™ã€‚
ä¿®æ­£å‰ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã—ãŸã€‚</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params1<span class="op">,</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params2<span class="op">,</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params3<span class="op">,</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params4<span class="op">,</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params5<span class="op">,</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params1<span class="op">,</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params2<span class="op">,</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params3<span class="op">)</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>ã“ã‚Œã‚’ã€æ¬¡ã®ã‚ˆã†ã«64ãƒ“ãƒƒãƒˆå¹…ã«ã—ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> point_cloud<span class="op">,</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                    ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> out_logits<span class="op">,</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> feat_params1<span class="op">,</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> feat_params2<span class="op">,</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> feat_params3<span class="op">,</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> feat_params4<span class="op">,</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> feat_params5<span class="op">,</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> cls_params1<span class="op">,</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> cls_params2<span class="op">,</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> cls_params3<span class="op">)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>ap_uint</code>ã¯ã€Vitis
HLSã§æä¾›ã•ã‚Œã¦ã„ã‚‹ã€ä»»æ„ãƒ“ãƒƒãƒˆé•·ã®ç¬¦å·ãªã—æ•´æ•°å‹ã§ã™ã€‚
ã“ã“ã§ã¯64ãƒ“ãƒƒãƒˆã¨ã—ã¦ã„ã¾ã™ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ããƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‰ãªã‘ã‚Œã°ã„ã‘ãªã„ã®ã§ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã«é–¢ã™ã‚‹éƒ¨åˆ†ã‚’å…¨ã¦ä¿®æ­£ã—ã¾ã™ã€‚
DRAMã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã—ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã™ã‚‹ã€é‡ã¿åˆæœŸåŒ–é–¢æ•°<code>InitializeFeatOpt1</code>ã€<code>InitializeClsOpt1</code>ã‚‚æ¬¡ã®ã‚ˆã†ã«ç›´ã—ã¦ã€æ–°ãŸã«<code>InitializeFeatOpt3</code>ã€<code>InitializeClsOpt3</code>ã¨ã—ã¾ã™ã€‚
å˜ã«ã€é–¢æ•°ã®å¼•æ•°ã‚’<code>float*</code>ã‹ã‚‰<code>ap_uint&lt;64&gt;*</code>ã«å¤‰æ›´ã—ãŸã ã‘ã§ã™ã€‚</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InitializeFeatOpt3<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> conv1<span class="op">,</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>                        LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> conv2<span class="op">,</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>                        LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> conv3<span class="op">,</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>                        LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> conv4<span class="op">,</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>                        LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> conv5<span class="op">,</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">&gt;*</span> bn3<span class="op">,</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">&gt;*</span> bn4<span class="op">,</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>                        BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;*</span> bn5<span class="op">,</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params1<span class="op">,</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params2<span class="op">,</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params3<span class="op">,</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params4<span class="op">,</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params5<span class="op">)</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsOpt2<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims0<span class="op">,</span> kFeatDims1<span class="op">&gt;(</span>conv1<span class="op">,</span> bn1<span class="op">,</span> params1<span class="op">);</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims1<span class="op">,</span> kFeatDims2<span class="op">&gt;(</span>conv2<span class="op">,</span> bn2<span class="op">,</span> params2<span class="op">);</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims2<span class="op">,</span> kFeatDims3<span class="op">&gt;(</span>conv3<span class="op">,</span> bn3<span class="op">,</span> params3<span class="op">);</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims3<span class="op">,</span> kFeatDims4<span class="op">&gt;(</span>conv4<span class="op">,</span> bn4<span class="op">,</span> params4<span class="op">);</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>  ReadBlockParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims4<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>conv5<span class="op">,</span> bn5<span class="op">,</span> params5<span class="op">);</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InitializeClsOpt3<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;*</span> fc3<span class="op">,</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>                       BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims1<span class="op">&gt;*</span> bn1<span class="op">,</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>                       BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">&gt;*</span> bn2<span class="op">,</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params1<span class="op">,</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params2<span class="op">,</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params3<span class="op">)</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>  ReadBatchNorm1dParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kClsDims1<span class="op">&gt;(</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    bn1<span class="op">,</span> params1<span class="op">,</span> kClsDims0 <span class="op">*</span> kClsDims1 <span class="op">+</span> kClsDims1<span class="op">);</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>  ReadBatchNorm1dParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">&gt;(</span></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>    bn2<span class="op">,</span> params2<span class="op">,</span> kClsDims1 <span class="op">*</span> kClsDims2 <span class="op">+</span> kClsDims2<span class="op">);</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>  ReadLinearParamsOpt1<span class="op">&lt;</span>T<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">&gt;(</span></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>    fc3<span class="op">,</span> params3<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>æœ€åˆã®å®Ÿè£…ã§ã¯<code>ReadLinearParamsNaive</code>ã€<code>ReadBatchNorm1dParamsNaive</code>ã€<code>ReadBlockParamsNaive</code>ã‚’ä½¿ã£ã¦ã„ã¾ã—ãŸãŒã€ã“ã“ã§ã¯æ–°ãŸã«<code>ReadLinearParamsOpt1</code>ã€<code>ReadBatchNorm1dParamsOpt1</code>ã€<code>ReadBlockParamsOpt1</code>ã€<code>ReadBlockParamsOpt2</code>ã®4ç¨®é¡ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚
è©³ã—ãä¸­èº«ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Read the parameters for a linear layer from a DDR memory and</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">// store them to BRAM buffers</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">&gt;</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadLinearParamsOpt1<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> InDims<span class="op">,</span> OutDims<span class="op">&gt;*</span> linear<span class="op">,</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>                          <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params<span class="op">,</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `params` contains weight parameters of size (`OutDims`, `InDims`) and</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bias parameters of size (`OutDims`) in a contiguous buffer</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>InDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`InDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>  ReadTensor2dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">,</span> InDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>weight<span class="op">,</span> params<span class="op">,</span> offset<span class="op">);</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>                               offset <span class="op">+</span> InDims <span class="op">*</span> OutDims<span class="op">);</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="co">// Read the parameters for a 1D batch normalization layer from a DDR memory and</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="co">// store them to BRAM buffers</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="co">// `Dims` is the number of input and output dimensions</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> Dims<span class="op">&gt;</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadBatchNorm1dParamsOpt1<span class="op">(</span>BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> Dims<span class="op">&gt;*</span> bn<span class="op">,</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>                               <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params<span class="op">,</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>                               <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `params` contains scale parameters of size (`Dims`),</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bias of size (`Dims`), and mean of size (`Dims`) in a contiguous buffer</span></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>Dims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`Dims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> Dims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>scale<span class="op">,</span> params<span class="op">,</span> offset<span class="op">);</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> Dims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span> offset <span class="op">+</span> Dims<span class="op">);</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> Dims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>mean<span class="op">,</span> params<span class="op">,</span> offset <span class="op">+</span> Dims <span class="op">*</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a><span class="co">// Read the parameters for a linear and 1D batch normalization layer</span></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a><span class="co">// from a DDR memory and store them to BRAM buffers</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">&gt;</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadBlockParamsOpt1<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> InDims<span class="op">,</span> OutDims<span class="op">&gt;*</span> linear<span class="op">,</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;*</span> bn<span class="op">,</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params<span class="op">)</span></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>InDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`InDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>  ReadTensor2dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">,</span> InDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>weight<span class="op">,</span> params<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span> InDims <span class="op">*</span> OutDims<span class="op">);</span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>scale<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims<span class="op">);</span></span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims <span class="op">*</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>mean<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims <span class="op">*</span> <span class="dv">3</span><span class="op">);</span></span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the parameter initialization</span></span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a><span class="co">// Read the parameters for a linear and 1D batch normalization layer</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a><span class="co">// from a DDR memory and store them to BRAM buffers</span></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for parameters</span></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">&gt;</span></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadBlockParamsOpt2<span class="op">(</span>LinearParams<span class="op">&lt;</span>T<span class="op">,</span> InDims<span class="op">,</span> OutDims<span class="op">&gt;*</span> linear<span class="op">,</span></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>                         BatchNorm1dParams<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;*</span> bn<span class="op">,</span></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>                         <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params<span class="op">)</span></span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE</span></span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>InDims <span class="op">==</span> <span class="dv">3</span><span class="op">,</span> <span class="st">&quot;`InDims` must be 3&quot;</span><span class="op">);</span></span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>  ReadTensor2dOpt2<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">,</span> InDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>weight<span class="op">,</span> params<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>linear<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span> InDims <span class="op">*</span> OutDims<span class="op">);</span></span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>scale<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims<span class="op">);</span></span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>bias<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims <span class="op">*</span> <span class="dv">2</span><span class="op">);</span></span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>  ReadTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> OutDims<span class="op">&gt;(</span>bn<span class="op">-&gt;</span>mean<span class="op">,</span> params<span class="op">,</span></span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a>                               InDims <span class="op">*</span> OutDims <span class="op">+</span> OutDims <span class="op">*</span> <span class="dv">3</span><span class="op">);</span></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>åŸºæœ¬çš„ã«ã¯å…ƒã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã¨åŒã˜ã§ã™ãŒã€å¼•æ•°ã®å‹ãŒ<code>float*</code>ã‹ã‚‰<code>ap_uint&lt;64&gt;*</code>ã«å¤‰ã‚ã£ã¦ã„ã¾ã™ã€‚
é–¢æ•°ã®ä¸­èº«ã‚‚å˜ç´”ã§ã€æŒ‡å®šã—ãŸã‚ªãƒ•ã‚»ãƒƒãƒˆã‹ã‚‰ã€æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ã“ã¨ã‚’ç¹°ã‚Šè¿”ã™ã ã‘ã§ã™ã€‚
ä¾‹ãˆã°ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ã¨ãã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã®é †ã«èª­ã¿å–ã‚Šã¾ã™ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã«ã¯äºˆã‚ã€æ­£ã—ã„ä½ç½®ã«ã“ã®é †ã§ä¸¦ã¹ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ä¸­ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹é–¢æ•°<code>ReadTensor1dOpt1</code>ã€<code>ReadTensor2dOpt1</code>ã€<code>ReadTensor2dOpt2</code>ã¯æ¬¡ã®é€šã‚Šã§ã™ã€‚</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">union</span> <span class="dt">conv32_t</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>uint32_t<span class="op"> </span>u32<span class="op">;</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> i32<span class="op">;</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">float</span> f<span class="op">;</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">// Interpret float as std::uint32_t</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="kw">inline</span> <span class="bu">std::</span>uint32_t<span class="op"> </span>FloatToU32<span class="op">(</span><span class="at">const</span> <span class="dt">float</span> f<span class="op">)</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">conv32_t</span> conv<span class="op">;</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>  conv<span class="op">.</span>f <span class="op">=</span> f<span class="op">;</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> conv<span class="op">.</span>u32<span class="op">;</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">// Interpret std::uint32_t as float</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="kw">inline</span> <span class="dt">float</span> U32ToFloat<span class="op">(</span><span class="at">const</span> <span class="bu">std::</span>uint32_t<span class="op"> </span>u32<span class="op">)</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">conv32_t</span> conv<span class="op">;</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>  conv<span class="op">.</span>u32 <span class="op">=</span> u32<span class="op">;</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> conv<span class="op">.</span>f<span class="op">;</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a 1D tensor from a DDR memory</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">&gt;</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadTensor1dNaive<span class="op">(</span>T tensor<span class="op">[</span>D0<span class="op">],</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> src<span class="op">,</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>src<span class="op">[</span>offset <span class="op">+</span> i<span class="op">]);</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a 1D tensor from a DDR memory</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">&gt;</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadTensor1dOpt1<span class="op">(</span>T tensor<span class="op">[</span>D0<span class="op">],</span></span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> src<span class="op">,</span></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>D0 <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`D0` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> D0Over2 <span class="op">=</span> D0 <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="dt">int</span> offset2 <span class="op">=</span> offset <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0Over2<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data <span class="op">=</span> src<span class="op">[</span>offset2 <span class="op">+</span> i<span class="op">];</span></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a 2D tensor from a DDR memory</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">,</span> <span class="dt">int</span> D1<span class="op">&gt;</span></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadTensor2dNaive<span class="op">(</span>T tensor<span class="op">[</span>D0<span class="op">][</span>D1<span class="op">],</span></span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> src<span class="op">,</span></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> D1<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>      <span class="at">const</span> <span class="dt">int</span> idx <span class="op">=</span> i <span class="op">*</span> D1 <span class="op">+</span> j<span class="op">;</span></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>      tensor<span class="op">[</span>i<span class="op">][</span>j<span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>src<span class="op">[</span>offset <span class="op">+</span> idx<span class="op">]);</span></span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a 2D tensor from a DDR memory</span></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">,</span> <span class="dt">int</span> D1<span class="op">&gt;</span></span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadTensor2dOpt1<span class="op">(</span>T tensor<span class="op">[</span>D0<span class="op">][</span>D1<span class="op">],</span></span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> src<span class="op">,</span></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>D1 <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`D1` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> D1Over2 <span class="op">=</span> D1 <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="dt">int</span> offset2 <span class="op">=</span> offset <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> D1Over2<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a>      <span class="at">const</span> <span class="dt">int</span> idx <span class="op">=</span> i <span class="op">*</span> D1Over2 <span class="op">+</span> j<span class="op">;</span></span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a>      <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data <span class="op">=</span> src<span class="op">[</span>offset2 <span class="op">+</span> idx<span class="op">];</span></span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a>      tensor<span class="op">[</span>i<span class="op">][</span>j <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a>      tensor<span class="op">[</span>i<span class="op">][</span>j <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a 2D tensor of size (`D0`, 3) from a DDR memory</span></span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">,</span> <span class="dt">int</span> D1<span class="op">&gt;</span></span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadTensor2dOpt2<span class="op">(</span>T tensor<span class="op">[</span>D0<span class="op">][</span>D1<span class="op">],</span></span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> src<span class="op">,</span></span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>D0 <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`D0` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>D1 <span class="op">==</span> <span class="dv">3</span><span class="op">,</span> <span class="st">&quot;`D1` must be 3&quot;</span><span class="op">);</span></span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-114"><a href="#cb25-114" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> Iter <span class="op">=</span> D0 <span class="op">*</span> D1 <span class="op">/</span> <span class="op">(</span><span class="dv">2</span> <span class="op">*</span> <span class="dv">3</span><span class="op">);</span></span>
<span id="cb25-115"><a href="#cb25-115" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="dt">int</span> offset2 <span class="op">=</span> offset <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> Iter<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> src_idx <span class="op">=</span> i <span class="op">*</span> <span class="dv">3</span><span class="op">;</span></span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> dst_idx <span class="op">=</span> i <span class="op">*</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data0 <span class="op">=</span> src<span class="op">[</span>offset2 <span class="op">+</span> src_idx <span class="op">+</span> <span class="dv">0</span><span class="op">];</span></span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data1 <span class="op">=</span> src<span class="op">[</span>offset2 <span class="op">+</span> src_idx <span class="op">+</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data2 <span class="op">=</span> src<span class="op">[</span>offset2 <span class="op">+</span> src_idx <span class="op">+</span> <span class="dv">2</span><span class="op">];</span></span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">0</span><span class="op">][</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data0<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">0</span><span class="op">][</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data0<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">0</span><span class="op">][</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data1<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">1</span><span class="op">][</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data1<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">1</span><span class="op">][</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data2<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a>    tensor<span class="op">[</span>dst_idx <span class="op">+</span> <span class="dv">1</span><span class="op">][</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>tensor_data2<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>æ¯”è¼ƒã§ãã‚‹ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ãšã¤èª­ã¿å–ã‚‹ã€å…ƒã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã‚‚è¼‰ã›ã¾ã—ãŸã€‚
å„é–¢æ•°ã®å‹•ä½œã‚’ã¾ã¨ã‚ã¾ã™ã€‚</p>
<ul>
<li><code>ReadTensor1dOpt1&lt;T, D0&gt;(tensor, src, offset)</code>:
æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡<code>src</code>ã®ã€<code>float</code>ã§<code>offset</code>å€‹åˆ†ã ã‘ãšã‚‰ã—ãŸå ´æ‰€ã‹ã‚‰
(<code>src</code>ã«<code>4 * offset</code>ãƒã‚¤ãƒˆåˆ†ã ã‘è¶³ã—ãŸã‚¢ãƒ‰ãƒ¬ã‚¹ã‹ã‚‰)ã€<code>D0</code>å€‹åˆ†ã®<code>float</code>ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‹ã€‚
èª­ã¿å–ã£ãŸãƒ‡ãƒ¼ã‚¿ã¯<code>float</code>ã‹ã‚‰<code>T</code>å‹ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸ1æ¬¡å…ƒã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡<code>tensor</code>
(ã‚µã‚¤ã‚º<code>(D0)</code>)ã«2ã¤ãšã¤æ ¼ç´ã™ã‚‹ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã§2ã¤ãšã¤èª­ã¿å–ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º<code>D0</code>ã¯å¶æ•°ã¨ä»®å®šã—ã¦ã„ã‚‹ã€‚</li>
<li><code>ReadTensor2dOpt1&lt;T, D0, D1&gt;(tensor, src, offset)</code>:
æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡<code>src</code>ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã£ã¦ã€2æ¬¡å…ƒã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡<code>tensor</code>
(ã‚µã‚¤ã‚º<code>(D0, D1)</code>)ã«æ ¼ç´ã™ã‚‹ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã§2ã¤ãšã¤èª­ã¿å–ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º<code>D1</code>ã¯å¶æ•°ã¨ä»®å®šã—ã¦ã„ã‚‹ã€‚</li>
<li><code>ReadTensor2dOpt2&lt;T, D0, D1&gt;(tensor, src, offset)</code>:
<code>D1</code>ãŒ3ã§ã‚ã‚‹å ´åˆã®å°‚ç”¨ã®å®Ÿè£…ã€‚
3ã‚µã‚¤ã‚¯ãƒ«æ›ã‘ã¦ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡<code>src</code>ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’6ã¤èª­ã¿å–ã£ãŸå¾Œã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡<code>tensor</code>ã«æ ¼ç´ã—ã¦ã„ãã€‚
å®Ÿè£…ã‚’ç°¡ç•¥åŒ–ã™ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚ºã«é–¢ã—ã¦ã¯ã€<code>D1</code>ã¯3ã€<code>D0</code>ã¯å¶æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ä»®å®šã—ã¦ã„ã‚‹
(è¦ç´ æ•°ãŒå¶æ•°)ã€‚</li>
</ul>
<p><code>ReadTensor2dOpt2</code>ãŠã‚ˆã³<code>ReadBlockParamsOpt2</code>ã¯ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹æœ€åˆã®å…¨çµåˆå±¤ã®é‡ã¿ã‚’è»¢é€ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™
(<code>InitializeFeatOpt3</code>ã‚’å‚ç…§)ã€‚
æœ€åˆã®å…¨çµåˆå±¤ã¯ã€3æ¬¡å…ƒã®ç‚¹ã®åº§æ¨™ã‚’64æ¬¡å…ƒã®ç‰¹å¾´ã«å¤‰æ›ã™ã‚‹ã®ã§ã€é‡ã¿ã®ã‚µã‚¤ã‚ºã¯<code>(64, 3)</code>ã¨ãªã‚Šã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚ŠãŸã„ã®ã«ã€2ç•ªç›®ã®æ¬¡å…ƒãŒå¥‡æ•°ã§ã€å®Ÿè£…ä¸Šã®éƒ½åˆãŒæ‚ªã„ã®ã§ã€å°‚ç”¨ã®é–¢æ•°ã‚’ç”¨æ„ã—ãŸã‚ã‘ã§ã™ã€‚
<code>ReadTensor2dOpt2</code>ã§ã¯ã€é‡ã¿ã‚’6ã¤ãšã¤èª­ã¿å–ã‚‹ã“ã¨ã§å¯¾å‡¦ã—ã¦ã„ã¾ã™ã€‚
åˆ¥ã®å¯¾å‡¦æ³•ã¨ã—ã¦ã¯ã€é‡ã¿ã®ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’<code>(64, 3)</code>ã‹ã‚‰<code>(64, 4)</code>ã«åºƒã’ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™
(4ç•ªç›®ã®æ¬¡å…ƒã¯å˜ã«ä½¿ã‚ãªã„)ã€‚</p>
<p><code>ReadBlockParamsOpt1</code>ã¨<code>ReadBlockParamsOpt2</code>ã®é•ã„ã¯ã€<code>ReadTensor2dOpt1</code>ã¨<code>ReadTensor2dOpt2</code>ã®ã©ã¡ã‚‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ã ã‘ã§ã™ã€‚
2ã¤ã®é–¢æ•°ã¯ã€C++17ã«ç”¨æ„ã•ã‚ŒãŸ<code>if constexpr</code>æ–‡ã‚’ä½¿ãˆã°ã€1ã¤ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ãŒã€ä»Šå›ã¯C++14ã¾ã§ã®æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€åˆ¥ã€…ã«ã—ã¦ã„ã¾ã™ã€‚</p>
<p><code>ap_uint</code>å‹ã«ã¯<code>range()</code>ã¨ã„ã†ä¾¿åˆ©ãªãƒ¡ã‚½ãƒƒãƒ‰ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€æŒ‡å®šã—ãŸãƒ“ãƒƒãƒˆã®éƒ¨åˆ†ã‚’è‡ªç”±ã«å–ã‚Šå‡ºã›ã¾ã™ã€‚
<code>range(31, 0)</code>ã§ä¸‹ä½32ãƒ“ãƒƒãƒˆã€<code>range(63, 32)</code>ã§ä¸Šä½32ãƒ“ãƒƒãƒˆã‚’å–ã‚Šå‡ºã—ã¦ã„ã¾ã™ã€‚</p>
<p><code>U32ToFloat()</code>ã€<code>FloatToU32()</code>ã¯ã€ãƒ“ãƒƒãƒˆè¡¨ç¾ã‚’ç¶­æŒã—ãŸã¾ã¾ã€åˆ¥ã®å‹ã«è§£é‡ˆã™ã‚‹ãŸã‚ã®é–¢æ•°ã§ã™
(<code>float</code>ã¨ç¬¦å·ãªã—32ãƒ“ãƒƒãƒˆæ•´æ•°)ã€‚
<code>tensor_data.range(31, 0)</code>ã¯32ãƒ“ãƒƒãƒˆã®ç¬¦å·ãªã—æ•´æ•°å‹
(<code>unsigned int</code>ã‚„<code>ap_uint&lt;32&gt;</code>)
ã§ã™ãŒã€å®Ÿéš›ã«ã¯<code>float</code>ã®ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã€<code>U32ToFloat()</code>ã‚’ä½¿ã£ã¦<code>float</code>ã«è§£é‡ˆã—ç›´ã—ã¦ã„ã¾ã™ã€‚
2ã¤ã®é–¢æ•°ã¯ã€å…±ç”¨ä½“ã‚’ä½¿ã£ã¦å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
C++20ã§ã‚ã‚Œã°ã€<code>std::bit_cast</code>ã§åŒç­‰ã®å‡¦ç†ãŒã§ãã¾ã™ã€‚</p>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–ã«ç€ç›®ã—ã¾ã™
(<code>InferenceFeatOpt2</code>ã‚’å‚ç…§)ã€‚
<code>i</code>ç•ªç›®ã®ç‚¹ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã‚‹<code>ReadPointNaive</code>ã‚‚ã€64ãƒ“ãƒƒãƒˆå¹…ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¾ã™ã€‚
ä¿®æ­£å¾Œã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’<code>ReadPointOpt1</code>ã¨ã—ã¾ã—ãŸã€‚</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a point from a DDR memory</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadPointNaive<span class="op">(</span><span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> idx<span class="op">,</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                    T x<span class="op">[</span><span class="dv">3</span><span class="op">])</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">3</span><span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    x<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>point_cloud<span class="op">[</span>idx <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> i<span class="op">]);</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Read a point from a DDR memory</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">&gt;</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReadPointOpt1<span class="op">(</span><span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> point_cloud<span class="op">,</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>                   <span class="at">const</span> <span class="dt">int</span> idx<span class="op">,</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>                   T x<span class="op">[</span><span class="dv">3</span><span class="op">])</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> point_data0 <span class="op">=</span> point_cloud<span class="op">[</span>idx <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">];</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> point_data1 <span class="op">=</span> point_cloud<span class="op">[</span>idx <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>  x<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>point_data0<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>  x<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>point_data0<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>  x<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> T<span class="op">(</span>U32ToFloat<span class="op">(</span>point_data1<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>ReadPointNaive</code>ã§ã¯ã€DRAMãƒãƒƒãƒ•ã‚¡<code>point_cloud</code>ã®ã‚µã‚¤ã‚ºãŒ<span
class="math inline">\((N, 3)\)</span>ã§ã‚ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã—ãŸã€‚
ä¸€æ–¹<code>ReadPointOpt1</code>ã§ã¯ã€å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºãŒ<span
class="math inline">\((N, 4)\)</span>ã§ã‚ã‚‹ã¨ã—ã¾ã™
(4ç•ªç›®ã®æ¬¡å…ƒã«ã¤ã„ã¦ã¯ä½¿ã‚ãªã„)ã€‚
<code>i</code>ç•ªç›®ã®ç‚¹ã‚’èª­ã¿å–ã‚‹ã¨ãã¯ã€ãƒãƒƒãƒ•ã‚¡ã®<code>idx * 2 + 0</code>ç•ªç›®ã¨<code>idx * 2 + 1</code>ç•ªç›®ã‚’å‚ç…§ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚</p>
<p>æœ€å¾Œã«ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–ã‚’ç›´ã—ã¾ã™
(<code>InferenceClsOpt1</code>ã‚’å‚ç…§)ã€‚
ç‚¹ç¾¤ã®ç‰¹å¾´é‡ã‹ã‚‰ã€ç‰©ä½“ã®å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã—ã€<code>WriteTensor1dNaive</code>ã«ã‚ˆã‚ŠDRAMãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã‚“ã§ã„ã¾ã™ã€‚
<code>WriteTensor1dNaive</code>ã‚’ã€64ãƒ“ãƒƒãƒˆå¹…ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¾ã™ã€‚
ä¿®æ­£å¾Œã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’<code>WriteTensor1dOpt1</code>ã¨ã—ã¾ã—ãŸã€‚</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Write a 1D tensor to a DDR memory</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">&gt;</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> WriteTensor1dNaive<span class="op">(</span><span class="dt">float</span><span class="op">*</span> dst<span class="op">,</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> T tensor<span class="op">[</span>D0<span class="op">],</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    dst<span class="op">[</span>offset <span class="op">+</span> i<span class="op">]</span> <span class="op">=</span> <span class="kw">static_cast</span><span class="op">&lt;</span><span class="dt">float</span><span class="op">&gt;(</span>tensor<span class="op">[</span>i<span class="op">]);</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Write a 1D tensor to a DDR memory</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="dt">int</span> D0<span class="op">&gt;</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> WriteTensor1dOpt1<span class="op">(</span>ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> dst<span class="op">,</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> T tensor<span class="op">[</span>D0<span class="op">],</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>                       <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>D0 <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`D0` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> D0Over2 <span class="op">=</span> D0 <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="dt">int</span> offset2 <span class="op">=</span> offset <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> D0Over2<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> tensor_data<span class="op">;</span></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)</span> <span class="op">=</span> FloatToU32<span class="op">(</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>      <span class="kw">static_cast</span><span class="op">&lt;</span><span class="dt">float</span><span class="op">&gt;(</span>tensor<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">]));</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    tensor_data<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)</span> <span class="op">=</span> FloatToU32<span class="op">(</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>      <span class="kw">static_cast</span><span class="op">&lt;</span><span class="dt">float</span><span class="op">&gt;(</span>tensor<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">]));</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    dst<span class="op">[</span>offset2 <span class="op">+</span> i<span class="op">]</span> <span class="op">=</span> tensor_data<span class="op">;</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡<code>tensor</code>ã«ç½®ã‹ã‚ŒãŸã‚µã‚¤ã‚º<code>(D0)</code>ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€1ã‚µã‚¤ã‚¯ãƒ«ã«2ã¤ãšã¤ã€DRAMã«æ›¸ãæˆ»ã—ã¦ã„ã¾ã™ã€‚
å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€<code>D0</code>ã¯å¶æ•°ã§ã‚ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚
2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã¯<code>T</code>å‹ã§ã™ãŒã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰åˆ©ç”¨ã—ã‚„ã™ã„ã‚ˆã†ã«<code>float</code>ã«ç›´ã—ã€æ›´ã«<code>FloatToU32</code>ã‚’ä½¿ã£ã¦ã€ãƒ“ãƒƒãƒˆè¡¨ç¾ã‚’ç¶­æŒã—ãŸã¾ã¾32ãƒ“ãƒƒãƒˆã®ç¬¦å·ãªã—æ•´æ•°å‹ã«å†è§£é‡ˆã—ã¦ã„ã¾ã™ã€‚
ã“ã‚Œã‚‰2ã¤ã‚’ã€<code>ap_uint&lt;64&gt;</code>å‹ã®ä¸Šä½32ãƒ“ãƒƒãƒˆã¨ä¸‹ä½32ãƒ“ãƒƒãƒˆã«è©°ã‚ã¦ã€DRAMãƒãƒƒãƒ•ã‚¡ã«æ›¸ãæˆ»ã—ã¦ã„ã¾ã™ã€‚</p>
<p>æœ€åˆã®2ã¤ã®å…¨çµåˆå±¤ (<code>LinearOpt1DDR</code>)
ã‚‚ç›´ã—ã¦ã€æ–°ãŸã«<code>LinearOpt2DDR</code>ã‚’ä½œã‚Šã¾ã™ã€‚
é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®è»¢é€éƒ¨åˆ†ã‚’å¤‰æ›´ã—ã¾ã™ã€‚
è»¢é€ã«è¦ã™ã‚‹ã‚µã‚¤ã‚¯ãƒ«æ•°ãŒåŠåˆ†ã»ã©ã«ãªã‚‹ã®ã§ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã®å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚
å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€å…¥å‡ºåŠ›ã®æ¬¡å…ƒãŒã„ãšã‚Œã‚‚å¶æ•°ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Parallel implementation of the fully-connected layer</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Weight and bias parameters are stored on the DDR memory</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Matrix-vector multiplication is parallelized along the output dimension</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">// `T` is the type for values</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">// `TParam` is the type for weight and bias</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">// `InDims` is the number of input dimensions</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">// `OutDims` is the number of output dimensions</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">// `ApplyReLU` is the flag to apply ReLU activation</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">// `B` is the block size for the output dimension</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> TParam<span class="op">,</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>          <span class="dt">int</span> InDims<span class="op">,</span> <span class="dt">int</span> OutDims<span class="op">,</span> <span class="dt">bool</span> ApplyReLU<span class="op">,</span> <span class="dt">int</span> B<span class="op">&gt;</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> LinearOpt2DDR<span class="op">(</span><span class="at">const</span> T x<span class="op">[</span>InDims<span class="op">],</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>                   T y<span class="op">[</span>OutDims<span class="op">],</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>                   <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;*</span> params<span class="op">,</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>                   <span class="at">const</span> <span class="dt">int</span> offset<span class="op">)</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `x` is of size (1, `InDims`)</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `y` is of size (1, `OutDims`)</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `params` contains weight parameters of size (`OutDims`, `InDims`) and</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">// bias parameters of size (`OutDims`) in a contiguous buffer</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `OutDims` must be a multiple of `B`</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> B <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of `B`&quot;</span><span class="op">);</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `B` must be larger than 1</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>B <span class="op">&gt;</span> <span class="dv">1</span><span class="op">,</span> <span class="st">&quot;`B` must be larger than 1&quot;</span><span class="op">);</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `InDims` must be a multiple of 2</span></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>InDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`InDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `OutDims` must be a multiple of 2</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>  <span class="kw">static_assert</span><span class="op">(</span>OutDims <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">,</span> <span class="st">&quot;`OutDims` must be a multiple of 2&quot;</span><span class="op">);</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>  <span class="co">// `offset` must be a multiple of 2</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>  <span class="ot">assert</span><span class="op">(</span>offset <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> BHalf <span class="op">=</span> B <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> OffsetToBias <span class="op">=</span> OutDims <span class="op">*</span> InDims <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> InDims2 <span class="op">=</span> InDims <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> <span class="at">const</span> <span class="dt">int</span> OutDims2 <span class="op">=</span> OutDims <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">const</span> <span class="dt">int</span> offset2 <span class="op">=</span> offset <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>  TParam bias<span class="op">[</span>OutDims<span class="op">];</span></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1</span></span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Copy the bias parameters in advance</span></span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> OutDims2<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE II=1</span></span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> bias_data <span class="op">=</span> params<span class="op">[</span>offset2 <span class="op">+</span> OffsetToBias <span class="op">+</span> i<span class="op">];</span></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>U32ToFloat<span class="op">(</span>bias_data<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">[</span>i <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span>U32ToFloat<span class="op">(</span>bias_data<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i0 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i0 <span class="op">&lt;</span> OutDims<span class="op">;</span> i0 <span class="op">+=</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE off</span></span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>    T vals<span class="op">[</span>B<span class="op">];</span></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1</span></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>    TParam weight<span class="op">[</span>B<span class="op">][</span>InDims<span class="op">];</span></span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1</span></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Copy the weight parameters for `B` outputs</span></span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">int</span> offset0 <span class="op">=</span> offset2 <span class="op">+</span> i0 <span class="op">*</span> InDims2<span class="op">;</span></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims2<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>        <span class="at">const</span> ap_uint<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;</span> weight_data <span class="op">=</span> params<span class="op">[</span>offset0 <span class="op">+</span> i1 <span class="op">*</span> InDims2 <span class="op">+</span> j<span class="op">];</span></span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>        weight<span class="op">[</span>i1<span class="op">][</span>j <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span></span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>          U32ToFloat<span class="op">(</span>weight_data<span class="op">.</span>range<span class="op">(</span><span class="dv">31</span><span class="op">,</span> <span class="dv">0</span><span class="op">)));</span></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>        weight<span class="op">[</span>i1<span class="op">][</span>j <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> TParam<span class="op">(</span></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>          U32ToFloat<span class="op">(</span>weight_data<span class="op">.</span>range<span class="op">(</span><span class="dv">63</span><span class="op">,</span> <span class="dv">32</span><span class="op">)));</span></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> InDims<span class="op">;</span> <span class="op">++</span>j<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> OutDims<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a>          T last <span class="op">=</span> <span class="op">(</span>j <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">?</span> T<span class="op">(</span>bias<span class="op">[</span>i<span class="op">])</span> <span class="op">:</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>          vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">=</span> last <span class="op">+</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> weight<span class="op">[</span>i1<span class="op">][</span>j<span class="op">];</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i1 <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i1 <span class="op">&lt;</span> B<span class="op">;</span> <span class="op">++</span>i1<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a>      <span class="dt">int</span> i <span class="op">=</span> i0 <span class="op">+</span> i1<span class="op">;</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> OutDims<span class="op">)</span> <span class="op">{</span></span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>ApplyReLU<span class="op">)</span></span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a>          y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">&gt;</span> T<span class="op">(</span><span class="dv">0</span><span class="op">)</span> <span class="op">?</span> vals<span class="op">[</span>i1<span class="op">]</span> <span class="op">:</span> T<span class="op">(</span><span class="dv">0</span><span class="op">);</span></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a>          y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> vals<span class="op">[</span>i1<span class="op">];</span></span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®å…¥å‡ºåŠ›ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’ä¿®æ­£ã—ã¾ã—ãŸã€‚
<code>InferenceFeatOpt2</code>ã¨<code>InferenceClsOpt1</code>ã«å¯¾ã—ã¦ã€ä¿®æ­£ã‚’æ–½ã—ãŸã‚‚ã®ã‚’<code>InferenceFeatOpt3</code>ã€<code>InferenceClsOpt3</code>ã¨ã—ã¾ã™ã€‚
<code>InferenceFeatOpt3</code>ã§ã¯ã€ç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹éš›ã«ã€<code>ReadPointNaive</code>ã®ä»£ã‚ã‚Šã«<code>ReadPointOpt1</code>ã‚’ä½¿ã£ã¦ã„ã¾ã™
(ä»–ã¯åŒã˜)ã€‚
ã¾ãŸ<code>InferenceClsOpt3</code>ã§ã¯ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’æ›¸ãè¾¼ã‚€éš›ã«ã€<code>WriteTensor1dNaive</code>ã§ã¯ãªã<code>WriteTensor1dOpt1</code>ã‚’ä½¿ã„ã€æœ€åˆã®2ã¤ã®å…¨çµåˆå±¤ã«ã¤ã„ã¦ã¯ã€<code>LinearOpt1DDR</code>ã®ä»£ã‚ã‚Šã«<code>LinearOpt2DDR</code>ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">,</span> <span class="dt">int</span> N<span class="op">&gt;</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceFeatOpt3<span class="op">(...)</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Zero-initialize the output feature</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  VectorNdSetZero<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">&gt;(</span>feature<span class="op">);</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute the feature</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> num_points<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ...</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Read a point from a DDR memory</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    ReadPointOpt1<span class="op">&lt;</span>T<span class="op">&gt;(</span>point_cloud<span class="op">,</span> i<span class="op">,</span> x0<span class="op">);</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute a point feature</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// ...</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Update the output feature</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    MaxPool1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> kFeatDims5<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span>x10<span class="op">,</span> feature<span class="op">);</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="kw">typename</span> T<span class="op">,</span> <span class="kw">typename</span> U<span class="op">&gt;</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> InferenceClsOpt3<span class="op">(...)</span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INLINE off</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Compute logits</span></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>  LinearOpt2DDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims0<span class="op">,</span> kClsDims1<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">16</span><span class="op">&gt;(</span></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    feature<span class="op">,</span> x0<span class="op">,</span> params1<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    x0<span class="op">,</span> x1<span class="op">,</span> bn1<span class="op">-&gt;</span>scale<span class="op">,</span> bn1<span class="op">-&gt;</span>bias<span class="op">,</span> bn1<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>  LinearOpt2DDR<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims1<span class="op">,</span> kClsDims2<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>    x1<span class="op">,</span> x2<span class="op">,</span> params2<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>  BatchNorm1dReLUOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>    x2<span class="op">,</span> x3<span class="op">,</span> bn2<span class="op">-&gt;</span>scale<span class="op">,</span> bn2<span class="op">-&gt;</span>bias<span class="op">,</span> bn2<span class="op">-&gt;</span>mean<span class="op">);</span></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>  LinearOpt1<span class="op">&lt;</span>T<span class="op">,</span> U<span class="op">,</span> kClsDims2<span class="op">,</span> kClsDims3<span class="op">,</span> <span class="kw">false</span><span class="op">,</span> <span class="dv">2</span><span class="op">&gt;(</span></span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>    x3<span class="op">,</span> x4<span class="op">,</span> fc3<span class="op">-&gt;</span>weight<span class="op">,</span> fc3<span class="op">-&gt;</span>bias<span class="op">);</span></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Write the result</span></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>  WriteTensor1dOpt1<span class="op">&lt;</span>T<span class="op">,</span> kClsDims3<span class="op">&gt;(</span>out_logits<span class="op">,</span> x4<span class="op">,</span> <span class="dv">0</span><span class="op">);</span></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆå¹…ã«ã‚ˆã£ã¦ã€ã©ã®ç¨‹åº¦å®Ÿè¡Œæ™‚é–“ã‚’å‰Šæ¸›ã§ããŸã§ã—ã‚‡ã†ã‹ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯<code>InferenceFeatOpt2</code>ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã¯1,112,259
(7.408ms)ã€æ–°ãŸã«ç”¨æ„ã—ãŸ<code>InferenceFeatOpt3</code>ã¯1,112,254
(7.408ms) ã§ã—ãŸã€‚ ã»ã¼ä¸€ç·’ã§ã™ã€‚
åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã€ãƒãƒ¼ãƒˆå¹…32ãƒ“ãƒƒãƒˆç”¨ã®<code>InferenceClsOpt1</code>ã¯711,969ã‚µã‚¤ã‚¯ãƒ«
(4.742ms)
ã§ã—ãŸãŒã€64ãƒ“ãƒƒãƒˆç”¨ã®<code>InferenceClsOpt3</code>ã§ã¯383,885ã‚µã‚¤ã‚¯ãƒ«
(2.557ms) ã«å‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚
ãƒãƒ¼ãƒˆå¹…ã‚’2å€ã«åºƒã’ãŸã“ã¨ã§ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã‚’1.85å€çŸ­ç¸®ã§ããŸã‚ã‘ã§ã™ã€‚</p>
<p>å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£… (<code>InferenceFeatNaive</code> +
<code>InferenceClsNaive</code>) ã¨ã€ã“ã“ã«ç¤ºã™å®Ÿè£…
(<code>InferenceFeatOpt3</code> + <code>InferenceClsOpt3</code>)
ã¨ã§ã€å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã¯ã©ã®ç¨‹åº¦å¤‰åŒ–ã—ãŸã§ã—ã‚‡ã†ã‹ã€‚
ä¸ŠãŒãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã€ä¸‹ãŒæœ€é©åŒ–æ¸ˆã¿ã®å®Ÿè£…ã§ã®çµæœã§ã™ã€‚
ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã§ã¯ã€æ¨è«–ã«163,279,213ã‚µã‚¤ã‚¯ãƒ« (1.087s)
è¦ã—ã¦ã„ã¾ã™ãŒã€æœ€é©åŒ–ã«ã‚ˆã£ã¦1,496,143ã‚µã‚¤ã‚¯ãƒ« (9.964ms)
ã«ã¾ã§å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚ ãŠã‚ˆã109å€ã®å·®ã§ã™ã­ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-naive-clock-cycles.png"><img src="point-cloud-classification-images/pointnet-naive-clock-cycles.png" width="80%" /></a></p>
<p><a
href="point-cloud-classification-images/pointnet-opt3-clock-cycles.png"><img src="point-cloud-classification-images/pointnet-opt3-clock-cycles.png" width="80%" /></a></p>
<p>ä»¥ä¸Šã§ã€é«˜ä½åˆæˆã®å®Ÿè£…ãŒã§ãã‚ãŒã‚Šã¾ã—ãŸã€‚
<code>hls/src/top_opt3.cpp</code>ã‚’ã”è¦§ãã ã•ã„ã€‚</p>
<h2 id="ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æº–å‚™">ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æº–å‚™</h2>
<p>é«˜ä½åˆæˆã®å®Ÿè£…ãŒã§ããŸã®ã§ã€Vitis
HLSã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€IPã‚³ã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚
ä»Šå›ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç’°å¢ƒã§ä½œæ¥­ã—ã¦ã„ã¾ã™
(è©¦ã™äººã¯ã„ãªã„ã¨æ€ã„ã¾ã™ãŒæ›¸ã„ã¦ãŠãã¾ã™)ã€‚</p>
<ul>
<li>Ubuntu 20.04.5 LTS</li>
<li>Intel(R) Xeon(R) E-2186G CPU @ 3.80GHz</li>
<li>64GB DRAM</li>
<li>Vivado ML Edition 2022.1
(ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å ´æ‰€ã¯<code>/tools/Xilinx</code>ä»¥ä¸‹)</li>
<li>CMake 3.16.3</li>
</ul>
<p>ã¾ãŸã€å¯¾è±¡ã®FPGAãƒœãƒ¼ãƒ‰ã¯ã€Xilinx ZCU104 Evaluation Board
(XCZU7EV-2FFVC1156)ã§ã™ã€‚</p>
<p>ä»Šå›ç”¨æ„ã—ãŸGitHubãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«<code>make</code>ã™ã‚‹ã ã‘ã§ã€è‡ªå‹•çš„ã«IPã‚³ã‚¢ã‚’ä½œæˆã§ãã¾ã™ã€‚
Tclã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨CMakeã‚’çµ„ã¿åˆã‚ã›ã¦å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚
ä¸Šã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ã«ã€Vitis
HLSã«ã¯GUIãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ãŒã€Tclã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ãˆã°ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä¸Šã§ã®ãƒãƒƒãƒå‡¦ç†ãŒå¯èƒ½ã§ã™ã€‚
é©å½“ãªå ´æ‰€ã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ãŸã‚‰ã€<code>hls</code>ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»ã£ã¦ã€ä½œæ¥­ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™ã—ã¾ã™ã€‚
ç¶šã„ã¦CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆã—ã€æ‰€æœ›ã®IPã‚³ã‚¢ã‚’<code>make</code>ã§ä½œæˆã—ã¾ã™ã€‚</p>
<pre><code># äºˆã‚Vivadoã¨Vitis HLSã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«sourceã™ã‚‹
&gt; source /tools/Xilinx/Vivado/2022.1/settings64.sh

# GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
&gt; git clone git@github.com:sterngerlach/advent_2022_point_cloud_classification.git
&gt; cd advent_2022_point_cloud_classification

# ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
&gt; cd hls
&gt; mkdir build
&gt; mkdir work

&gt; cd build

# CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆ
# settings64.shã«ã‚ˆã£ã¦CMakeãŒæ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã®CMakeã‚’ä½¿ã†
&gt; /usr/bin/cmake ..

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã‹ã‚‰IPã‚³ã‚¢ã‚’ä½œæˆ
# workãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ä½œã‚‰ã‚Œã‚‹
&gt; make pointnet_naive_150_csynth_export

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) IPã‚³ã‚¢ã‚’ä½œæˆ
&gt; make pointnet_opt1_csynth_export

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸIPã‚³ã‚¢ã‚’ä½œæˆ
&gt; make pointnet_opt2_csynth_export

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸIPã‚³ã‚¢ã‚’ä½œæˆ
&gt; make pointnet_opt3_csynth_export</code></pre>
<p>IPã‚³ã‚¢ã‚’ä½œæˆã—ãŸã‚‰ã€GUIã‚’èµ·å‹•ã—ã¦ã€åˆæˆçµæœã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†
(ä¸Šã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ãªç”»é¢ãŒé–‹ãã¾ã™)ã€‚</p>
<pre><code>&gt; cd hls/work

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ç”¨ã®Vitis HLSãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’GUIã§é–‹ã
&gt; vitis_hls -p pointnet_naive_150

# ä»–ã‚‚åŒæ§˜
&gt; vitis_hls -p pointnet_opt1
&gt; vitis_hls -p pointnet_opt2
&gt; vitis_hls -p pointnet_opt3</code></pre>
<p>Vitis
HLSã‚’ä½¿ã†ã®ã¯ã“ã“ã¾ã§ã§ã€ã“ã‚Œä»¥é™ã¯ã€Vivadoã‚’ä½¿ã£ãŸä½œæ¥­ã«ç§»ã‚Šã¾ã™ã€‚
ç¶šã„ã¦ã€ã“ã®IPã‚³ã‚¢ã‚’ã€åˆ¥ã®IPã‚³ã‚¢ã¨çµ„ã¿åˆã‚ã›ã¦ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ç”¨æ„ã—ã¾ã™ã€‚
ä»Šå›ã¯ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã®ä½œæˆã«ã¤ã„ã¦ã¯çœç•¥ã—ã¾ã™ã€‚
æœ€åˆã«ã€<code>vivado</code>ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»ã£ã¦ã€ä½œæ¥­ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™ã—ã¾ã™ã€‚
ç¶šã„ã¦CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆã—ã€æ‰€æœ›ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’<code>make</code>ã§ä½œæˆã—ã¾ã™ã€‚</p>
<pre><code># ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
&gt; cd vivado
&gt; mkdir build
&gt; mkdir work
&gt; mkdir bitstream

&gt; cd build

# CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆ
# settings64.shã«ã‚ˆã£ã¦CMakeãŒæ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã®CMakeã‚’ä½¿ã†
# Vitis HLSã«ã‚ˆã‚‹IPã‚³ã‚¢ã®åˆæˆãŒçµ‚ã‚ã£ã¦ã„ãªã„ã¨ã‚¨ãƒ©ãƒ¼
&gt; /usr/bin/cmake ..

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã®IPã‚³ã‚¢ã‹ã‚‰ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆ
&gt; make pointnet_naive_150_create

# æœ€é©åŒ–æ¸ˆã¿ã®IPã‚³ã‚¢ã‹ã‚‰ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆ
&gt; make pointnet_opt1_create
&gt; make pointnet_opt2_create
&gt; make pointnet_opt3_create</code></pre>
<p>ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆã—ãŸã‚‰ã€GUIã‚’èµ·å‹•ã—ã¦ã€ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</p>
<pre><code>&gt; cd vivado/work
&gt; vivado -project pointnet_naive_150/pointnet_naive_150.xpr
&gt; vivado -project pointnet_opt1/pointnet_opt1.xpr
&gt; vivado -project pointnet_opt2/pointnet_opt2.xpr
&gt; vivado -project pointnet_opt3/pointnet_opt3.xpr</code></pre>
<p><a
href="point-cloud-classification-images/pointnet-opt3-vivado.png"><img src="point-cloud-classification-images/pointnet-opt3-vivado.png" width="80%" /></a></p>
<p>å·¦å´ã®Flow Navigatorã‹ã‚‰ã€ã€ŒOpen Block
Designã€ã‚’é¸æŠã™ã‚‹ã¨ã€ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-opt3-vivado2.png"><img src="point-cloud-classification-images/pointnet-opt3-vivado2.png" width="80%" /></a></p>
<p>ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’æ‹¡å¤§ã—ãŸã‚‚ã®ãŒä»¥ä¸‹ã§ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/board-design.svg"><img src="point-cloud-classification-images/board-design.svg" width="100%" /></a></p>
<p>ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã«å¯¾ã—ã¦ã€è«–ç†åˆæˆã¨é…ç½®é…ç·šã‚’è¡Œã„ã€å›è·¯æƒ…å ±ã‚’ã¾ã¨ã‚ãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
(Bitstream) ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚
ãƒã‚·ãƒ³ã®ã‚¹ãƒšãƒƒã‚¯ã«ã‚‚ã‚ˆã‚Šã¾ã™ãŒã€ã“ã¡ã‚‰ã®ç’°å¢ƒã§ã¯ã€1ã¤ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã®è«–ç†åˆæˆã¨é…ç½®é…ç·šã«ã€30åˆ†ä»¥ä¸Šæ›ã‹ã‚Šã¾ã—ãŸ
(8ã‚³ã‚¢ã‚’ä½¿ã£ãŸå ´åˆ)ã€‚
ä»Šå›ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã¯ã€ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚‚å…¥ã‚Œã¦ã‚ã‚‹ã®ã§ã€ã“ã®ä½œæ¥­ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“
(è©¦ã—ã¦ã¿ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™)ã€‚</p>
<pre><code>&gt; cd vivado/build
&gt; make pointnet_naive_150_impl &amp;&amp; make pointnet_naive_150_copy_bitstream
&gt; make pointnet_opt1_impl &amp;&amp; make pointnet_opt1_copy_bitstream
&gt; make pointnet_opt2_impl &amp;&amp; make pointnet_opt2_copy_bitstream
&gt; make pointnet_opt3_impl &amp;&amp; make pointnet_opt3_copy_bitstream</code></pre>
<p>ã‚‚ã†ä¸€åº¦GUIã‚’èµ·å‹•ã—ã¦ã€åˆæˆæ¸ˆã¿ã®å›è·¯ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ å·¦å´ã®Flow
Navigatorã‹ã‚‰ã€ã€ŒOpen Implemented Designã€ã‚’é¸æŠã—ã¾ã™ã€‚
å€‹äººçš„ã«ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã®ãƒãƒ³ãƒãƒƒã‚¿ãƒ³ã®ã‚ˆã†ã«ã¿ãˆã¦ã€ç¾ã—ã„ã¨æ€ã„ã¾ã™ã€‚
GUIä¸Šã§ã€ãƒªã‚½ãƒ¼ã‚¹ã®ä½¿ç”¨ç‡ (Utilization) ã‚„ã€é›»åŠ›æ¶ˆè²»ã®è¦‹ç©ã‚‚ã‚Š
(Power)ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚° (Timing) ãªã©ã‚’ç¢ºèªã§ãã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-opt3-vivado3.png"><img src="point-cloud-classification-images/pointnet-opt3-vivado3.png" width="80%" /></a></p>
<p><code>vivado/bitstream</code>ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã«ã€ç”Ÿæˆã•ã‚ŒãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒã‚³ãƒ”ãƒ¼ã•ã‚Œã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ  (æ‹¡å¼µå­<code>.bit</code>) ã®ä»–ã«ã€Hardware
Handoffãƒ•ã‚¡ã‚¤ãƒ« (æ‹¡å¼µå­<code>.hwh</code>) ã‚‚ã‚ã‚Šã¾ã™ã€‚
Handoffãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€å›è·¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚
FPGAãƒœãƒ¼ãƒ‰ã«ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ã¯ã€2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚»ãƒƒãƒˆã§å¿…è¦ã«ãªã‚Šã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’èª­ã¿ç›´ã›ã°ã€å‹•ã‹ã™å›è·¯ã‚’ä½•åº¦ã§ã‚‚åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã¨ã„ã†ã®ãŒã€ASICã«å¯¾ã™ã‚‹FPGAã®å¤§ããªåˆ©ç‚¹ã§ã™ã€‚
ã•ã¦ã€ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’<code>scp</code>ãªã©ã§FPGAãƒœãƒ¼ãƒ‰ä¸Šã«è»¢é€ã™ã‚Œã°ã€å›è·¯ã‚’å‹•ã‹ã™æº–å‚™ãŒæ•´ã„ã¾ã™ã€‚</p>
<pre><code>&gt; cd vivado/bitstream
&gt; ls
-rw-rw-r-- 1 x x  19M Dec 14 23:34 pointnet_naive_150.bit
-rw-rw-r-- 1 x x 363K Dec 14 23:34 pointnet_naive_150.hwh
-rw-rw-r-- 1 x x  19M Dec 15 00:01 pointnet_opt1.bit
-rw-rw-r-- 1 x x 363K Dec 15 00:01 pointnet_opt1.hwh
-rw-rw-r-- 1 x x  19M Dec 14 23:20 pointnet_opt2.bit
-rw-rw-r-- 1 x x 363K Dec 14 23:20 pointnet_opt2.hwh
-rw-rw-r-- 1 x x  19M Dec 15 18:07 pointnet_opt3.bit
-rw-rw-r-- 1 x x 363K Dec 15 18:07 pointnet_opt3.hwh</code></pre>
<h2 id="å›è·¯ã‚’å‹•ã‹ã™">å›è·¯ã‚’å‹•ã‹ã™</h2>
<p>ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ç”¨æ„ã§ããŸã®ã§ã€ã„ã‚ˆã„ã‚ˆå›è·¯ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚
ä»Šå›ä½¿ç”¨ã™ã‚‹FPGAãƒœãƒ¼ãƒ‰ã€Xilinx ZCU104 Evaluation Kitã¯ã€SoC
(System-on-Chip) ã¨ã‚ˆã°ã‚Œã¦ã„ã¾ã™ã€‚ FPGAã®ä»–ã«ã€ã‚¯ã‚¢ãƒƒãƒ‰ã‚³ã‚¢ ARM
Cortex-A53 CPU
(1.2GHz)ã€2GBã®DRAMã‚„ã€æ§˜ã€…ãªå‘¨è¾ºå›è·¯ãŒçµ±åˆã•ã‚Œã¦ã„ã¦ã€LinuxãŒå‹•ä½œã—ã¾ã™ã€‚
ã“ã“ã§ã¯OSã¨ã—ã¦ã€Ubuntu 20.04ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸPynq Linux 2.7ã‚’ä½¿ã„ã¾ã™ã€‚
Pynq
Linuxã«ã¯<code>pynq</code>ã¨ã‚ˆã°ã‚Œã‚‹Pythonã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä»˜å±ã—ã¦ãŠã‚Šã€Pythonã‹ã‚‰FPGAé–¢é€£ã®å‡¦ç†ã‚’ç°¡å˜ã«è¡Œãˆã¾ã™ã€‚</p>
<p>ä»¥ä¸‹ã‚’è©¦ã™ãŸã‚ã«ã¯ã€Pynq Linuxä¸Šã«ã€PyTorch 1.11.0ã‚„ã€TorchVision
0.12.0ã€NumPyã€SciPyã€H5pyã€Tqdmãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’äºˆã‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ã“ã“ã§ã¯èª¬æ˜ãŒé•·ããªã£ã¦ã—ã¾ã†ãŸã‚å‰²æ„›ã—ã¾ã™ã€‚
åŸºæœ¬çš„ã«ã¯<code>pip</code>ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚ ãªãŠã€Xilinx
ZCU104ã€Pynq Linux 2.7ç”¨ã«ãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸPyTorch 1.11.0ã€TorchVision
0.12.0ã®Wheelãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€<a
href="https://github.com/sterngerlach/pytorch-pynq-builds">ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒª</a>ã«ç½®ã„ã¦ã‚ã‚Šã¾ã™ã€‚
ã“ã“ã¾ã§è‹¦åŠ´ã—ã¦ã€ãªãœFPGAä¸Šã§æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ãã†ã¨ã™ã‚‹ã®ã‹ã€ãŸã¾ã«è‡ªå•è‡ªç­”ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p>ã“ã‚Œä»¥é™ã¯C/C++ã§ã¯ãªãã€Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ãã¾ã™ã€‚</p>
<p>æœ€åˆã«ã€PyTorchã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã‚’å†æ²ã—ã¾ã™
(<code>net/model.py</code>)ã€‚ ä½•ã®æ»ã‚Šã‚‚ãªãã€ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã­ã€‚</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PointNetFeat(torch.nn.Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv4 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">1</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv5 <span class="op">=</span> torch.nn.Conv1d(<span class="dv">128</span>, <span class="dv">1024</span>, <span class="dv">1</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn3 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">64</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn4 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn5 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">1024</span>)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, N, 3]</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 3, N]</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024, N]</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.conv1(x)))</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn2(<span class="va">self</span>.conv2(x)))</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn3(<span class="va">self</span>.conv3(x)))</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn4(<span class="va">self</span>.conv4(x)))</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn5(<span class="va">self</span>.conv5(x)))</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024]</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.<span class="bu">max</span>(x, dim<span class="op">=</span><span class="dv">2</span>)[<span class="dv">0</span>]</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PointNetCls(torch.nn.Module):</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes: <span class="bu">int</span>):</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feat <span class="op">=</span> PointNetFeat()</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification network</span></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> torch.nn.Linear(<span class="dv">1024</span>, <span class="dv">512</span>)</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> torch.nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>)</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> torch.nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">512</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> torch.nn.BatchNorm1d(<span class="dv">256</span>)</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, N, 3]</span></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, 1024]</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.feat(x)</span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, `num_classes`]</span></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn1(<span class="va">self</span>.fc1(x)))</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.bn2(<span class="va">self</span>.fc2(x)))</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
<p>æ¬¡ã«ã€FPGAã§é«˜é€ŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç¤ºã—ã¾ã™
(<code>host/model_zcu104.py</code>)ã€‚
ãƒ¢ãƒ‡ãƒ«ã®åå‰ã¯<code>PointNetClsZCU104</code>ã§ã™ã€‚ ä¸Šè¨˜ã®CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«
(<code>PointNetCls</code>) ã¨ã€ä½¿ã„å‹æ‰‹ãŒåŒã˜ã«ãªã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> net.model <span class="im">import</span> PointNetCls</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the 64-bit address</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_address(addr: <span class="bu">int</span>) <span class="op">-&gt;</span> Tuple[<span class="bu">int</span>, <span class="bu">int</span>]:</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> (<span class="dv">1</span> <span class="op">&lt;&lt;</span> <span class="dv">32</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> addr <span class="op">&amp;</span> mask, addr <span class="op">&gt;&gt;</span> <span class="dv">32</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Allocate a contiguous buffer for torch.nn.Conv1d (torch.nn.Linear)</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> allocate_linear_buffer(in_dims: <span class="bu">int</span>, out_dims: <span class="bu">int</span>) <span class="op">\</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">-&gt;</span> pynq.<span class="bu">buffer</span>.PynqBuffer:</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    buf_size <span class="op">=</span> in_dims <span class="op">*</span> out_dims <span class="op">+</span> out_dims</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pynq.allocate(shape<span class="op">=</span>(buf_size,), dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Allocate a contiguous buffer for a block with torch.nn.Conv1d</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="co"># (torch.nn.Linear) and torch.nn.BatchNorm1d</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> allocate_block_buffer(in_dims: <span class="bu">int</span>, out_dims: <span class="bu">int</span>) <span class="op">\</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    <span class="op">-&gt;</span> pynq.<span class="bu">buffer</span>.PynqBuffer:</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    buf_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    buf_size <span class="op">+=</span> in_dims <span class="op">*</span> out_dims <span class="op">+</span> out_dims</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    buf_size <span class="op">+=</span> out_dims <span class="op">*</span> <span class="dv">3</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pynq.allocate(shape<span class="op">=</span>(buf_size,), dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the torch.nn.Conv1d parameters to the contiguous buffer</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_conv1d_params(buf: pynq.<span class="bu">buffer</span>.PynqBuffer,</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>                        layer: torch.nn.Conv1d,</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>                        offset: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> layer.kernel_size <span class="op">!=</span> (<span class="dv">1</span>,):</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f&quot;Kernel size should be 1&quot;</span>)</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>    weight_size <span class="op">=</span> layer.out_channels <span class="op">*</span> layer.in_channels</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>    bias_size <span class="op">=</span> layer.out_channels</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>weight_size] <span class="op">=</span> layer.weight.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> weight_size</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>bias_size] <span class="op">=</span> layer.bias.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> bias_size</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> offset</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the torch.nn.Linear parameters to the contiguous buffer</span></span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_linear_params(buf: pynq.<span class="bu">buffer</span>.PynqBuffer,</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>                        layer: torch.nn.Linear,</span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a>                        offset: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>    weight_size <span class="op">=</span> layer.out_features <span class="op">*</span> layer.in_features</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>    bias_size <span class="op">=</span> layer.out_features</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>weight_size] <span class="op">=</span> layer.weight.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> weight_size</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>bias_size] <span class="op">=</span> layer.bias.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> bias_size</span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> offset</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the torch.nn.BatchNorm1d parameters to the contiguous buffer</span></span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_batchnorm1d_params(buf: pynq.<span class="bu">buffer</span>.PynqBuffer,</span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a>                             layer: torch.nn.BatchNorm1d,</span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a>                             offset: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a>    dims <span class="op">=</span> layer.num_features</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `scale` is the multiplication of the weight and reciprocal of the</span></span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># standard deviation (to reduce the on-chip memory consumption)</span></span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a>    std_inv <span class="op">=</span> torch.sqrt(layer.running_var.data <span class="op">+</span> layer.eps)</span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a>    std_inv <span class="op">=</span> torch.reciprocal(std_inv)</span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> std_inv <span class="op">*</span> layer.weight.data</span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>dims] <span class="op">=</span> scale.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> dims</span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>dims] <span class="op">=</span> layer.bias.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-69"><a href="#cb37-69" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> dims</span>
<span id="cb37-70"><a href="#cb37-70" aria-hidden="true" tabindex="-1"></a>    buf[offset:offset<span class="op">+</span>dims] <span class="op">=</span> layer.running_mean.data.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb37-71"><a href="#cb37-71" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">+=</span> dims</span>
<span id="cb37-72"><a href="#cb37-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> offset</span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the block (torch.nn.Conv1d and torch.nn.BatchNorm1d) parameters</span></span>
<span id="cb37-76"><a href="#cb37-76" aria-hidden="true" tabindex="-1"></a><span class="co"># to the contiguous buffer</span></span>
<span id="cb37-77"><a href="#cb37-77" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_conv_batchnorm1d_params(buf: pynq.<span class="bu">buffer</span>.PynqBuffer,</span>
<span id="cb37-78"><a href="#cb37-78" aria-hidden="true" tabindex="-1"></a>                                  conv: torch.nn.Conv1d,</span>
<span id="cb37-79"><a href="#cb37-79" aria-hidden="true" tabindex="-1"></a>                                  bn: torch.nn.BatchNorm1d):</span>
<span id="cb37-80"><a href="#cb37-80" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-81"><a href="#cb37-81" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> write_conv1d_params(buf, conv, offset)</span>
<span id="cb37-82"><a href="#cb37-82" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> write_batchnorm1d_params(buf, bn, offset)</span>
<span id="cb37-83"><a href="#cb37-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-84"><a href="#cb37-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the block (torch.nn.Linear and torch.nn.BatchNorm1d) parameters</span></span>
<span id="cb37-85"><a href="#cb37-85" aria-hidden="true" tabindex="-1"></a><span class="co"># to the contiguous buffer</span></span>
<span id="cb37-86"><a href="#cb37-86" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_linear_batchnorm1d_params(buf: pynq.<span class="bu">buffer</span>.PynqBuffer,</span>
<span id="cb37-87"><a href="#cb37-87" aria-hidden="true" tabindex="-1"></a>                                    linear: torch.nn.Linear,</span>
<span id="cb37-88"><a href="#cb37-88" aria-hidden="true" tabindex="-1"></a>                                    bn: torch.nn.BatchNorm1d):</span>
<span id="cb37-89"><a href="#cb37-89" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-90"><a href="#cb37-90" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> write_linear_params(buf, linear, offset)</span>
<span id="cb37-91"><a href="#cb37-91" aria-hidden="true" tabindex="-1"></a>    offset <span class="op">=</span> write_batchnorm1d_params(buf, bn, offset)</span>
<span id="cb37-92"><a href="#cb37-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-93"><a href="#cb37-93" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PointNetClsZCU104(torch.nn.Module):</span>
<span id="cb37-94"><a href="#cb37-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Operation modes (refer to hls/src/op_modes.hpp)</span></span>
<span id="cb37-95"><a href="#cb37-95" aria-hidden="true" tabindex="-1"></a>    MODE_INIT_WEIGHTS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb37-96"><a href="#cb37-96" aria-hidden="true" tabindex="-1"></a>    MODE_INFERENCE <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb37-97"><a href="#cb37-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-98"><a href="#cb37-98" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model_cpu: PointNetCls,</span>
<span id="cb37-99"><a href="#cb37-99" aria-hidden="true" tabindex="-1"></a>                 overlay_path: <span class="bu">str</span>, num_points: <span class="bu">int</span>):</span>
<span id="cb37-100"><a href="#cb37-100" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-101"><a href="#cb37-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-102"><a href="#cb37-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load an overlay</span></span>
<span id="cb37-103"><a href="#cb37-103" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.overlay <span class="op">=</span> <span class="va">self</span>.load_overlay(overlay_path)</span>
<span id="cb37-104"><a href="#cb37-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the IP core module</span></span>
<span id="cb37-105"><a href="#cb37-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net_ip: pynq.DefaultIP <span class="op">=</span> <span class="va">self</span>.overlay.PointNetClsTop</span>
<span id="cb37-106"><a href="#cb37-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the control registers of the IP core</span></span>
<span id="cb37-107"><a href="#cb37-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers <span class="op">=</span> <span class="va">self</span>.net_ip.register_map</span>
<span id="cb37-108"><a href="#cb37-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-109"><a href="#cb37-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check the data width of the AXI master interface</span></span>
<span id="cb37-110"><a href="#cb37-110" aria-hidden="true" tabindex="-1"></a>        net_ip_params <span class="op">=</span> <span class="va">self</span>.overlay.ip_dict[<span class="st">&quot;PointNetClsTop&quot;</span>][<span class="st">&quot;parameters&quot;</span>]</span>
<span id="cb37-111"><a href="#cb37-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.axi_m_addr_width <span class="op">=</span> <span class="bu">int</span>(net_ip_params[<span class="st">&quot;C_M_AXI_GMEM0_ADDR_WIDTH&quot;</span>])</span>
<span id="cb37-112"><a href="#cb37-112" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.axi_m_data_width <span class="op">=</span> <span class="bu">int</span>(net_ip_params[<span class="st">&quot;C_M_AXI_GMEM0_DATA_WIDTH&quot;</span>])</span>
<span id="cb37-113"><a href="#cb37-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-114"><a href="#cb37-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Allocate buffers for PointNet feature extraction network</span></span>
<span id="cb37-115"><a href="#cb37-115" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params1 <span class="op">=</span> allocate_block_buffer(<span class="dv">3</span>, <span class="dv">64</span>)</span>
<span id="cb37-116"><a href="#cb37-116" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params2 <span class="op">=</span> allocate_block_buffer(<span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb37-117"><a href="#cb37-117" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params3 <span class="op">=</span> allocate_block_buffer(<span class="dv">64</span>, <span class="dv">64</span>)</span>
<span id="cb37-118"><a href="#cb37-118" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params4 <span class="op">=</span> allocate_block_buffer(<span class="dv">64</span>, <span class="dv">128</span>)</span>
<span id="cb37-119"><a href="#cb37-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params5 <span class="op">=</span> allocate_block_buffer(<span class="dv">128</span>, <span class="dv">1024</span>)</span>
<span id="cb37-120"><a href="#cb37-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-121"><a href="#cb37-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Allocate buffers for classification network</span></span>
<span id="cb37-122"><a href="#cb37-122" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params1 <span class="op">=</span> allocate_block_buffer(<span class="dv">1024</span>, <span class="dv">512</span>)</span>
<span id="cb37-123"><a href="#cb37-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params2 <span class="op">=</span> allocate_block_buffer(<span class="dv">512</span>, <span class="dv">256</span>)</span>
<span id="cb37-124"><a href="#cb37-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params3 <span class="op">=</span> allocate_linear_buffer(<span class="dv">256</span>, <span class="dv">40</span>)</span>
<span id="cb37-125"><a href="#cb37-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-126"><a href="#cb37-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Allocate a buffer for point cloud</span></span>
<span id="cb37-127"><a href="#cb37-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_points <span class="op">=</span> num_points</span>
<span id="cb37-128"><a href="#cb37-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.axi_m_data_width <span class="op">==</span> <span class="dv">32</span>:</span>
<span id="cb37-129"><a href="#cb37-129" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_point_cloud: pynq.<span class="bu">buffer</span>.PynqBuffer <span class="op">=</span> pynq.allocate(</span>
<span id="cb37-130"><a href="#cb37-130" aria-hidden="true" tabindex="-1"></a>                shape<span class="op">=</span>(<span class="va">self</span>.num_points, <span class="dv">3</span>), dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-131"><a href="#cb37-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.axi_m_data_width <span class="op">==</span> <span class="dv">64</span>:</span>
<span id="cb37-132"><a href="#cb37-132" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_point_cloud: pynq.<span class="bu">buffer</span>.PynqBuffer <span class="op">=</span> pynq.allocate(</span>
<span id="cb37-133"><a href="#cb37-133" aria-hidden="true" tabindex="-1"></a>                shape<span class="op">=</span>(<span class="va">self</span>.num_points, <span class="dv">4</span>), dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-134"><a href="#cb37-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb37-135"><a href="#cb37-135" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f&quot;Unexpected data width for AXI master&quot;</span>)</span>
<span id="cb37-136"><a href="#cb37-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-137"><a href="#cb37-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Allocate a buffer for output logits</span></span>
<span id="cb37-138"><a href="#cb37-138" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_out_logits: pynq.<span class="bu">buffer</span>.PynqBuffer <span class="op">=</span> pynq.allocate(</span>
<span id="cb37-139"><a href="#cb37-139" aria-hidden="true" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="dv">40</span>,), dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-140"><a href="#cb37-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-141"><a href="#cb37-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Copy parameters for PointNet feature extraction network</span></span>
<span id="cb37-142"><a href="#cb37-142" aria-hidden="true" tabindex="-1"></a>        write_conv_batchnorm1d_params(<span class="va">self</span>.buf_feat_params1,</span>
<span id="cb37-143"><a href="#cb37-143" aria-hidden="true" tabindex="-1"></a>            model_cpu.feat.conv1, model_cpu.feat.bn1)</span>
<span id="cb37-144"><a href="#cb37-144" aria-hidden="true" tabindex="-1"></a>        write_conv_batchnorm1d_params(<span class="va">self</span>.buf_feat_params2,</span>
<span id="cb37-145"><a href="#cb37-145" aria-hidden="true" tabindex="-1"></a>            model_cpu.feat.conv2, model_cpu.feat.bn2)</span>
<span id="cb37-146"><a href="#cb37-146" aria-hidden="true" tabindex="-1"></a>        write_conv_batchnorm1d_params(<span class="va">self</span>.buf_feat_params3,</span>
<span id="cb37-147"><a href="#cb37-147" aria-hidden="true" tabindex="-1"></a>            model_cpu.feat.conv3, model_cpu.feat.bn3)</span>
<span id="cb37-148"><a href="#cb37-148" aria-hidden="true" tabindex="-1"></a>        write_conv_batchnorm1d_params(<span class="va">self</span>.buf_feat_params4,</span>
<span id="cb37-149"><a href="#cb37-149" aria-hidden="true" tabindex="-1"></a>            model_cpu.feat.conv4, model_cpu.feat.bn4)</span>
<span id="cb37-150"><a href="#cb37-150" aria-hidden="true" tabindex="-1"></a>        write_conv_batchnorm1d_params(<span class="va">self</span>.buf_feat_params5,</span>
<span id="cb37-151"><a href="#cb37-151" aria-hidden="true" tabindex="-1"></a>            model_cpu.feat.conv5, model_cpu.feat.bn5)</span>
<span id="cb37-152"><a href="#cb37-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-153"><a href="#cb37-153" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Copy parameters for classification network</span></span>
<span id="cb37-154"><a href="#cb37-154" aria-hidden="true" tabindex="-1"></a>        write_linear_batchnorm1d_params(<span class="va">self</span>.buf_cls_params1,</span>
<span id="cb37-155"><a href="#cb37-155" aria-hidden="true" tabindex="-1"></a>            model_cpu.fc1, model_cpu.bn1)</span>
<span id="cb37-156"><a href="#cb37-156" aria-hidden="true" tabindex="-1"></a>        write_linear_batchnorm1d_params(<span class="va">self</span>.buf_cls_params2,</span>
<span id="cb37-157"><a href="#cb37-157" aria-hidden="true" tabindex="-1"></a>            model_cpu.fc2, model_cpu.bn2)</span>
<span id="cb37-158"><a href="#cb37-158" aria-hidden="true" tabindex="-1"></a>        write_linear_params(<span class="va">self</span>.buf_cls_params3, model_cpu.fc3)</span>
<span id="cb37-159"><a href="#cb37-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-160"><a href="#cb37-160" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set the physical addresses of the buffers</span></span>
<span id="cb37-161"><a href="#cb37-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.point_cloud_1, <span class="va">self</span>.registers.point_cloud_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-162"><a href="#cb37-162" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_point_cloud.device_address)</span>
<span id="cb37-163"><a href="#cb37-163" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.out_logits_1, <span class="va">self</span>.registers.out_logits_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-164"><a href="#cb37-164" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_out_logits.device_address)</span>
<span id="cb37-165"><a href="#cb37-165" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.feat_params1_1, <span class="va">self</span>.registers.feat_params1_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-166"><a href="#cb37-166" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_feat_params1.device_address)</span>
<span id="cb37-167"><a href="#cb37-167" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.feat_params2_1, <span class="va">self</span>.registers.feat_params2_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-168"><a href="#cb37-168" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_feat_params2.device_address)</span>
<span id="cb37-169"><a href="#cb37-169" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.feat_params3_1, <span class="va">self</span>.registers.feat_params3_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-170"><a href="#cb37-170" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_feat_params3.device_address)</span>
<span id="cb37-171"><a href="#cb37-171" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.feat_params4_1, <span class="va">self</span>.registers.feat_params4_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-172"><a href="#cb37-172" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_feat_params4.device_address)</span>
<span id="cb37-173"><a href="#cb37-173" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.feat_params5_1, <span class="va">self</span>.registers.feat_params5_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-174"><a href="#cb37-174" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_feat_params5.device_address)</span>
<span id="cb37-175"><a href="#cb37-175" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.cls_params1_1, <span class="va">self</span>.registers.cls_params1_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-176"><a href="#cb37-176" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_cls_params1.device_address)</span>
<span id="cb37-177"><a href="#cb37-177" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.cls_params2_1, <span class="va">self</span>.registers.cls_params2_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-178"><a href="#cb37-178" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_cls_params2.device_address)</span>
<span id="cb37-179"><a href="#cb37-179" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.cls_params3_1, <span class="va">self</span>.registers.cls_params3_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-180"><a href="#cb37-180" aria-hidden="true" tabindex="-1"></a>            split_address(<span class="va">self</span>.buf_cls_params3.device_address)</span>
<span id="cb37-181"><a href="#cb37-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-182"><a href="#cb37-182" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Synchronize the buffers</span></span>
<span id="cb37-183"><a href="#cb37-183" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params1.sync_to_device()</span>
<span id="cb37-184"><a href="#cb37-184" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params2.sync_to_device()</span>
<span id="cb37-185"><a href="#cb37-185" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params3.sync_to_device()</span>
<span id="cb37-186"><a href="#cb37-186" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params4.sync_to_device()</span>
<span id="cb37-187"><a href="#cb37-187" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_feat_params5.sync_to_device()</span>
<span id="cb37-188"><a href="#cb37-188" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params1.sync_to_device()</span>
<span id="cb37-189"><a href="#cb37-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params2.sync_to_device()</span>
<span id="cb37-190"><a href="#cb37-190" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.buf_cls_params3.sync_to_device()</span>
<span id="cb37-191"><a href="#cb37-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-192"><a href="#cb37-192" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the weights (transfer the weights to the on-chip buffers)</span></span>
<span id="cb37-193"><a href="#cb37-193" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.op_mode <span class="op">=</span> PointNetClsZCU104.MODE_INIT_WEIGHTS</span>
<span id="cb37-194"><a href="#cb37-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.CTRL.AP_START <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb37-195"><a href="#cb37-195" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wait_for_ip()</span>
<span id="cb37-196"><a href="#cb37-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-197"><a href="#cb37-197" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_overlay(<span class="va">self</span>, overlay_path):</span>
<span id="cb37-198"><a href="#cb37-198" aria-hidden="true" tabindex="-1"></a>        overlay <span class="op">=</span> pynq.Overlay(overlay_path)</span>
<span id="cb37-199"><a href="#cb37-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-200"><a href="#cb37-200" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> overlay.is_loaded():</span>
<span id="cb37-201"><a href="#cb37-201" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f&quot;Unable to load overlay: </span><span class="sc">{</span>overlay_path<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb37-202"><a href="#cb37-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-203"><a href="#cb37-203" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> overlay</span>
<span id="cb37-204"><a href="#cb37-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-205"><a href="#cb37-205" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> wait_for_ip(<span class="va">self</span>):</span>
<span id="cb37-206"><a href="#cb37-206" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="va">self</span>.registers.CTRL.AP_DONE <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb37-207"><a href="#cb37-207" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb37-208"><a href="#cb37-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-209"><a href="#cb37-209" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb37-210"><a href="#cb37-210" aria-hidden="true" tabindex="-1"></a>        <span class="co"># `x` is of size [B, N, 3]</span></span>
<span id="cb37-211"><a href="#cb37-211" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x.ndim <span class="op">!=</span> <span class="dv">3</span> <span class="kw">or</span> x.shape[<span class="dv">2</span>] <span class="op">!=</span> <span class="dv">3</span>:</span>
<span id="cb37-212"><a href="#cb37-212" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f&quot;Unexpected shape of the input: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb37-213"><a href="#cb37-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-214"><a href="#cb37-214" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb37-215"><a href="#cb37-215" aria-hidden="true" tabindex="-1"></a>        num_points <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb37-216"><a href="#cb37-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-217"><a href="#cb37-217" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reallocate the buffer for point cloud if necessary</span></span>
<span id="cb37-218"><a href="#cb37-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> num_points <span class="op">&gt;</span> <span class="va">self</span>.num_points:</span>
<span id="cb37-219"><a href="#cb37-219" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.num_points <span class="op">=</span> num_points</span>
<span id="cb37-220"><a href="#cb37-220" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_point_cloud.freebuffer()</span>
<span id="cb37-221"><a href="#cb37-221" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.axi_m_data_width <span class="op">==</span> <span class="dv">32</span>:</span>
<span id="cb37-222"><a href="#cb37-222" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.buf_point_cloud: pynq.<span class="bu">buffer</span>.PynqBuffer <span class="op">=</span> pynq.allocate(</span>
<span id="cb37-223"><a href="#cb37-223" aria-hidden="true" tabindex="-1"></a>                    shape<span class="op">=</span>(<span class="va">self</span>.num_points, <span class="dv">3</span>),</span>
<span id="cb37-224"><a href="#cb37-224" aria-hidden="true" tabindex="-1"></a>                    dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-225"><a href="#cb37-225" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.axi_m_data_width <span class="op">==</span> <span class="dv">64</span>:</span>
<span id="cb37-226"><a href="#cb37-226" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.buf_point_cloud: pynq.<span class="bu">buffer</span>.PynqBuffer <span class="op">=</span> pynq.allocate(</span>
<span id="cb37-227"><a href="#cb37-227" aria-hidden="true" tabindex="-1"></a>                    shape<span class="op">=</span>(<span class="va">self</span>.num_points, <span class="dv">4</span>),</span>
<span id="cb37-228"><a href="#cb37-228" aria-hidden="true" tabindex="-1"></a>                    dtype<span class="op">=</span>np.float32, cacheable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-229"><a href="#cb37-229" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb37-230"><a href="#cb37-230" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="ss">f&quot;Unexpected data width for AXI master&quot;</span>)</span>
<span id="cb37-231"><a href="#cb37-231" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.registers.point_cloud_1, <span class="va">self</span>.registers.point_cloud_2 <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-232"><a href="#cb37-232" aria-hidden="true" tabindex="-1"></a>                split_address(<span class="va">self</span>.buf_point_cloud.device_address)</span>
<span id="cb37-233"><a href="#cb37-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-234"><a href="#cb37-234" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Allocate the Tensor for output</span></span>
<span id="cb37-235"><a href="#cb37-235" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.empty(size<span class="op">=</span>(batch_size, <span class="dv">40</span>),</span>
<span id="cb37-236"><a href="#cb37-236" aria-hidden="true" tabindex="-1"></a>                          dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)</span>
<span id="cb37-237"><a href="#cb37-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-238"><a href="#cb37-238" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run the inference</span></span>
<span id="cb37-239"><a href="#cb37-239" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.op_mode <span class="op">=</span> PointNetClsZCU104.MODE_INFERENCE</span>
<span id="cb37-240"><a href="#cb37-240" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.registers.num_points <span class="op">=</span> num_points</span>
<span id="cb37-241"><a href="#cb37-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-242"><a href="#cb37-242" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb37-243"><a href="#cb37-243" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Copy the input point cloud</span></span>
<span id="cb37-244"><a href="#cb37-244" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_point_cloud[:num_points, :<span class="dv">3</span>] <span class="op">=</span> x[i].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb37-245"><a href="#cb37-245" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_point_cloud.sync_to_device()</span>
<span id="cb37-246"><a href="#cb37-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-247"><a href="#cb37-247" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Run the inference</span></span>
<span id="cb37-248"><a href="#cb37-248" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.registers.CTRL.AP_START <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb37-249"><a href="#cb37-249" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wait_for_ip()</span>
<span id="cb37-250"><a href="#cb37-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-251"><a href="#cb37-251" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Copy the output logits</span></span>
<span id="cb37-252"><a href="#cb37-252" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.buf_out_logits.sync_from_device()</span>
<span id="cb37-253"><a href="#cb37-253" aria-hidden="true" tabindex="-1"></a>            out[i, :] <span class="op">=</span> torch.from_numpy(<span class="va">self</span>.buf_out_logits)</span>
<span id="cb37-254"><a href="#cb37-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-255"><a href="#cb37-255" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
<h3 id="ipã‚³ã‚¢ã®åˆæœŸåŒ–">IPã‚³ã‚¢ã®åˆæœŸåŒ–</h3>
<p><code>PointNetClsZCU104</code>ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§åˆæœŸåŒ–ã—ã€IPã‚³ã‚¢ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
ã“ã®æ‰‹é †ã§è¡Œã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ å„æ‰‹é †ã«ã¤ã„ã¦ã€é †ç•ªã«èª¬æ˜ã—ã¾ã™ã€‚
è©³ã—ãã¯ã€<a
href="https://pynq.readthedocs.io/en/latest/">Pynqã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</a>ã‚’ã”è¦§ãã ã•ã„ã€‚</p>
<ol type="1">
<li>ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒ­ãƒ¼ãƒ‰ (<code>load_overlay</code>)</li>
<li>DRAMãƒãƒƒãƒ•ã‚¡ã®ç¢ºä¿
(<code>allocate_block_buffer</code>ã€<code>pynq.allocate</code>)</li>
<li>DRAMãƒãƒƒãƒ•ã‚¡ã¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼
(<code>write_conv_batchnorm1d_params</code>ã€<code>write_linear_batchnorm1d_params</code>ã€<code>write_linear_params</code>)</li>
<li>DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ã€ãƒãƒ¼ãƒˆã®ãƒ¬ã‚¸ã‚¹ã‚¿ã«å¯¾ã—ã¦è¨­å®š</li>
<li>DRAMãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’åŒæœŸ (<code>sync_to_device</code>)</li>
<li>é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã§ã€IPã‚³ã‚¢ã‚’å‹•ä½œã•ã›ã€DRAMãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ä¸Šã«ã‚³ãƒ”ãƒ¼</li>
<li>IPã‚³ã‚¢ã®å‹•ä½œçµ‚äº†ã‚’å¾…æ©Ÿ (<code>wait_for_ip</code>)</li>
</ol>
<p>ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¯<code>pynq.Overlay</code>ã§ã‚ã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¸ãˆã¦ã€æŒ‡å®šã—ãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
æ‹¡å¼µå­ãŒ<code>.bit</code>ã®ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ä»–ã«ã€<code>.hwh</code>ã®Handoffãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒ<code>path/to/X.bit</code>ã§ã‚ã‚Œã°ã€å¯¾å¿œã™ã‚‹HandoffãŒ<code>path/to/X.hwh</code>ã«ãªã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼ã¨ãªã‚Šã¾ã™ã€‚
<code>pynq.Overlay</code>ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹<code>self.overlay</code>ã‚’èµ·ç‚¹ã¨ã—ã¦ã€FPGAã«å¯¾ã™ã‚‹æ§˜ã€…ãªå‡¦ç†ã‚’è¡Œã£ã¦ã„ãã¾ã™ã€‚</p>
<p>ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ (ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ )
ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã€è‡ªä½œã®IPã‚³ã‚¢<code>PointNetClsTop</code>ã‚’å–ã‚Šå‡ºã—ã¦ã€<code>self.net_ip</code>ã«æ ¼ç´ã—ã¾ã™ã€‚
IPã‚³ã‚¢ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£åã¯ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã«ãŠã‘ã‚‹å„IPã®åå‰ã¨å¯¾å¿œã—ã¦ã„ã¾ã™
(<a
href="point-cloud-classification-images/board-design.svg">ã“ã¡ã‚‰ã®ç”»åƒ</a>ã‚’å‚ç…§ã€‚)
ä¾‹ãˆã°ã€å‰²è¾¼ã¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ© (AXI Interrupt Controller)
ã«ã¯ã€<code>axi_intc_0</code>ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é€šã˜ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚
IPã‚³ã‚¢ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯<code>pynq.DefaultIP</code>ã¨ãªã£ã¦ã„ã¾ã™ã€‚
ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ã€è‡ªä½œã®IPã‚³ã‚¢ã‚’ã‚ˆã‚Šä¾¿åˆ©ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã€æ§˜ã€…ãªãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
ã•ã‚‰ã«ã€IPã‚³ã‚¢ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹<code>register_map</code>
(<code>pynq.registers.RegisterMap</code>ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹)
ã‚’å–ã‚Šå‡ºã—ã¦ã€<code>self.registers</code>ã«æ ¼ç´ã—ã¾ã™ã€‚</p>
<p>æ¬¡ã®3è¡Œã§ã€IPã‚³ã‚¢ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®ã‚¢ãƒ‰ãƒ¬ã‚¹å¹…ã¨ãƒ‡ãƒ¼ã‚¿å¹…ã‚’èª¿ã¹ã¦ã€<code>self.axi_m_addr_width</code>ãŠã‚ˆã³<code>self.axi_m_data_width</code>ã«æ ¼ç´ã—ã¾ã™ã€‚
å‰è€…ã¯64ã€å¾Œè€…ã¯32ã¾ãŸã¯64ã§ã™
(å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å‹ã‚’<code>ap_uint&lt;64&gt;*</code>ã¨ã—ãŸå ´åˆã¯64ã€<code>float*</code>ã®ã¾ã¾ã§ã‚ã‚Œã°32)ã€‚
å‰è¿°ã®é€šã‚Šã€ãƒãƒ¼ãƒˆå¹…ãŒ32ãƒ“ãƒƒãƒˆã§ã‚ã‚Œã°ã€ç‚¹ç¾¤ãƒãƒƒãƒ•ã‚¡ã®ã‚µã‚¤ã‚ºã¯<span
class="math inline">\((N,
3)\)</span>ã§ã‚ˆã„ã®ã§ã™ãŒã€64ãƒ“ãƒƒãƒˆã®å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‹é–¢ä¿‚ä¸Šã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’<span
class="math inline">\((N, 4)\)</span>ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
<code>self.axi_m_data_width</code>ã‚’å‚ç…§ã™ã‚Œã°ã€ç‚¹ç¾¤ãƒãƒƒãƒ•ã‚¡ã®ã‚µã‚¤ã‚ºã‚’æ±ºå®šã§ãã¾ã™ã€‚</p>
<p>ç¶šã„ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å…¥å‡ºåŠ›ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã®ãƒãƒƒãƒ•ã‚¡ã¯å°‘ã—ç‰¹æ®Šãªã‚‚ã®ã§ã€Linuxã‚«ãƒ¼ãƒãƒ«ã®CMA (Contiguous Memory
Allocator) ã¨ã„ã†æ©Ÿèƒ½ã«ã‚ˆã‚Šç¢ºä¿ã•ã‚Œã¾ã™ã€‚
é€šå¸¸ã®<code>malloc()</code>ã‚„<code>new</code>ã‚’ä½¿ã£ã¦ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã™ã‚‹ã¨ã€ãã®ãƒãƒƒãƒ•ã‚¡ã¸ã®ä»®æƒ³ã‚¢ãƒ‰ãƒ¬ã‚¹ã—ã‹åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚
ä¸€æ–¹ã€FPGAå´ã‹ã‚‰ã¯ã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒ•ã‚¡ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã®ã§ã€ä»®æƒ³ã‚¢ãƒ‰ãƒ¬ã‚¹ã ã‘ã§ãªãã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚‚äºˆã‚çŸ¥ã£ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p><code>allocate_linear_buffer</code>é–¢æ•°ã¯ã€ãã®åã®é€šã‚Šã€å…¨çµåˆå±¤
(å…¥åŠ›æ¬¡å…ƒ<code>in_dims</code>ã€å‡ºåŠ›æ¬¡å…ƒ<code>out_dims</code>)
ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚ æœ€åˆã«ã€å…¨çµåˆå±¤ã®é‡ã¿
(<code>in_dims * out_dims</code>) ã¨ãƒã‚¤ã‚¢ã‚¹ (<code>out_dims</code>)
ã®è¦ç´ æ•°ã‚’è¶³ã—ã¦ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’æ±ºå®šã—ã¾ã™ã€‚
ç¶šã„ã¦ã€<code>pynq.allocate</code>é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºãŠã‚ˆã³ãƒ‡ãƒ¼ã‚¿å‹<code>np.float32</code>
(<code>float</code>) ã®ã€1æ¬¡å…ƒã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã®ãƒãƒƒãƒ•ã‚¡ã¯DRAMã®ç‰¹æ®Šãªé ˜åŸŸã«ç½®ã‹ã‚Œã¦ã€ãƒ¡ãƒ¢ãƒªä¸Šã§é€£ç¶šã—ã¦ã„ã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¾ã™ã€‚
<code>allocate_block_buffer</code>é–¢æ•°ã¯ã€å…¨çµåˆå±¤ã¨ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¦ç´ æ•°ã‚’è¶³ã—åˆã‚ã›ã¦ã‚µã‚¤ã‚ºã‚’æ±ºå®šã—ã€<code>pynq.allocate</code>é–¢æ•°ã‚’ä½¿ã£ã¦ã€1æ¬¡å…ƒã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒãƒƒãƒ•ã‚¡ã¯<code>pynq.buffer.PynqBuffer</code>ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã™ãŒã€NumPyé…åˆ—
(<code>np.ndarray</code>) ã¨åŒã˜ã‚ˆã†ã«åˆ©ç”¨ã§ãã¾ã™ã€‚
ä¾‹ãˆã°ã€<code>torch.from_numpy</code>é–¢æ•°ã«ã‚ˆã‚Šã€PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã§ãã¾ã™ã€‚</p>
<p>ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
(<code>buf_feat_params1</code>ã‹ã‚‰<code>buf_feat_params5</code>)
ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
(<code>buf_cls_params1</code>ã‹ã‚‰<code>buf_cls_params3</code>)
ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚ ãã®å¾Œã€å…¥åŠ› (ç‚¹ç¾¤) ã¨å‡ºåŠ›
(ãƒ­ã‚¸ãƒƒãƒˆ) ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚‚ç¢ºä¿ã—ã¾ã™ã€‚
å…¥åŠ›ã«ã¤ã„ã¦ã¯ä¸Šè¿°ã®é€šã‚Šã€ãƒãƒ¼ãƒˆã®ãƒ“ãƒƒãƒˆå¹…ãŒ64ã§ã‚ã‚Œã°<code>(self.num_points, 4)</code>ã€32ã§ã‚ã‚Œã°<code>(self.num_points, 3)</code>ã¨ã—ã¾ã™ã€‚</p>
<p>DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—çµ‚ãˆãŸã‚‰ã€æ¬¡ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã¸ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã¯<code>PointNetCls</code>ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã€ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®å¼•æ•°<code>model_cpu</code>ã¨ã—ã¦æ¸¡ã•ã‚Œã¾ã™ã€‚
<code>write_conv1d_params</code>ã€<code>write_linear_params</code>ã¯ã€ãã‚Œãã‚Œ<code>torch.nn.Conv1d</code>ã€<code>torch.nn.Linear</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã«ä½¿ã‚ã‚Œã¾ã™ã€‚
<code>write_conv1d_params</code>ã§ã¯ã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºãŒ1ã§ã‚ã‚‹
(ãã‚Œã‚†ãˆå…¨çµåˆå±¤<code>torch.nn.Linear</code>ã¨å‹•ä½œãŒåŒã˜ã§ã‚ã‚‹)
ã“ã¨ã‚’å‰æã¨ã—ã¾ã™ã€‚
é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®é †ã§ã€æŒ‡å®šã•ã‚ŒãŸ1æ¬¡å…ƒã®DRAMãƒãƒƒãƒ•ã‚¡ã«ä¸¦ã¹ã¦ã‚†ãã¾ã™ã€‚
IPã‚³ã‚¢å´ã®æœŸå¾…é€šã‚Šã«ãƒ‡ãƒ¼ã‚¿ãŒé…ç½®ã•ã‚Œã‚‹ã‚ˆã†ã«ã€ç´°å¿ƒã®æ³¨æ„ã‚’æ‰•ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‚‰2ã¤ã®é–¢æ•°ã¯ã€é«˜ä½åˆæˆã®å®Ÿè£…ã«ãŠã‘ã‚‹ã€<code>ReadLinearParamsNaive</code>ã‚„<code>ReadLinearParamsOpt1</code>ã¨é©åˆã™ã‚‹ã‚ˆã†ã«ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚</p>
<p><code>write_batchnorm1d_params</code>ã¯ã€<code>torch.nn.BatchNorm1d</code>ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
IPã‚³ã‚¢å´ã§ã¯ã€<code>ReadBatchNorm1dParamsNaive</code>ã‚„<code>ReadBatchNorm1dParamsOpt1</code>ã«ç¤ºã™ã‚ˆã†ã«ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã®é †ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸¦ã¶ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®åˆ†æ•£ã¨é‡ã¿ã‹ã‚‰ã€ã‚¹ã‚±ãƒ¼ãƒ«ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™
(è¨ˆç®—å¼ã«ã¤ã„ã¦ã¯å…ˆè¿°)ã€‚</p>
<p><code>write_conv_batchnorm1d_params</code>ã¨<code>write_linear_batchnorm1d_params</code>ã¯ã€å…¨çµåˆå±¤
(<code>torch.nn.Conv1d</code>ã€<code>torch.nn.Linear</code>)
ã¨ãƒãƒƒãƒæ­£è¦åŒ–å±¤ (<code>torch.nn.BatchNorm1d</code>)
ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
å…¨çµåˆå±¤ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã€ãã‚Œã‹ã‚‰ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã‚’ã€ã“ã®é †ã§ä¸¦ã¹ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
IPã‚³ã‚¢å´ã®<code>ReadBlockParamsNaive</code>ã€<code>ReadBlockParamsOpt1</code>ã€<code>ReadBlockParamsOpt2</code>ã¨å¯¾å¿œã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã§ã™ãŒã€ãã®ã¾ã¾DRAMãƒãƒƒãƒ•ã‚¡
(<code>pynq.buffer.PynqBuffer</code>) ã«ä»£å…¥ã§ãã¾ã™ã€‚</p>
<p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç„¡äº‹ã«ã‚³ãƒ”ãƒ¼ã§ããŸã®ã§ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚
IPã‚³ã‚¢ã®ãƒˆãƒƒãƒ—é–¢æ•°<code>PointNetClsTop</code>ã¯æ¬¡ã®ã‚ˆã†ã«å®£è¨€ã•ã‚Œã¦ã„ã¾ã—ãŸ
(<code>float*</code>ã®ä»£ã‚ã‚Šã«<code>ap_uint&lt;64&gt;*</code>ã‚‚ã‚ã‚Š)ã€‚</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode c++"><code class="sourceCode cpp"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> PointNetClsTop<span class="op">(</span><span class="at">const</span> <span class="dt">int</span> op_mode<span class="op">,</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> point_cloud<span class="op">,</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">int</span> num_points<span class="op">,</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dt">float</span><span class="op">*</span> out_logits<span class="op">,</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params1<span class="op">,</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params2<span class="op">,</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params3<span class="op">,</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params4<span class="op">,</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> feat_params5<span class="op">,</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params1<span class="op">,</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params2<span class="op">,</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">const</span> <span class="dt">float</span><span class="op">*</span> cls_params3<span class="op">)</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=op_mode bundle=control</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=num_points bundle=control</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=out_logits bundle=control</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma HLS INTERFACE s_axilite port=return bundle=control</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>op_mode</code>ã¨<code>num_points</code>ã‚’é™¤ãã€DRAMãƒãƒƒãƒ•ã‚¡ç”¨ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã€<code>#pragma HLS INTERFACE m_axi</code>ã¨<code>#pragma HLS INTERFACE s_axilite</code>ã®è¨˜è¿°ãŒã¿ã‚‰ã‚Œã¾ã™ã€‚
ã“ã®2ã¤ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’ä»˜ä¸ã™ã‚‹ã¨ã€å„ãƒãƒ¼ãƒˆã«å¯¾ã—ã¦ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒä½œæˆã•ã‚Œã¾ã™ã€‚
ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯64ãƒ“ãƒƒãƒˆã§ã™ãŒã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã®ãƒ‡ãƒ¼ã‚¿å¹…ã¯32ãƒ“ãƒƒãƒˆãªã®ã§ã€ä¸Šä½32ãƒ“ãƒƒãƒˆã¨ä¸‹ä½32ãƒ“ãƒƒãƒˆç”¨ã«ã€2ã¤ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚
ä¾‹ãˆã°ã€<code>point_cloud</code>ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã¯ã€<code>point_cloud_1</code>
(ä¸‹ä½32ãƒ“ãƒƒãƒˆ) ã¨ã€<code>point_cloud_2</code> (ä¸Šä½32ãƒ“ãƒƒãƒˆ)
ã®ã€2ã¤ã§ã™ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã™ã‚Œã°ã€ãƒãƒ¼ãƒˆã¨DRAMãƒãƒƒãƒ•ã‚¡ã¨ãŒç´ã¥ã‘ã‚‰ã‚Œã€FPGAå´ã‹ã‚‰ãƒãƒƒãƒ•ã‚¡ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã¨ã€æ™®é€šã«å€¤ã‚’ä»£å…¥ã—ã¦ã„ã‚‹ã‚ˆã†ã«ã¿ãˆã¾ã™ãŒã€å®Ÿéš›ã«ã¯ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/Oã§å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€å„åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã«ã¯å°‚ç”¨ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ãŠã‚Šã€ãã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã«å¯¾ã—ã¦èª­ã¿æ›¸ãã—ã¦ã„ã¾ã™ã€‚
åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«ã¯ã€å…ˆã»ã©ã®<code>self.registers</code>ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚</p>
<p><code>op_mode</code>ã¨<code>num_points</code>ã«ã¤ã„ã¦ã‚‚ã€<code>#pragma HLS INTERFACE s_axilite</code>ã®è¨˜è¿°ãŒã‚ã‚‹ã®ã§ã€ã“ã®2ã¤
(å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã¨ç‚¹ã®å€‹æ•°) ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚</p>
<p>ã“ã“ã¾ã§æ¸ˆã‚“ã ã‚‰ã€<code>sync_to_device</code>ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚ŠDRAMãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’åŒæœŸã•ã›ã¦ã€FPGAå´ã‹ã‚‰æ­£ã—ãèª­ã‚ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚</p>
<p>æœ€å¾Œã«ã€å‹•ä½œãƒ¢ãƒ¼ãƒ‰<code>op_mode</code>ã‚’<strong>é‡ã¿åˆæœŸåŒ–</strong>ã«è¨­å®šã—ã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã®ã†ã¡<code>CTRL.AP_START</code>ã‚’1ã«ã™ã‚‹ã“ã¨ã§ã€IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ã¾ã™ã€‚
é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å‡ºã—ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã—ã¾ã™ã€‚
<code>#pragma HLS INTERFACE s_axilite port=return bundle=control</code>ã®è¨˜è¿°ãŒã‚ã‚‹ãŠã‹ã’ã§ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰IPã‚³ã‚¢ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®<code>CTRL</code>ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚
IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ãŸã‚‰ã€<code>wait_for_ip</code>ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã‚“ã§ã€å‹•ä½œçµ‚äº†
(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è»¢é€å®Œäº†) ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚
<code>wait_for_ip</code>ãƒ¡ã‚½ãƒƒãƒ‰å†…ã§ã¯ã€<code>CTRL</code>ãƒ¬ã‚¸ã‚¹ã‚¿ã®<code>AP_DONE</code>ãŒ1ã«ãªã‚‹ã¾ã§ã€ãƒ“ã‚¸ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã—ã¾ã™ã€‚
ä»¥ä¸Šã§åˆæœŸåŒ–ãŒãŠã—ã¾ã„ã§ã™ã€‚</p>
<h3 id="æ¨è«–">æ¨è«–</h3>
<p>åˆæœŸåŒ–ã«ã¯æ§˜ã€…ãªå·¥ç¨‹ãŒã‚ã£ã¦é¢å€’ã§ã™ãŒã€æ¨è«–ã¯æ¯”è¼ƒçš„ç°¡å˜ã§ã™ã€‚
PyTorchã®é€šå¸¸ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨åŒã˜ãã€<code>forward</code>ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¨è«–å‡¦ç†ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
å…¥åŠ›ç‚¹ç¾¤<code>x</code>ã¯ã€ã‚µã‚¤ã‚ºãŒ<span class="math inline">\((B, N,
3)\)</span>ã®ãƒãƒƒãƒã§ã‚ã‚‹ã¨ã—ã¾ã™ (<span
class="math inline">\(B\)</span>ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã€<span
class="math inline">\(N\)</span>ã¯ç‚¹ã®å€‹æ•°)ã€‚
ä»Šå›ã®IPã‚³ã‚¢ã¯ã€ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚ˆã†ã«ã¯ä½œã£ã¦ã„ãªã„ã®ã§ã€ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã‚’1ã¤ãšã¤å‡¦ç†ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
å‡ºåŠ›<code>out</code>ã¯ã€ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹æ•°ã‚’<span
class="math inline">\(K\)</span>ã¨ã™ã‚‹ã¨ã€ã‚µã‚¤ã‚ºãŒ<span
class="math inline">\((B, K)\)</span>ã¨ãªã‚Šã¾ã™ã€‚
ä»Šå›ã¯ModelNet40ã¨ã‚ˆã°ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã†ã®ã§ã€ã‚¯ãƒ©ã‚¹æ•°ã¯<span
class="math inline">\(K = 40\)</span>ã§ã™ã€‚</p>
<p>æœ€åˆã«ã€ç‚¹ç¾¤ã®ã‚µã‚¤ã‚º<span
class="math inline">\(N\)</span>ãŒã€ç‚¹ç¾¤ç”¨ã«ç¢ºä¿ã—ã¦ã‚ã‚‹ç¾åœ¨ã®DRAMãƒãƒƒãƒ•ã‚¡ã‚ˆã‚Šã‚‚å¤§ãã‘ã‚Œã°ã€DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ç›´ã—ã¾ã™ã€‚
ç¶šã„ã¦ã€ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦æ¨è«–å‡¦ç†ã‚’è¡Œã£ã¦ã€ç‰©ä½“ã®å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ
(ã‚¹ã‚³ã‚¢) ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ç‚¹ç¾¤ç”¨ã®DRAMãƒãƒƒãƒ•ã‚¡<code>buf_point_cloud</code>ã«ç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€FPGAå´ã‹ã‚‰æ­£ã—ãèª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ã€ãƒãƒƒãƒ•ã‚¡ã‚’åŒæœŸã—ã¾ã™ã€‚
ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã¯ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹… (32ã‹64ã‹ã©ã†ã‹)
ã¯ãã‚Œã»ã©æ„è­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ 2ã¤ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿
(å‹•ä½œãƒ¢ãƒ¼ãƒ‰<code>op_mode</code>ã¨ç‚¹ã®å€‹æ•°<code>num_points</code>)
ã¯ã€äºˆã‚è¨­å®šã—ã¦ãŠãã¾ã™ã€‚</p>
<p><code>CTRL</code>ãƒ¬ã‚¸ã‚¹ã‚¿ã®<code>AP_START</code>ã‚’1ã«ã™ã‚‹ã“ã¨ã§ã€<strong>æ¨è«–</strong>ãƒ¢ãƒ¼ãƒ‰ã§ã®IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ã¾ã™ã€‚
<code>wait_for_ip</code>ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚Šå‹•ä½œã®çµ‚äº†ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã§ã‚ã‚‹ãƒ­ã‚¸ãƒƒãƒˆã¯ã€IPã‚³ã‚¢å´ã‹ã‚‰DRAMãƒãƒƒãƒ•ã‚¡<code>buf_out_logits</code>ã«æ›¸ãè¾¼ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ãŸã†ãˆã§ã€å‡ºåŠ›ç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«<code>out</code>ã«æ”¹ã‚ã¦æ›¸ãè¾¼ã¿ã¾ã™ã€‚
ä»¥ä¸ŠãŒæ¨è«–å‡¦ç†ã®èª¬æ˜ã§ã—ãŸã€‚</p>
<p>ã“ã®ã‚ˆã†ã«ã€IPã‚³ã‚¢ã®å®Ÿè£…ã ã‘ã§ãªãã€ãã‚Œã‚’å®Ÿéš›ã«ä½¿ã†ãŸã‚ã®ãƒ‰ãƒ©ã‚¤ãƒã‚‚ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã€æ‰‹é–“ãŒæ›ã‹ã‚Šã¾ã™ã­ã€‚
ä»Šå›ã¯ã€Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸã®ã§ã€FPGAã«é–¢ã™ã‚‹å‡¦ç†ã¯ã€æ¯”è¼ƒçš„å®¹æ˜“ã«è¨˜è¿°ã§ãã¾ã—ãŸã€‚
ã¾ãŸã€CPUãƒ»GPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã‚ˆã†ã«ä½¿ã„ãŸã„ã®ã§ã€PyTorchã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
(<code>torch.nn.Module</code>) ã¨ã—ã¦ãƒ‰ãƒ©ã‚¤ãƒã‚’ä½œæˆã—ã¾ã—ãŸã€‚
Pythonã®ä»£ã‚ã‚Šã«C++ã‚’ä½¿ã†ã“ã¨ã‚‚ã€ã‚‚ã¡ã‚ã‚“å¯èƒ½ã§ã™ã€‚
ãã®å ´åˆã¯ã€ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒ­ãƒ¼ãƒ‰ (<a
href="https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/bitstream_loader.cpp">ä¾‹ãˆã°ã“ã¡ã‚‰</a>)ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/Oã®æº–å‚™
(<a
href="https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/mmio.cpp">ä¾‹ãˆã°ã“ã¡ã‚‰</a>)ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç¢ºä¿
(<a
href="https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/cma_memory.cpp">ä¾‹ãˆã°ã“ã¡ã‚‰</a>)ãªã©ã‚’ã€C++ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™
(Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãã®ã¾ã¾ç§»æ¤ã—ãŸã®ã‚’è¦šãˆã¦ã„ã¾ã™)ã€‚</p>
<h1 id="è©•ä¾¡">è©•ä¾¡</h1>
<h2 id="æ¨è«–æ™‚é–“ã®æ¯”è¼ƒ">æ¨è«–æ™‚é–“ã®æ¯”è¼ƒ</h2>
<p>ã‚ˆã†ã‚„ãã€IPã‚³ã‚¢ã‚’ä½¿ã£ãŸè©•ä¾¡ã«å…¥ã‚Šã¾ã—ãŸã€‚
æœ€åˆã«ã€æ¨è«–æ™‚é–“ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ©ç”¨ã—ã¾ã™
(<code>host/time_zcu104.py</code>)ã€‚</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parse the command-line arguments</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> parse_command_line()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a PointNet classification model</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> PointNetCls(num_classes<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an FPGA model</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    model_zcu104 <span class="op">=</span> PointNetClsZCU104(model, args.bitstream, args.num_points)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    model_zcu104.<span class="bu">eval</span>()</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test the output</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a random input point cloud</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    point_cloud <span class="op">=</span> torch.rand(size<span class="op">=</span>(<span class="dv">1</span>, args.num_points, <span class="dv">3</span>))</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    out_cpu <span class="op">=</span> model(point_cloud)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    out_zcu104 <span class="op">=</span> model_zcu104(point_cloud)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Output (CPU):</span><span class="ch">\n</span><span class="sc">{</span>out_cpu<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Output (FPGA):</span><span class="ch">\n</span><span class="sc">{</span>out_zcu104<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Measure the inference times</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    times_cpu <span class="op">=</span> []</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    times_zcu104 <span class="op">=</span> []</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(args.runs):</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a random input point cloud</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>        point_cloud <span class="op">=</span> torch.rand(size<span class="op">=</span>(<span class="dv">1</span>, args.num_points, <span class="dv">3</span>))</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.monotonic()</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>        model(point_cloud)</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>        elapsed_cpu <span class="op">=</span> (time.monotonic() <span class="op">-</span> t0) <span class="op">*</span> <span class="fl">1e3</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.monotonic()</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>        model_zcu104(point_cloud)</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>        elapsed_zcu104 <span class="op">=</span> (time.monotonic() <span class="op">-</span> t0) <span class="op">*</span> <span class="fl">1e3</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>        times_cpu.append(elapsed_cpu)</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>        times_zcu104.append(elapsed_zcu104)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>    time_avg_cpu <span class="op">=</span> np.mean(times_cpu)</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>    time_std_cpu <span class="op">=</span> np.std(times_cpu)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>    time_avg_zcu104 <span class="op">=</span> np.mean(times_zcu104)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>    time_std_zcu104 <span class="op">=</span> np.std(times_zcu104)</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>    speedup_factor <span class="op">=</span> time_avg_cpu <span class="op">/</span> time_avg_zcu104</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Inference time (CPU): &quot;</span> \</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;mean: </span><span class="sc">{</span>time_avg_cpu<span class="sc">:.3f}</span><span class="ss">ms, &quot;</span> \</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;std: </span><span class="sc">{</span>time_std_cpu<span class="sc">:.3f}</span><span class="ss">ms&quot;</span>)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Inference time (FPGA): &quot;</span> \</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;mean: </span><span class="sc">{</span>time_avg_zcu104<span class="sc">:.3f}</span><span class="ss">ms, &quot;</span> \</span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;std: </span><span class="sc">{</span>time_std_zcu104<span class="sc">:.3f}</span><span class="ss">ms&quot;</span>)</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Speedup: </span><span class="sc">{</span>speedup_factor<span class="sc">:.3f}</span><span class="ss">x&quot;</span>)</span></code></pre></div>
<p>ã“ã“ã§ã¯ç²¾åº¦ã¯æ°—ã«ã—ãªã„ã®ã§ã€å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ã¯çœã‹ã‚Œã¦ã„ã¾ã™ã€‚
ä½†ã—ã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«<code>PointNetCls</code>ã¨ã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«<code>PointNetClsZCU104</code>ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æƒãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã™ã€‚
ã¾ãŸã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¯<code>eval</code>ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã•ã›ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®æŒ™å‹•ãŒè¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«ãªã‚Šã€ãƒãƒƒãƒæ•°ãŒ1ã®ã¨ãã«ã‚¨ãƒ©ãƒ¼ã¨ãªã‚Šã¾ã™ã€‚
ã¾ãŸã€è¨“ç·´æ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ãªãã€å…¥åŠ›ã®ãƒãƒƒãƒã‹ã‚‰å¹³å‡ã‚„æ¨™æº–åå·®ãŒè¨ˆç®—ã•ã‚Œã‚‹ã®ã§ã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¨å‡ºåŠ›çµæœãŒåˆã‚ãªããªã‚Šã¾ã™ã€‚
æŒ‡å®šã•ã‚ŒãŸå›æ•°<code>args.runs</code>ã ã‘ã€æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬ã‚’è¡Œã„ã€å¹³å‡ã¨æ¨™æº–åå·®ã€ã¾ãŸé«˜é€ŸåŒ–ç‡ã‚’ç®—å‡ºã—ã¾ã™ã€‚
ã¾ãŸæœ€åˆã«ã€åŒæ–¹ã®ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒåˆã£ã¦ã„ã‚‹ã‹ã©ã†ã‹
(å¤§ä½“è¿‘ã„å€¤ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã‹) ã‚’ç¢ºèªã—ã¦ã„ã¾ã™
(æœ¬å½“ã¯ã€IPã‚³ã‚¢ã®ä½œæˆæ™‚ã«ã‚‚ãƒ†ã‚¹ãƒˆã—ã¾ã™)ã€‚</p>
<p>FPGAãƒœãƒ¼ãƒ‰ä¸Šã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚</p>
<pre><code>&gt; cd advent_2022_point_cloud_classification/host

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_naive_150.bit

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt2.bit

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt3.bit</code></pre>
<p>ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã—ãŸå ´åˆã®å‡ºåŠ›ä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚</p>
<pre><code>$ sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_naive_150.bit
Output (CPU):
tensor([[-0.0594, -0.0272,  0.0115, -0.0481, -0.0529,  0.0449, -0.0634, -0.0328,
          0.0348, -0.0071, -0.0228,  0.0412,  0.0128, -0.0175, -0.0086, -0.0023,
         -0.0192, -0.0101, -0.0072,  0.0520, -0.0106, -0.0110,  0.0113,  0.0499,
         -0.0563, -0.0523, -0.0711, -0.0104, -0.0048, -0.0404,  0.0375,  0.0089,
          0.0326, -0.0408, -0.0302, -0.0041,  0.0534, -0.0349,  0.0380, -0.0020]],
       grad_fn=&lt;AddmmBackward0&gt;)
Output (FPGA):
tensor([[-0.0592, -0.0274,  0.0114, -0.0491, -0.0527,  0.0446, -0.0632, -0.0335,
          0.0337, -0.0071, -0.0258,  0.0399,  0.0119, -0.0170, -0.0091, -0.0030,
         -0.0216, -0.0112, -0.0106,  0.0522, -0.0111, -0.0130,  0.0114,  0.0487,
         -0.0571, -0.0523, -0.0714, -0.0103, -0.0058, -0.0389,  0.0383,  0.0068,
          0.0306, -0.0421, -0.0314, -0.0052,  0.0539, -0.0360,  0.0399, -0.0031]])
Inference time (CPU): mean: 369.048ms, std: 1.086ms
Inference time (FPGA): mean: 1071.358ms, std: 0.023ms
Speedup: 0.344x</code></pre>
<p>CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã¯<code>float</code>ã‚’ä½¿ã„ã¾ã™ãŒã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å›ºå®šå°æ•°ç‚¹æ•°
(<code>ap_fixed</code>)
ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å…¥åŠ›ã‚’ä¸ãˆã¦ã‚‚ã€å‡ºåŠ›çµæœã«ã¯å¤šå°‘ã®ãšã‚ŒãŒç”Ÿã˜ã¾ã™
(ã“ã“ã§ã¯ã€å›ºå®šå°æ•°ç‚¹æ•°ã®ãƒ“ãƒƒãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã€æ•´æ•°éƒ¨ã‚’16ãƒ“ãƒƒãƒˆã€å°æ•°éƒ¨ã‚’16ãƒ“ãƒƒãƒˆã«è¨­å®šã—ã¦ã„ã¾ã™)ã€‚
ã—ã‹ã—ã€CPUç‰ˆã¨FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã€å¤§ä½“ä¼¼ãŸã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™
(å°æ•°ç¬¬2ä½ãã‚‰ã„ã¾ã§ã¯åˆã£ã¦ã„ã¾ã™)ã€‚
ã‚¯ãƒ©ã‚¹åˆ†é¡å•é¡Œã§ã‚ã‚Œã°ã€ã“ã‚Œã§å•é¡Œãªã„ã¨æ€ã„ã¾ã™ã€‚
æ¨è«–æ™‚é–“ã‚’ã¿ã‚‹ã¨ã€ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ã¯ã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚3å€ç¨‹åº¦é…ã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚</p>
<p>å„å®Ÿè£…ã«å¯¾ã™ã‚‹æ¨è«–æ™‚é–“ã‚’ã¾ã¨ã‚ã¾ã™ã€‚</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">å®Ÿè£…</th>
<th style="text-align: left;">æ¨è«–æ™‚é–“ã®å¹³å‡ (ms)</th>
<th style="text-align: left;">æ¨™æº–åå·® (ms)</th>
<th style="text-align: left;">é«˜é€ŸåŒ–ç‡ (ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢æ¯”)</th>
<th style="text-align: left;">é«˜é€ŸåŒ–ç‡ (ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…æ¯”)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CPUç‰ˆ</td>
<td style="text-align: left;">369.0</td>
<td style="text-align: left;">1.086</td>
<td style="text-align: left;"><strong>1.0x</strong></td>
<td style="text-align: left;">2.904x</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (100MHz)</td>
<td style="text-align: left;">1606.4</td>
<td style="text-align: left;">0.041</td>
<td style="text-align: left;">0.230x</td>
<td style="text-align: left;">0.667x</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (150MHz)</td>
<td style="text-align: left;">1071.4</td>
<td style="text-align: left;">0.023</td>
<td style="text-align: left;">0.344x</td>
<td style="text-align: left;"><strong>1.0x</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (200MHz)</td>
<td style="text-align: left;">872.05</td>
<td style="text-align: left;">0.077</td>
<td style="text-align: left;">0.423x</td>
<td style="text-align: left;">1.223x</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (250MHz)</td>
<td style="text-align: left;">665.33</td>
<td style="text-align: left;">0.073</td>
<td style="text-align: left;">0.555x</td>
<td style="text-align: left;">1.610x</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz)</td>
<td style="text-align: left;">34.60</td>
<td style="text-align: left;">0.027</td>
<td style="text-align: left;">10.66x</td>
<td style="text-align: left;">30.97x</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz)</td>
<td style="text-align: left;">12.93</td>
<td style="text-align: left;">0.016</td>
<td style="text-align: left;">28.54x</td>
<td style="text-align: left;">82.86x</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz)</td>
<td style="text-align: left;"><strong>10.80</strong></td>
<td style="text-align: left;"><strong>0.012</strong></td>
<td style="text-align: left;"><strong>34.17x</strong></td>
<td style="text-align: left;"><strong>99.20x</strong></td>
</tr>
</tbody>
</table>
<p>ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£… (150MHz) ã¯ã€CPUã«æ¯”ã¹ã¦æ€§èƒ½ãŒãŸã£ãŸã®0.344å€ã§ã—ãŸã€‚
ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã®ã¾ã¾ã§ã¯ã€å‹•ä½œå‘¨æ³¢æ•°ã‚’250MHzã¾ã§ä¸Šã’ã¦ã‚‚ã€ä¾ç„¶ã¨ã—ã¦CPUã‚ˆã‚Šã‚‚é…ã„ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã®åˆ©ç”¨ã«ã‚ˆã£ã¦ã€æ¨è«–æ™‚é–“ã¯30.97å€ã‚‚çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦10.66å€é«˜é€Ÿã«ãªã‚Šã¾ã—ãŸã€‚
Vitis HLSã«ã‚ˆã‚Šå‡ºåŠ›ã•ã‚ŒãŸã‚¯ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’ã¿ã‚‹ã¨ã€ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…
(150MHz) ã§ã¯161,945,604 (1.079s)ã€ä¸¦åˆ—åŒ–å¾Œã®å®Ÿè£…ã§ã¯4,462,596
(29.72ms)ã¨ãªã£ã¦ã„ã¾ã™ã€‚
å®Ÿéš›ã«ã¯ã€å‰è€…ã¯1.071sã€å¾Œè€…ã¯34.60msãªã®ã§ã€å¤§ä½“åˆã£ã¦ã„ã‚‹ã¨ã„ãˆã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã®æ´»ç”¨ã«ã‚ˆã£ã¦ã€æ¨è«–æ™‚é–“ã¯ã•ã‚‰ã«2.68å€çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦28.54å€ã€å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã«æ¯”ã¹ã¦82.86å€ã‚‚é«˜é€Ÿã«ãªã‚Šã¾ã—ãŸã€‚
ã•ã‚‰ã«ãƒãƒ¼ãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã‹ã‚‰64ãƒ“ãƒƒãƒˆã«æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€ä¸»ã«åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚
æ¨è«–æ™‚é–“ã¯1.20å€çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦34.17å€ã€å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã¨æ¯”ã¹ã‚‹ã¨99.20å€ã®é«˜é€ŸåŒ–ã¨ãªã‚Šã¾ã—ãŸã€‚
ã“ã®ã‚ˆã†ã«ã€å„ç¨®æœ€é©åŒ–ã‚’æ–½ã™ã“ã¨ã§ã€ç€å®Ÿã«é«˜é€ŸåŒ–ã§ãã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ã—ã‹ã‚‚ã€åŸºæœ¬çš„ã«ã¯ã€å„ç¨®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ¿å…¥ã™ã‚‹ã ã‘ã‚ˆã„ã®ã§ã€éå¸¸ã«æ¥½ã§ã™ã€‚</p>
<h2 id="ç²¾åº¦">ç²¾åº¦</h2>
<p>ã¤ãã«ãƒ¢ãƒ‡ãƒ«ã®åˆ†é¡ç²¾åº¦ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
ã“ã“ã§ã¯ModelNet40ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯<a
href="https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip">ã“ã¡ã‚‰</a>ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
å„ã‚µãƒ³ãƒ—ãƒ«ã¯ã€é£›è¡Œæ©Ÿã€è‡ªå‹•è»Šã€ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã€äººé–“ãªã©ã€å˜ä¸€ã®ç‰©ä½“ã‚’è¡¨ã™CADãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã€2048å€‹ã®ç‚¹ã‚’ã‚‚ã¤ç‚¹ç¾¤ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ©ç”¨ã—ã¾ã™ (<code>host/test_zcu104.py</code>)ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ã¤ã„ã¦ã¯ã€GitHubã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(args: argparse.Namespace,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>         model: torch.nn.Module,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>         model_zcu104: torch.nn.Module,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>         test_loader: torch.utils.data.DataLoader):</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Testing PointNet ...&quot;</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model.eval()</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    model_zcu104.<span class="bu">eval</span>()</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># test_loss_total = 0.0</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># correct = 0</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    test_loss_total_zcu104 <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    correct_zcu104 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader):</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Testing batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>            data, target <span class="op">=</span> batch[<span class="st">&quot;points&quot;</span>], batch[<span class="st">&quot;label&quot;</span>]</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># out = model(data)</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pred = out.argmax(dim=1, keepdim=True)</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># loss = F.cross_entropy(out, target)</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># correct += pred.eq(target.view_as(pred)).sum().item()</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># test_loss_total += loss.item() * len(data)</span></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>            out_zcu104 <span class="op">=</span> model_zcu104(data)</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>            pred_zcu104 <span class="op">=</span> out_zcu104.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>            loss_zcu104 <span class="op">=</span> F.cross_entropy(out_zcu104, target)</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>            correct_zcu104 <span class="op">+=</span> pred_zcu104.eq(</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>                target.view_as(pred_zcu104)).<span class="bu">sum</span>().item()</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>            test_loss_total_zcu104 <span class="op">+=</span> loss_zcu104.item() <span class="op">*</span> <span class="bu">len</span>(data)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># test_loss_avg = test_loss_total / len(test_loader.dataset)</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># test_acc = correct * 1e2 / len(test_loader.dataset)</span></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>    test_loss_avg_zcu104 <span class="op">=</span> test_loss_total_zcu104 <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>    test_acc_zcu104 <span class="op">=</span> correct_zcu104 <span class="op">*</span> <span class="fl">1e2</span> <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f&quot;Test result (CPU): &quot; \</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       f&quot;loss: {test_loss_avg:.6f}, &quot; \</span></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       f&quot;accuracy: {test_acc:.3f}%, &quot; \</span></span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">#       f&quot;correct: {correct}&quot;)</span></span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Test result (FPGA): &quot;</span> \</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;loss: </span><span class="sc">{</span>test_loss_avg_zcu104<span class="sc">:.6f}</span><span class="ss">, &quot;</span> \</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;accuracy: </span><span class="sc">{</span>test_acc_zcu104<span class="sc">:.3f}</span><span class="ss">%, &quot;</span> \</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;correct: </span><span class="sc">{</span>correct_zcu104<span class="sc">}</span><span class="ss">, &quot;</span> \</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f&quot;total: </span><span class="sc">{</span><span class="bu">len</span>(test_loader.dataset)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>FPGAãƒœãƒ¼ãƒ‰ä¸Šã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚</p>
<pre><code>&gt; cd advent_2022_point_cloud_classification/host

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt2.bit

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
&gt; sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt3.bit</code></pre>
<p>å‡ºåŠ›çµæœã®ä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚</p>
<pre><code>&gt; sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit
Testing batch 0 ....
Testing batch 5 ...
...
Testing batch 2445 ...
Testing batch 2450 ...
Testing batch 2455 ...
Testing batch 2460 ...
Testing batch 2465 ...
Test result (FPGA): loss: 0.375841, accuracy: 89.506%, correct: 2209, total: 2468</code></pre>
<p>å„å®Ÿè£…ã«å¯¾ã™ã‚‹ç²¾åº¦ã‚’ã¾ã¨ã‚ã¾ã™ã€‚
å…¨éƒ¨ã§2,468å€‹ã®ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚
ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã«é–¢ã—ã¦ã¯ã€æ™‚é–“ãŒæ›ã‹ã‚Šã™ãã‚‹ã®ã§çœç•¥ã—ã¦ã„ã¾ã™ã€‚</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">å®Ÿè£…</th>
<th style="text-align: left;">æ­£è§£æ•°</th>
<th style="text-align: left;">ç²¾åº¦</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CPUç‰ˆ</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz)</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz)</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz)</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
</tbody>
</table>
<p>ã„ãšã‚Œã®IPã‚³ã‚¢ã‚‚ã€CPUä¸Šã§å‹•ã‹ã—ãŸå ´åˆã¨å…¨ãåŒã˜ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
<code>float</code>ã®ä»£ã‚ã‚Šã«å›ºå®šå°æ•°ç‚¹æ•°<code>ap_fixed</code>ã‚’ä½¿ã£ã¦ã„ã¾ã™ãŒã€ã„ã¾ã®ã¨ã“ã‚ã¯ç²¾åº¦ä½ä¸‹ã¯ã¿ã‚‰ã‚Œã¾ã›ã‚“ã€‚</p>
<h2 id="ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»">ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»</h2>
<p>å„ç¨®IPã‚³ã‚¢ã®ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ã€LUT
(ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«)ã€FF (ãƒ•ãƒªãƒƒãƒ—ãƒ•ãƒ­ãƒƒãƒ—)ã€BRAM (BlockRAM)ã€URAM
(UltraRAM)ã€DSP (Digital Signal Processor)ã®5ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚</p>
<p><a
href="point-cloud-classification-images/pointnet-opt3-vivado4.png"><img src="point-cloud-classification-images/pointnet-opt3-vivado4.png" width="80%" /></a></p>
<p>ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’è¡¨ã«ã¾ã¨ã‚ã¾ã™ã€‚</p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">å®Ÿè£…</th>
<th style="text-align: left;">LUT</th>
<th style="text-align: left;">FF</th>
<th style="text-align: left;">BRAM (36Kb)</th>
<th style="text-align: left;">URAM</th>
<th style="text-align: left;">DSP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">åˆè¨ˆ</td>
<td style="text-align: left;">230,400</td>
<td style="text-align: left;">460,800</td>
<td style="text-align: left;">312</td>
<td style="text-align: left;">96</td>
<td style="text-align: left;">1,728</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (100MHz)</td>
<td style="text-align: left;">22,378 (9.71%)</td>
<td style="text-align: left;">11,045 (2.40%)</td>
<td style="text-align: left;">149.5 (47.92%)</td>
<td style="text-align: left;">2 (2.08%)</td>
<td style="text-align: left;">48 (2.78%)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (150MHz)</td>
<td style="text-align: left;">22,140 (9.61%)</td>
<td style="text-align: left;">12,428 (2.70%)</td>
<td style="text-align: left;">161.5 (51.76%)</td>
<td style="text-align: left;">2 (2.08%)</td>
<td style="text-align: left;">48 (2.78%)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (200MHz)</td>
<td style="text-align: left;">21,344 (9.26%)</td>
<td style="text-align: left;">13,616 (2.95%)</td>
<td style="text-align: left;">149.5 (47.92%)</td>
<td style="text-align: left;">2 (2.08%)</td>
<td style="text-align: left;">48 (2.78%)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒŠã‚¤ãƒ¼ãƒ– (250MHz)</td>
<td style="text-align: left;">20,663 (8.97%)</td>
<td style="text-align: left;">14,713 (3.19%)</td>
<td style="text-align: left;">149.5 (47.92%)</td>
<td style="text-align: left;">2 (2.08%)</td>
<td style="text-align: left;">20 (1.16%)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz)</td>
<td style="text-align: left;">58,223 (25.27%)</td>
<td style="text-align: left;">42,755 (9.28%)</td>
<td style="text-align: left;">287.5 (92.15%)</td>
<td style="text-align: left;">0 (0.00%)</td>
<td style="text-align: left;">768 (44.44%)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz)</td>
<td style="text-align: left;">136,408 (59.20%)</td>
<td style="text-align: left;">48,940 (10.62%)</td>
<td style="text-align: left;">310.5 (99.52%)</td>
<td style="text-align: left;">0 (0.00%)</td>
<td style="text-align: left;">808 (46.76%)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz)</td>
<td style="text-align: left;">84,263 (36.57%)</td>
<td style="text-align: left;">49,660 (10.78%)</td>
<td style="text-align: left;">263.5 (84.46%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">808 (46.76%)</td>
</tr>
</tbody>
</table>
<p>ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã™ã‚‹ã¨ã€è¤‡æ•°ã®ç©å’Œæ¼”ç®—ã‚’ä¸¦åˆ—ã«è¡Œã†å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€DSPã®æ¶ˆè²»ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ä¸€æ–¹ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’ç”¨ã„ã¦ã‚‚ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ãã‚Œã»ã©å¢—ãˆã¦ã„ã¾ã›ã‚“
(ãŸã ã—ã€BRAMãŒä¸è¶³ã—ã¦ã€LUTã‚’LUTRAMã¨ã—ã¦ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€LUTã®æ¶ˆè²»ã¯å¢—åŠ ã—ã¦ã„ã¾ã™)ã€‚
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã«ã‚ˆã£ã¦ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã®å¢—åŠ ã‚’æŠ‘ãˆã¤ã¤ã€å›è·¯ã®æ€§èƒ½ã‚’æ”¹å–„ã§ãã¾ã™ã€‚
ãƒãƒ¼ãƒˆå¹…ã‚’æ‹¡å¼µã—ã¦ã‚‚ã€URAMä»¥å¤–ã®ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ã‚ã¾ã‚Šå¤‰ã‚ã£ã¦ã„ã¾ã›ã‚“
(BRAMãŒä¸è¶³ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã®ã§ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®ä¸€éƒ¨ã‚’URAMã§å®Ÿè£…ã—ã¦ã„ã¾ã™)ã€‚</p>
<p>ä»Šå›ã¯20ä¸‡å††ç¨‹åº¦ã™ã‚‹FPGAãƒœãƒ¼ãƒ‰ã€Xilinx ZCU104 Evaluation
Kitã‚’ä½¿ã„ã¾ã—ãŸã€‚ ã“ã®ãƒœãƒ¼ãƒ‰ã®FPGAãƒãƒƒãƒ— (XCZU7EV-2FFVC1156)
ã«ã¯ã€BRAMã ã‘ã§ãªãURAMã‚‚æä¾›ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æ¯”è¼ƒçš„å¤§ããªã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡
(æ•°MBç¨‹åº¦) ã‚’ä½œæˆã§ãã¾ã™ã€‚ URAM (UltraRAM) ã¯BRAM (BlockRAM)
ã«æ¯”ã¹ã¦å€‹æ•°ãŒå°‘ãªã„ã§ã™ãŒ
(BRAMãŒ312å€‹ã«å¯¾ã—ã¦URAMã¯96å€‹)ã€1å€‹ã‚ãŸã‚Šã®å®¹é‡ã¯å¤§ãã„ã®ã§ã€ç²—ç²’åº¦ã ã¨ã„ãˆã¾ã™ã€‚
ä½ä¾¡æ ¼ã®FPGAãƒœãƒ¼ãƒ‰ã ã¨ã€URAMãŒæä¾›ã•ã‚Œã¦ã„ãªã„ã®ã§ã€BRAMã‚’å¤§åˆ‡ã«ä½¿ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
å€‹äººçš„ã«ã¯ã€BRAMãŒä¸€ç•ªæœ€åˆã«æ¯æ¸‡ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™
(FPGAã«æ…£ã‚Œã¦ã„ãªã„åˆå¿ƒè€…ãªã®ã§ã€ã†ã¾ãå®Ÿè£…ã§ãã¾ã›ã‚“)ã€‚</p>
<h2 id="å€¤ã®ãƒ“ãƒƒãƒˆå¹…å‰Šæ¸›">å€¤ã®ãƒ“ãƒƒãƒˆå¹…å‰Šæ¸›</h2>
<p>ã„ã¾ã¾ã§ã¯ã€å±¤ã®å…¥å‡ºåŠ›ã‚„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ã€32ãƒ“ãƒƒãƒˆã®å›ºå®šå°æ•°ç‚¹æ•°
(æ•´æ•°éƒ¨ã¨å°æ•°éƒ¨ãŒ16ãƒ“ãƒƒãƒˆãšã¤) ã‚’ä½¿ã£ã¦ã„ã¾ã—ãŸã€‚
ç²¾åº¦ã‚’ã‚ã‚‹ç¨‹åº¦ä¿ã£ãŸã¾ã¾ã€ãƒ“ãƒƒãƒˆæ•° (ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»)
ã‚’æŠ‘ãˆã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã‹ã€‚ ã“ã“ã§ã¯ã€ä»¥ä¸‹ã®ãƒ“ãƒƒãƒˆæ•°ã®çµ„ã¿åˆã‚ã›ã§ã€IPã‚³ã‚¢
(å‹•ä½œå‘¨æ³¢æ•°150MHz) ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
IPã‚³ã‚¢ã¯ã€ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ–½ã—ã€ã•ã‚‰ã«ãƒãƒ¼ãƒˆå¹…ã‚’æ‹¡å¼µã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ“ãƒƒãƒˆæ•°ã¯ä½•ã¨ãªãæ±ºã‚ã¾ã—ãŸã€‚
ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ–¹ã¯ã€å±¤ã®å…¥å‡ºåŠ›ã«æ¯”ã¹ã¦å€¤åŸŸãŒç‹­ã„ã®ã§ã€ã‚ˆã‚Šãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">åå‰</th>
<th style="text-align: left;">å±¤ã®å…¥å‡ºåŠ› (<code>value_t</code>)</th>
<th style="text-align: left;">ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
(<code>param_t</code>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">28-28</td>
<td style="text-align: left;">28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨14 + å°æ•°éƒ¨14)</td>
<td style="text-align: left;">28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨18)</td>
</tr>
<tr class="even">
<td style="text-align: left;">28-24</td>
<td style="text-align: left;">28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨14 + å°æ•°éƒ¨14)</td>
<td style="text-align: left;">24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨8 + å°æ•°éƒ¨16)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">24-24</td>
<td style="text-align: left;">24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12)</td>
<td style="text-align: left;">24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨8 + å°æ•°éƒ¨16)</td>
</tr>
<tr class="even">
<td style="text-align: left;">24-20</td>
<td style="text-align: left;">24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12)</td>
<td style="text-align: left;">20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨6 + å°æ•°éƒ¨14)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">24-16</td>
<td style="text-align: left;">24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12)</td>
<td style="text-align: left;">16ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨4 + å°æ•°éƒ¨12)</td>
</tr>
<tr class="even">
<td style="text-align: left;">20-20</td>
<td style="text-align: left;">20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨10)</td>
<td style="text-align: left;">20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨6 + å°æ•°éƒ¨14)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">20-16</td>
<td style="text-align: left;">20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨10)</td>
<td style="text-align: left;">16ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨4 + å°æ•°éƒ¨12)</td>
</tr>
</tbody>
</table>
<p>å„å®Ÿè£…ã«ãŠã‘ã‚‹ç²¾åº¦ã‚’ã¾ã¨ã‚ã¾ã™ã€‚</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">å®Ÿè£…</th>
<th style="text-align: left;">æ­£è§£æ•°</th>
<th style="text-align: left;">ç²¾åº¦</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">CPUç‰ˆ</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz)</td>
<td style="text-align: left;">2209</td>
<td style="text-align: left;">89.506%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-28)</td>
<td style="text-align: left;">2206</td>
<td style="text-align: left;">89.384%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-24)</td>
<td style="text-align: left;">2206</td>
<td style="text-align: left;">89.384%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-24)</td>
<td style="text-align: left;">2200</td>
<td style="text-align: left;">89.141%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-20)</td>
<td style="text-align: left;">550</td>
<td style="text-align: left;">22.285%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-16)</td>
<td style="text-align: left;">121</td>
<td style="text-align: left;">4.903%</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€20-20)</td>
<td style="text-align: left;">448</td>
<td style="text-align: left;">18.152%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€20-16)</td>
<td style="text-align: left;">122</td>
<td style="text-align: left;">4.903%</td>
</tr>
</tbody>
</table>
<p>ã¾ãŸã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’ä»¥ä¸‹ã«ã¾ã¨ã‚ã¾ã™ã€‚</p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">å®Ÿè£…</th>
<th style="text-align: left;">LUT</th>
<th style="text-align: left;">FF</th>
<th style="text-align: left;">BRAM (36Kb)</th>
<th style="text-align: left;">URAM</th>
<th style="text-align: left;">DSP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">åˆè¨ˆ</td>
<td style="text-align: left;">230,400</td>
<td style="text-align: left;">460,800</td>
<td style="text-align: left;">312</td>
<td style="text-align: left;">96</td>
<td style="text-align: left;">1,728</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz)</td>
<td style="text-align: left;">84,263 (36.57%)</td>
<td style="text-align: left;">49,660 (10.78%)</td>
<td style="text-align: left;">263.5 (84.46%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">808 (46.76%)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-28)</td>
<td style="text-align: left;">74,342 (32.27%)</td>
<td style="text-align: left;">47,267 (10.26%)</td>
<td style="text-align: left;">261.5 (83.81%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">808 (46.76%)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-24)</td>
<td style="text-align: left;">63,749 (27.67%)</td>
<td style="text-align: left;">39,139 (8.49%)</td>
<td style="text-align: left;">257 (82.37%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">404 (23.38%)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-24)</td>
<td style="text-align: left;">59,970 (26.03%)</td>
<td style="text-align: left;">36,240 (7.86%)</td>
<td style="text-align: left;">257 (82.37%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">404 (23.38%)</td>
</tr>
<tr class="even">
<td style="text-align: left;">ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-20)</td>
<td style="text-align: left;">75,997 (32.98%)</td>
<td style="text-align: left;">40,762 (8.85%)</td>
<td style="text-align: left;">259 (83.01%)</td>
<td style="text-align: left;">64 (66.67%)</td>
<td style="text-align: left;">202 (11.69%)</td>
</tr>
</tbody>
</table>
<p>ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã—ã¦ã‚‚ã€æ¨è«–æ™‚é–“ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
ãƒ“ãƒƒãƒˆæ•°ã®å‰Šæ¸›ã«å¿œã˜ã¦ã€å®Ÿè£…ã‚’å°‘ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ã€‚</p>
<p>ä¸Šè¨˜ã®çµæœã‚’ã¿ã‚‹ã¨ã€é‡ã¿ã®ãƒ“ãƒƒãƒˆæ•°ã‚’24ãƒ“ãƒƒãƒˆã‹ã‚‰20ãƒ“ãƒƒãƒˆã«å‰Šæ¸›ã—ãŸé€”ç«¯ã«ã€åˆ†é¡ç²¾åº¦ãŒä¸€æ°—ã«ä½ä¸‹ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™
(ã“ã“ã¾ã§ã®æ€¥æ¿€ãªä½ä¸‹ã«ã¯é©šãã¾ã—ãŸ)ã€‚
å±¤ã®å…¥å‡ºåŠ›ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã„ãšã‚Œã‚‚24ãƒ“ãƒƒãƒˆã«è¨­å®šã—ãŸIPã‚³ã‚¢ãŒã€æœ€ã‚‚ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒã‚ˆã„ã¨ã„ãˆã¾ã™ã€‚
ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’ã¿ã‚‹ã¨ã€ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§å›è·¯ã®è¤‡é›‘ã•ãŒå¾ã€…ã«ä¸‹ãŒã£ã¦ã‚†ãã€ãã‚Œã«ä¼´ã£ã¦LUTã‚„FFã®ä½¿ç”¨é‡ãŒæ¼¸æ¸›ã—ã¦ã„ã¾ã™ã€‚
28ãƒ“ãƒƒãƒˆã‹ã‚‰24ãƒ“ãƒƒãƒˆã«è½ã¨ã™ã¨ã€ç©å’Œæ¼”ç®—ã«å¿…è¦ãªDSPãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ãŒåŠæ¸›ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
24ãƒ“ãƒƒãƒˆã‹ã‚‰20ãƒ“ãƒƒãƒˆã«ã™ã‚‹ã¨ã€DSPã®ä½¿ç”¨é‡ã¯ã•ã‚‰ã«åŠæ¸›ã—ã¦ã„ã¾ã™
(ãã®åˆ†LUTã¨FFãŒå¢—åŠ ã—ã¦ã„ã¾ã™)ã€‚
BRAMã‚„URAMã«ã¤ã„ã¦ã¯ã€ãƒ“ãƒƒãƒˆæ•°ã‚’ã‚‚ã†å°‘ã—æ¸›ã‚‰ã•ãªã„ã¨ã€æ¶ˆè²»é‡ãŒæ¸›ã‚‰ãªã„ã‚ˆã†ã§ã™
(ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®ä¸è¶³ãŒé ­ç—›ã®ç¨®ã«ãªã‚Šã¾ã™)ã€‚</p>
<h1 id="ãŠã‚ã‚Šã«">ãŠã‚ã‚Šã«</h1>
<p>ä»Šå›ã¯ã€FPGAã‚’ç”¨ã„ã¦ã€ç‚¹ç¾¤ã®åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’é«˜é€ŸåŒ–ã—ã¾ã—ãŸã€‚
åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯ã€è»½é‡ã‹ã¤ã‚·ãƒ³ãƒ—ãƒ«ãªPointNetã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚
FPGAã®ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡ç•¥åŒ–ã—ã€ã¾ãŸè¨ˆç®—é †åºã‚’å¤‰æ›´ã—ã¾ã—ãŸã€‚
ç¶šã„ã¦ã€Xilinxç¤¾ã®é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«Vitis HLS
2022.1ã‚’ä½¿ã£ã¦ã€PointNetç”¨ã®ã‚«ã‚¹ã‚¿ãƒ IPã‚³ã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã€å±¤ã®è¨ˆç®—ã®ä¸¦åˆ—åŒ–
(ãƒ«ãƒ¼ãƒ—ã®ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²)ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãªã©ã‚’ä½¿ã£ã¦ã€IPã‚³ã‚¢ã®å®Ÿè£…ã‚’å°‘ã—ãšã¤æ”¹å–„ã—ã¦ã„ãã¾ã—ãŸã€‚</p>
<p>IPã‚³ã‚¢ã‚’ä»–ã®IPã‚³ã‚¢ã¨ç¹‹ãåˆã‚ã›ã¦ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆã—ã€Xilinx Vivado
2022.1ã«ã‚ˆã‚Šè«–ç†åˆæˆãƒ»é…ç½®é…ç·šã‚’è¡Œã£ã¦ã€FPGAã«æ›¸ãè¾¼ã¿å¯èƒ½ãªãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦é«˜é€Ÿã«æ¨è«–ã™ã‚‹ãŸã‚ã®ãƒ‰ãƒ©ã‚¤ãƒã‚’ã€Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã‚Šè¨˜è¿°ã—ã¾ã—ãŸã€‚
ModelNet40ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€Xilinx ZCU104 Evaluation
Kitä¸Šã§ã€æ¨è«–æ™‚é–“ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã€åˆ†é¡ç²¾åº¦ã®3ã¤ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸã€‚
ã¾ãŸã€è¤‡æ•°ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€å„ç¨®æœ€é©åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’èª¿ã¹ã¾ã—ãŸã€‚
ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã—ã€ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚‚è©¦ã¿ã¾ã—ãŸã€‚</p>
<p>é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã“ã¨ã§ã€Verilog
HDLãªã—ã§ã€C/C++ã ã‘ã§ã€é«˜åŠ¹ç‡ãªIPã‚³ã‚¢ã‚’ä½œæˆã§ãã¾ã—ãŸã€‚
ã—ã‹ã—ãã‚Œã§ã‚‚ã€PyTorchãªã©ã®æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã®ã¨æ¯”ã¹ã¦ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®è¨˜è¿°é‡ã¯ä½•å€ã‚‚å¤šããªã‚Šã¾ã—ãŸã€‚
å†…éƒ¨ã§è¡Œã‚ã‚Œã¦ã„ã‚‹å‡¦ç†ã®æµã‚Œã‚’ã‚ˆãè¦³å¯Ÿã—ã¦ã€å…¨ã¦ç†è§£ã—ãªã„ã¨ã€ãã‚Œã‚’é«˜é€ŸåŒ–ã™ã‚‹IPã‚³ã‚¢ã‚‚ä½œæˆã§ãã¾ã›ã‚“ã€‚
ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ãªã©ã€è€ƒãˆãªãã¦ã¯ãªã‚‰ãªã„äº‹æŸ„ã‚‚å¤šã„ã§ã™ã€‚
ä½œæ¥­å·¥ç¨‹ãŒå¤šãã¦å¤§å¤‰ã§ã™ãŒã€è‡ªä½œã®IPã‚³ã‚¢ãŒæ­£ã—ãå‹•ä½œã—ãŸ
(ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å®Ÿè£…ã¨åŒã˜ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚ŒãŸ)
ã¨ãã‚„ã€å®Ÿè£…ã‚’é«˜é€ŸåŒ–ã§ããŸã¨ãã®æ­“ã³ã¯ã€ãã®ã¶ã‚“å¤§ãã„ã¨æ€ã„ã¾ã™ã€‚
æœ‰é›£ã†ã”ã–ã„ã¾ã—ãŸã€‚</p>
<p>GPUã£ã¦ä¾¿åˆ©ã ãªã‚ã¨æ€ã†ã“ã¨ã—ãã‚Šã§ã™ã€‚</p>
</body>
</html>
