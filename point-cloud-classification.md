
---
title: ç‚¹ç¾¤å‡¦ç†ã®FPGAé«˜é€ŸåŒ–
author: SternGerlach
---

<!--
 pandoc -s -f markdown -t html5 --mathjax --css style.css point-cloud-classification.md -o point-cloud-classification.html
-->

[ãƒ›ãƒ¼ãƒ ã«æˆ»ã‚‹](./index.html)

# ã“ã®ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦

ã“ã®ãƒšãƒ¼ã‚¸ã¯ã€[æ…¶æ‡‰ç†å·¥ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼2022](https://adventar.org/calendars/7773)ã®22æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚
å»å¹´ã®è¨˜äº‹ã¯[ã“ã¡ã‚‰](./scan-matching-branch-and-bound.html)ã¨[ã“ã¡ã‚‰](./scan-matching-branch-and-bound-impl.html)ã§ã™ã€‚

æ—©é€Ÿä½™è«‡ã§ã™ãŒã€1983å¹´12æœˆ22æ—¥ã¯ã€Yellow Magic Orchestra (YMO) ãŒè¡Œã£ãŸæœ€å¾Œã®å›½å†…ãƒ„ã‚¢ãƒ¼ã®æœ€çµ‚æ—¥ã§ã€é–‹å‚¬å ´æ‰€ã¯æ—¥æœ¬æ­¦é“é¤¨ã§ã—ãŸã€‚
ä»Šæ—¥ã¯ã€ãã®æ•£é–‹ãƒ„ã‚¢ãƒ¼ã‹ã‚‰ã¡ã‚‡ã†ã©39å¹´ç›®ã®è¨˜å¿µã™ã¹ãæ—¥ã§ã™ã€‚
1984å¹´2æœˆ22æ—¥ç™ºå£²ã®ã€Œã‚¢ãƒ•ã‚¿ãƒ¼ãƒ»ã‚µãƒ¼ãƒ´ã‚£ã‚¹ã€ã‚„ã€1992å¹´11æœˆ21æ—¥ç™ºå£²ã®ã€Œã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆãƒ»ã‚µãƒ¼ãƒ´ã‚£ã‚¹ã€ã«éŸ³æºãŒåéŒ²ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã¿ãªã•ã‚“æ˜¯éè´ã„ã¦ã¿ã¦ãã ã•ã„ã€‚
ã¾ãŸä½™è«‡ã§ã™ãŒã€æ™®æ®µã¯(ç ”ç©¶ãã£ã¡ã®ã‘ã§)CDã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚
70å¹´ä»£ã‹ã‚‰80å¹´ä»£ã«ã‹ã‘ã¦ã®ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆãŒå¥½ãã§ã™ã€‚
æœ€è¿‘ã¯ã€å°‚ã‚‰ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã‚’è´ã„ã¦ã„ã¾ã™ã€‚
ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã®æ—§è¦æ ¼ç›¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯[ã“ã¡ã‚‰](./off-course-ca35-series.html)ã«ã‚ã‚Šã¾ã™ã€‚
ã¾ãŸã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¯[ã“ã¡ã‚‰](./cds.html)ã¨[ã“ã¡ã‚‰](./toshiba-emi.html)ã«ã¾ã¨ã‚ã¦ã‚ã‚Šã¾ã™ã€‚
æš‡ãªã¨ãã«ã”è¦§ãã ã•ã„ã€‚

ã‚‚ã†ä¸€ã¤ä½™è«‡ã€‚
ä»Šå¹´è´ã„ãŸãªã‹ã§æœ€ã‚‚è‰¯ã‹ã£ãŸã‚¢ãƒ«ãƒãƒ ã€‚

1. ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒHaloã€(1983å¹´ / VICL-62399 / 2007å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œä¸˜ã«å¹ãé¢¨ã€ğŸ¥ˆã€Œæ„›ã‚’æŠ±ãã—ã‚ã¦ã€ğŸ¥‰ã€Œè¼ãæ˜Ÿã€ã€Œæƒ³ã„å‡ºã®ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã€ã€Œã‚³ã‚¹ãƒ¢ã‚¹ã®å’²ãéƒ·ã€ã€Œæ˜Ÿç©ºã®ä¼è¨€ã€ã€Œã‚»ãƒ«ãƒªã‚¢ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼ã€
2. ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€Œã“ã®é“ã‚’ã‚†ã‘ã° ã‚ªãƒ•ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»ãƒ©ã‚¦ãƒ³ãƒ‰2ã€(1974å¹´ / CA35-1033 / 1983å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œã¯ãŸã¡ã®é ƒã€ğŸ¥ˆã€Œåˆ¥ã‚Œã®æƒ…æ™¯(1)ã€ğŸ¥‰ã€Œé¦–è¼ªã®ãªã„çŠ¬ã€ã€Œã‚ã®è§’ã‚’ã¾ãŒã‚Œã°ã€ã€Œæ—¥æ›œæ—¥ã®ãŸã„ãã¤ã€
3. ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€ŒI Love Youã€(1982å¹´ / CA35-1002 / 1982å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œå“€ã—ãè¡—ã€ğŸ¥ˆã€Œæ±ºã—ã¦å½¼ç­‰ã®ã‚ˆã†ã§ã¯ãªãã€ğŸ¥‰ã€ŒYes-Yes-Yesã€ã€Œæ„›ã®ã‚†ããˆã€
4. ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€Œãƒ¯ã‚¤ãƒ³ã®åŒ‚ã„ã€(1975å¹´ / CA35-1032 / 1983å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œå¹»æƒ³ã€ğŸ¥ˆã€Œè€äººã®ã¤ã¶ã‚„ãã€ğŸ¥‰ã€Œæ†‚ãä¸–ã«ã€ã€Œé›¨ã‚ˆæ¿€ã—ãã€ã€Œå€–ã›ãªã‚“ã¦ã€ã€Œãƒ¯ã‚¤ãƒ³ã®åŒ‚ã„ã€ã€Œçœ ã‚Œã¬å¤œã€
5. ã‚ªãƒ•ã‚³ãƒ¼ã‚¹ã€ŒSong Is Loveã€(1976å¹´ / CA35-1041 / 1983å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œå†¬ãŒæ¥ã‚‹ã¾ãˆã«ã€ğŸ¥ˆã€Œé’ç©ºã¨äººç”Ÿã¨ã€ğŸ¥‰ã€Œæ­Œã‚’æ§ã’ã¦ã€ã€Œé’æ˜¥ã€ã€Œã²ã¨ã‚Šã§ç”Ÿãã¦ã‚†ã‘ã‚Œã°ã€
6. ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒNew Tuneã€(1985å¹´ / 35FD-1005 / 1985å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œã‚‚ã£ã¨å¹¸ã›ã«ç´ ç›´ã«ãªã‚ŒãŸã‚‰ã€ğŸ¥ˆã€Œãƒ­ãƒ™ãƒªã‚¢ã€ğŸ¥‰ã€ŒOur Songã€ã€ŒãµãŸã¤ã‚ã®ã‚¯ãƒªã‚¹ãƒã‚¹ã€ã€Œãã‚“ãªç”·ã«ãªã‚ŒãŸã‚‰ã€
7. å¤§æ»è© ä¸€ã€ŒEach Timeã€(1984å¹´ / 35DH 78 / 1984å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€ŒBachelor Girlã€ğŸ¥ˆã€Œãƒšãƒ‘ãƒ¼ãƒŸãƒ³ãƒˆãƒ»ãƒ–ãƒ«ãƒ¼ã€ğŸ¥‰ã€Œé­”æ³•ã®ç³ã€ã€Œæ‹ã®ãƒŠãƒƒã‚¯ãƒ«ãƒœãƒ¼ãƒ«ã€
8. éº—ç¾ã€Œ"R"ã€(1984å¹´ / 35C31-7250 / 1984å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œæ˜Ÿã®ã‚¯ãƒ©ã‚¤ãƒãƒ¼ã€ğŸ¥ˆã€Œé¢¨ã¯æ˜æ—¥ã¸ã€ğŸ¥‰ã€Œç©ºãŒä¸€é¢æµ·ã«è¦‹ãˆãŸæ—¥ã€ã€Œæ‹ã®ä¸€æ™‚é–“ã¯å­¤ç‹¬ã®åƒå¹´ã€ã€Œé’æ˜¥ã®ãƒªã‚°ãƒ¬ãƒƒãƒˆã€ã€Œãƒãƒ‹ãƒ¼ãƒ†ã‚¤ãƒ«ã€
9. ãƒã‚¤ãƒ»ãƒ•ã‚¡ã‚¤ãƒ»ã‚»ãƒƒãƒˆã€ŒSweet Locomotionã€(1986å¹´ / 32DH 393 / 1986å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œã²ã¨ãã‚Œã®æ‹ã€ğŸ¥ˆã€ŒãŸã£ãŸä¸€æšã®ãƒ•ã‚©ãƒˆã‚°ãƒ©ãƒ•ã€ğŸ¥‰ã€ŒElevator Townã€ã€ŒDo You Remember Me?ã€
10. å’Œä¹…äº•æ˜ è¦‹ã€ŒFloraã€(1990å¹´ / PSCR-1006 / 1990å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œãƒã‚¤ãƒ»ãƒ­ãƒ³ãƒªã‚£ãƒ»ã‚°ãƒƒãƒã‚¤ãƒ»ã‚¯ãƒ©ãƒ–ã€ğŸ¥ˆã€Œå¶ç„¶ã®æ—…äººã€ğŸ¥‰ã€Œå¤¢ã§ä¼šã„ã¾ã—ã‚‡ã†ã€ã€Œç¥æ§˜ãŒã„ãªã„åœŸæ›œæ—¥ã€
11. éˆ´æœ¨åº·åšã€ŒSincerelyã€(1983å¹´ / CA35-1043 / 1983å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œç‘ ç’ƒè‰²ã®å¤œæ˜ã‘ã€ğŸ¥ˆã€Œåƒ•ã¨æµ·ã¸ã€ğŸ¥‰ã€Œãƒ©ãƒ©ãƒ© ï½æ„›ã®ä¸–ç•Œã¸ï½ã€ã€Œå…¥ã‚Šæ±Ÿã€ã€Œå›ã®èª•ç”Ÿæ—¥ã€
12. å²¡ç”°æœ‰å¸Œå­ã€Œãƒ´ã‚£ãƒ¼ãƒŠã‚¹èª•ç”Ÿã€(1986å¹´ / D32A0164 / 1986å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œãƒ´ã‚£ãƒ¼ãƒŠã‚¹èª•ç”Ÿã€ğŸ¥ˆã€ŒéŠ€æ²³ã®ãƒã‚«ãƒ³ã‚¹ã€ğŸ¥‰ã€Œçœ ã‚Œã¬å¤œã®Aquariusã€ã€ŒWonder Trip Loverã€ã€ŒSpring Accidentã€
13. å°¾å´äºœç¾ã€ŒKidsã€(1986å¹´ / D32A0235 / 1986å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œæµã‚Œæ˜ŸãŒå¥½ãã€ğŸ¥ˆã€Œã‚·ãƒ£ã‚¤ãƒã‚¹ãƒœãƒ¼ã‚¤ã€ğŸ¥‰ã€ŒSt.Valentine's Day Rhapsodyã€ã€ŒCom'on Mamyã€
14. ä¹…ä¿ç”°æ—©ç´€ã€Œå¤œã®åº•ã¯æŸ”ã‚‰ã‹ãªå¹»ã€(1984å¹´ / DYCL-17 / 2005å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œãƒ”ã‚¢ãƒ‹ãƒƒã‚·ãƒ¢ã§...ã€ğŸ¥ˆã€Œå¯’ã„çµµè‘‰æ›¸ã€ğŸ¥‰ã€Œæœˆã®æµœè¾ºãƒœã‚¿ãƒ³ãŒã²ã¨ã¤ã€ã€Œãƒ¡ãƒ©ãƒ³ã‚³ãƒªãƒ¼ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¯ãƒ­ã‚¹ã€
15. è–¬å¸«ä¸¸ã²ã‚å­ã€ŒèŠ±å›³é‘‘ã€(1986å¹´ / CA32-1260 / 1986å¹´ç›¤)
    - ç‰¹ã«ã‚ˆã‹ã£ãŸæ›²: ğŸ¥‡ã€Œç´…ã„èŠ±ã€é’ã„èŠ±ã€ğŸ¥ˆã€Œå¯’æ¤¿ã€å’²ã„ãŸã€ğŸ¥‰ã€Œãƒ­ãƒ¼ã‚ºãƒ»ãƒ†ã‚£ãƒ¼ã¯ã„ã‹ãŒ?ã€ã€Œå“€ã—ã¿ã®ç¨®ã€ã€Œé€æ˜ãªãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ã€Œéº¦ã‚ã‚‰å¸½å­ã®ã‚¢ãƒ³ã€

ã‚¤ãƒ³ãƒˆãƒ­ãŒè‰¯ã„æ›² (ãŠã¾ã‘)ã€‚

1. ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—ã€ŒShooting Starã€(1981å¹´)
2. äº•ä¸Šé‘‘ã€ŒKarsavina ï½ãƒ‹ã‚¸ãƒ³ã‚¹ã‚­ãƒ¼ã®ç¿¼ã€(1983å¹´)
3. äº•ä¸Šé‘‘ã€ŒRunning Fence -Ode A Christoã€(1982å¹´)

ä»Šå¹´ã¯ã€ç‚¹ç¾¤å‡¦ç† (ç‚¹ç¾¤åˆ†é¡ã‚¿ã‚¹ã‚¯) å‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®FPGAé«˜é€ŸåŒ–ã‚’è©¦ã—ã¦ã¿ã¾ã™ã€‚
LeNetã‚„ResNetãªã©ã€ç”»åƒå‡¦ç†å‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®FPGAé«˜é€ŸåŒ–ã‚‚é¢ç™½ã„ã®ã§ã™ãŒã€æ—¢ã«ãŸãã•ã‚“ã®ç´ æ™´ã‚‰ã—ã„è¨˜äº‹ãŒå‡ºã¦ã„ã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸã€‚
éŸ³æ¥½ã®è©±ã‚‚ã€èª°ã«ã‚‚é€šã˜ãªã„ã—ã€ã‚¦ã‚±ãªã„ã¨æ€ã£ãŸã®ã§ã‚„ã‚ã¾ã—ãŸã€‚
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§é–²è¦§ã•ã‚Œã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

# ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æº–å‚™

ç‚¹ç¾¤ã®åˆ†é¡ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã—ãŸä»£è¡¨çš„ãªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€2017å¹´ã«CVPRã§ç™ºè¡¨ã•ã‚ŒãŸPointNetãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
PointNetã¯ã€MLPã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã‹ã‚‰ãªã‚‹ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
åˆ†é¡ã‚¿ã‚¹ã‚¯å‘ã‘ã®PointNetã®æ§‹é€ ã‚’ã€ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-layers.svg" width="100%" />](point-cloud-classification-images/pointnet-layers.svg)

ãƒ¢ãƒ‡ãƒ«ã¯ã€ç‚¹ç¾¤ã‹ã‚‰ã®ç‰¹å¾´æŠ½å‡ºã¨ã€ç‰¹å¾´ã«åŸºã¥ãåˆ†é¡ã®ã€2ã¤ã®éƒ¨åˆ†ã«åˆ†ã‘ã‚‰ã‚Œã¾ã™ (å›³ã®Feature extractionã¨Classification)ã€‚

å›³ã®å·¦ç«¯ã«ç¤ºã™ã‚ˆã†ã«ã€$N$å€‹ã®ç‚¹ã‚’å«ã‚€ã€3æ¬¡å…ƒã®ç‚¹ç¾¤$\mathcal{P} = \left\{ \boldsymbol{p}_1, \ldots, \boldsymbol{p}_N \right\} \in \mathbb{R}^{N \times 3}$ãŒå…¥åŠ›ã§ã™ã€‚
MLPã‚’ç”¨ã„ã¦ã€å„ç‚¹$\boldsymbol{p}_i \in \mathbb{R}^3$ã«å¯¾ã—ã¦ã€1024æ¬¡å…ƒã®ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´$\boldsymbol{\psi}_i \in \mathbb{R}^{1024}$ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
å…¨ã¦ã®ç‚¹ã«å¯¾ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\Psi} = \left\{ \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_N \right\} \in \mathbb{R}^{N \times 1024}$ã‚’è¨ˆç®—ã—ãŸã‚‰ã€ãã‚Œã‚‰ã‚’Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã‚ˆã‚Šé›†ç´„ã—ã¦ã€ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi} \in \mathbb{R}^{1024}$ã‚’å¾—ã¾ã™ ($\boldsymbol{\phi} \gets \max(\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_N)$)ã€‚

åˆ†é¡ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ã“ã®ç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’å…¥åŠ›ã¨ã—ã¦ã€å„ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ (ã‚¹ã‚³ã‚¢)ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹æ•°ã‚’$K$ã¨ã™ã‚Œã°ã€å‡ºåŠ›ã¯$K$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ãªã‚Šã¾ã™ã€‚

å›³ã®Input TransformãŠã‚ˆã³Feature Transformã¯ã€ç‚¹ç¾¤ã®ç‰¹å¾´ã«å¯¾ã—ã¦ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã‚’æ–½ã—ã€å‰›ä½“å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªç‰¹å¾´é‡ã‚’å¾—ã‚‹ãŸã‚ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ãŒã€å®Ÿè£…ãŒé¢å€’ãªã®ã§å–ã‚Šé™¤ãã¾ã™(**æœ€é©åŒ–ãã®1: ãƒ¢ãƒ‡ãƒ«ã®ç°¡ç•¥åŒ–**)ã€‚
å¾“ã£ã¦ã€ä»Šå›FPGAä¸Šã«å®Ÿè£…ã™ã‚‹PointNetã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ç”»åƒèªè­˜å‘ã‘ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šã€ç•³ã¿è¾¼ã¿å±¤ãŒã‚ã‚Šã¾ã›ã‚“ã€‚
ã¾ãŸã€MLPã¯ã€å…¨çµåˆå±¤ã€ReLUæ´»æ€§åŒ–å±¤ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã¨ã—ã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-layers2.svg" width="80%" />](point-cloud-classification-images/pointnet-layers2.svg)

PyTorchã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ (`net/model.py`)ã€‚
ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰å…¨ä½“ã¯[ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/sterngerlach/advent_2022_point_cloud_classification)ã«ç½®ã‹ã‚Œã¦ã„ã‚‹ã®ã§ã€é©å®œã”å‚ç…§ãã ã•ã„ã€‚

```Python
class PointNetFeat(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = torch.nn.Conv1d(3, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 64, 1)
        self.conv3 = torch.nn.Conv1d(64, 64, 1)
        self.conv4 = torch.nn.Conv1d(64, 128, 1)
        self.conv5 = torch.nn.Conv1d(128, 1024, 1)
        self.bn1 = torch.nn.BatchNorm1d(64)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.bn3 = torch.nn.BatchNorm1d(64)
        self.bn4 = torch.nn.BatchNorm1d(128)
        self.bn5 = torch.nn.BatchNorm1d(1024)

    def forward(self, x: torch.Tensor):
        # `x` is of size [B, N, 3]
        N = x.shape[1]
        # `x` is of size [B, 3, N]
        x = x.transpose(1, 2)

        # `x` is of size [B, 1024, N]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))

        # `x` is of size [B, 1024]
        x = torch.max(x, dim=2)[0]

        return x

class PointNetCls(torch.nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()

        # Feature extraction
        self.feat = PointNetFeat()

        # Classification network
        self.fc1 = torch.nn.Linear(1024, 512)
        self.fc2 = torch.nn.Linear(512, 256)
        self.fc3 = torch.nn.Linear(256, num_classes)
        self.bn1 = torch.nn.BatchNorm1d(512)
        self.bn2 = torch.nn.BatchNorm1d(256)

    def forward(self, x):
        # `x` is of size [B, N, 3]
        # `x` is of size [B, 1024]
        x = self.feat(x)

        # `x` is of size [B, `num_classes`]
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)

        return x
```

ã•ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾å®Ÿè£…ã™ã‚‹å ´åˆã€æ¬¡ã®ã‚ˆã†ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºéƒ¨åˆ† (å›³ã®Feature extraction)ã«æ³¨ç›®ã—ã¾ã™ã€‚
å›³ä¸­ã®ç°è‰²ã®å››è§’ã«ç¤ºã™ã‚ˆã†ã«ã€$N$å€‹å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚„ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\Psi}$ã‚’ã€ã©ã“ã‹ã«ä¿æŒã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
å¤§å®¹é‡ã®ãƒ¡ãƒ¢ãƒªã‚’æ­è¼‰ã—ãŸGPUã§ã‚ã‚Œã°ã€ã“ã‚Œã§ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ãŒã€FPGAå†…éƒ¨ã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª (BlockRAM) ã¯éå¸¸ã«å®¹é‡ãŒå°‘ãªã„ã®ã§ã€å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚’ä¿æŒã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªãŒã‚ã£ã¨ã„ã†é–“ã«æ¯æ¸‡ã™ã‚‹ã§ã—ã‚‡ã†ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€æ­è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®å®¹é‡ã«ã‚ˆã£ã¦ã€ç‚¹ã®å€‹æ•°$N$ãŒåˆ¶é™ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚
ã“ã‚Œã¯é¿ã‘ãŸã„ã‚‚ã®ã§ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®ä»£ã‚ã‚Šã«ã€å®¹é‡ã®å¤§ããªDRAMä¸Šã«ç½®ãã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã¯é•·ããªã‚Šã¾ã™ã€‚
å…¨ã¦ã®å±¤ã®ä¸­é–“çµæœã‚’DRAMã«ç½®ãã¨ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¢—åŠ ã—ã¦ã€æ€§èƒ½ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã—ã¾ã™ã€‚
å±¤ã®ä¸­é–“çµæœã¯ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«ç½®ããŸã„ã‚‚ã®ã§ã™ã€‚

ãã“ã§ã€å…¨ã¦ã®ç‚¹$\mathcal{P}$ã«å¯¾ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\Psi}$ã‚’ä¸€æ°—ã«è¨ˆç®—ã™ã‚‹ã®ã§ã¯ãªãã€1ã¤ãšã¤ã®ç‚¹$\boldsymbol{p}$ã«å¯¾ã—ã¦é †ã«ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\psi}$ã‚’è¨ˆç®—ã—ã¾ã—ã‚‡ã†ã€‚
ä¸€æ°—ã«è¨ˆç®—ã™ã‚‹ã®ã¨æ¯”ã¹ã¦è¨ˆç®—åŠ¹ç‡ã¯è½ã¡ã¾ã™ãŒã€1ã¤ã®ç‚¹ã«å¯¾ã™ã‚‹ä¸­é–“çµæœã‚„ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ã ã‘ã‚’ä¿æŒã™ã‚Œã°ã‚ˆã„ã®ã§ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®æ¶ˆè²»ã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ã€‚

ä»¥å‰ã¯ (PyTorchãªã©ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ã†å ´åˆã¯)ã€ç‰¹å¾´æŠ½å‡ºã¯æ¬¡ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã¦ã„ã¾ã—ãŸã€‚

1. å…¨ã¦ã®ç‚¹$\mathcal{P}$ã«å¯¾ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ã‚’$\boldsymbol{\Psi}$ã‚’ã¾ã¨ã‚ã¦è¨ˆç®—ã™ã‚‹ ($(N, 64)$ã‚„$(N, 1024)$ã®ãƒãƒƒãƒ•ã‚¡ãŒå¿…è¦)ã€‚
2. Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã‚ˆã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\Psi}$ã‚’é›†ç´„ã—ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’å¾—ã‚‹ ($\boldsymbol{\phi} \gets \max(\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_N)$)ã€‚
3. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’MLPã«å…¥åŠ›ã—ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ($K$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«)ã‚’å¾—ã‚‹ã€‚

ã“ã‚Œã‚’ã€æ¬¡ã®ã‚ˆã†ã«å¤‰æ›´ã—ã¾ã™(**æœ€é©åŒ–ãã®2: è¨ˆç®—é †åºã®å¤‰æ›´**)ã€‚

1. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’ã€$\boldsymbol{0}$ã§åˆæœŸåŒ–ã™ã‚‹ã€‚
1. å„ç‚¹$\boldsymbol{p}_i \ (i = 1, \ldots, N)$ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã†ã€‚
    1. MLPã®é †ä¼æ’­ã«ã‚ˆã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\psi}_i$ã‚’å¾—ã‚‹ ($(1, 64)$ã‚„$(1, 1024)$ã®ãƒãƒƒãƒ•ã‚¡ãŒã‚ã‚Œã°ã‚ˆã„)ã€‚
    2. $\boldsymbol{\phi}$ã¨$\boldsymbol{\psi}_i$ã¨ã®ã€è¦ç´ ã”ã¨ã®$\max$ã‚’ã¨ã‚‹ã“ã¨ã§ã€$\boldsymbol{\phi}$ã‚’æ›´æ–°ã™ã‚‹ ($\boldsymbol{\phi} \gets \max(\boldsymbol{\phi}, \boldsymbol{\psi}_i)$)ã€‚
3. ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’MLPã«å…¥åŠ›ã—ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ($K$æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«)ã‚’å¾—ã‚‹ã€‚

å…¨ã¦ã®ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\Psi}$ã‚’é›†ç´„ã™ã‚‹ã®ã§ã¯ãªãã€å„ç‚¹$\boldsymbol{p}_i$ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\psi}_i$ã‚’ä½¿ã£ã¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡$\boldsymbol{\phi}$ã‚’é€æ¬¡çš„ã«æ›´æ–°ã—ã¦ã„ãã¾ã™ã€‚
ã“ã‚Œã¯è¿‘ä¼¼ã§ã¯ãªã„ã®ã§ã€å…¨ãåŒã˜çµæœã¨ãªã‚Šã¾ã™ã€‚

æœ€çµ‚çš„ã«ã€ä»Šå›FPGAä¸Šã«å®Ÿè£…ã™ã‚‹PointNetã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-layers3.svg" width="80%" />](point-cloud-classification-images/pointnet-layers3.svg)

# é«˜ä½åˆæˆã«ã‚ˆã‚‹å®Ÿè£…

ä»Šå›ã¯ã€é«˜ä½åˆæˆ (HLS: High-Level Synthesis)ã‚’ç”¨ã„ã¦ã€ä¸Šè¨˜ã«ç¤ºã™PointNetã®å°‚ç”¨å›è·¯ (**IPã‚³ã‚¢**) ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®æ¨è«–ã‚’å®Ÿç¾ã™ã‚‹åˆ¥ã®æ‰‹æ®µã¨ã—ã¦ã€è¡Œåˆ—æ¼”ç®—ã‚„ç•³ã¿è¾¼ã¿æ¼”ç®—ç”¨ã®ã€å·¨å¤§ã‹ã¤æ±ç”¨çš„ãªæ¼”ç®—å›è·¯ã‚’FPGAä¸Šã«å®Ÿè£…ã—ã€ãã‚Œã«ç¹°ã‚Šè¿”ã—ãƒ‡ãƒ¼ã‚¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

é«˜ä½åˆæˆã¯ã€C/C++ã«ã‚ˆã‚‹å‹•ä½œãƒ¬ãƒ™ãƒ« (Behavior Level) ã®å›è·¯è¨˜è¿°ã‚’ã€Verilog HDLã‚„SystemVerilogã«ã‚ˆã‚‹ãƒ¬ã‚¸ã‚¹ã‚¿è»¢é€ãƒ¬ãƒ™ãƒ« (RTL: Register Transfer Level) ã®å›è·¯è¨˜è¿°ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚
Verilog HDLã‚’ç›´æ¥è¨˜è¿°ã™ã‚‹ã®ã«æ¯”ã¹ã¦ã€é¥ã‹ã«æ¥½ã§ã€ã‚¹ãƒˆãƒ¬ã‚¹ãŒå°‘ãªãã€ç”Ÿç”£æ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚
ä½†ã—ã€C/C++ã§è¨˜è¿°ã™ã‚‹ã¨ã¯ã„ã£ã¦ã‚‚ã€é€šå¸¸ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã¨ã¯å…¨ãæ§˜ç›¸ãŒç•°ãªã‚Šã¾ã™ã€‚
`malloc()`ã‚„`new`ã¯ã‚‚ã¡ã‚ã‚“ã®ã“ã¨ã€ã“ã‚Œã‚‰ã«ä¾å­˜ã™ã‚‹`std::vector`ãªã©ã®ä¾¿åˆ©ãªãƒ‡ãƒ¼ã‚¿å‹ã‚‚ä½¿ãˆãªã„ã®ã§ã€å›ºå®šé•·ã®é…åˆ—ã«ç½®ãæ›ãˆã¦ã©ã†ã«ã‹ã—ã¾ã™ã€‚
ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã‚µã‚¤ã‚ºãŒå›ºå®šã§ã€ä¸€èˆ¬ã«ã¯æ±ºã¾ã£ãŸå‹•ä½œã‚’ã™ã‚‹ã®ã§ã€FPGAä¸Šã«å®Ÿè£…ã—ã‚„ã™ã„ã§ã™ã€‚

é«˜ä½åˆæˆç”¨ã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã€Xilinxç¤¾ã®Vitis HLS 2022.1ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ã¾ãŸå®Ÿè£…å¯¾è±¡ã®FPGAã¨ã—ã¦ã€Xilinx ZCU104 Evaluation Board (XCZU7EV-2FFVC1156)ã‚’ä½¿ã„ã¾ã™ã€‚
Xilinx ZCU104ã«ã¯ã€FPGAã®ã»ã‹ã«ã€ã‚¯ã‚¢ãƒƒãƒ‰ã‚³ã‚¢ ARM Cortex-A53 CPU (1.2GHz)ã¨2GBã®DRAMã‚‚æ­è¼‰ã•ã‚Œã¦ãŠã‚Šã€LinuxãŒå‹•ä½œã—ã¾ã™ã€‚

æ—©é€Ÿã€PointNetã®IPã‚³ã‚¢ã‚’ç¤ºã—ã¾ã™ (é©å®œGitHubã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã”è¦§ãã ã•ã„)ã€‚
é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒGCC 6.2ã§ã™ã®ã§ã€C++14ã‚„C++17ã®ä¸€éƒ¨æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã™ã€‚
ä½†ã—ã€ãƒ„ãƒ¼ãƒ«ã®ãƒã‚°ã‚’è¸ã‚€ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ã‚ã¾ã‚Šå‡ã£ãŸæ©Ÿèƒ½ã¯ä½¿ã‚ãªã„ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

```C++
// Size of the PointNet classification network
// Refer to net/model.py for details

// Size of the feature extraction network
constexpr const int kFeatDims0 = 3;
constexpr const int kFeatDims1 = 64;
constexpr const int kFeatDims2 = 64;
constexpr const int kFeatDims3 = 64;
constexpr const int kFeatDims4 = 128;
constexpr const int kFeatDims5 = 1024;

// Size of the classification network
// ModelNet40 has 40 object classes
constexpr const int kClsDims0 = kFeatDims5;
constexpr const int kClsDims1 = 512;
constexpr const int kClsDims2 = 256;
constexpr const int kClsDims3 = 40;

// Top function
void PointNetClsTop(const int op_mode,
                    const float* point_cloud,
                    const int num_points,
                    float* out_logits,
                    const float* feat_params1,
                    const float* feat_params2,
                    const float* feat_params3,
                    const float* feat_params4,
                    const float* feat_params5,
                    const float* cls_params1,
                    const float* cls_params2,
                    const float* cls_params3)
{
#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params2 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params3 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params4 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params5 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params1 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params2 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0

#pragma HLS INTERFACE s_axilite port=op_mode bundle=control
#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control
#pragma HLS INTERFACE s_axilite port=num_points bundle=control
#pragma HLS INTERFACE s_axilite port=out_logits bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params2 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params3 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params4 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params5 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params1 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params2 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control
#pragma HLS INTERFACE s_axilite port=return bundle=control

  // Parameters for feature extraction
  LinearParams<param_t, kFeatDims0, kFeatDims1> feat_conv1;
  LinearParams<param_t, kFeatDims1, kFeatDims2> feat_conv2;
  LinearParams<param_t, kFeatDims2, kFeatDims3> feat_conv3;
  LinearParams<param_t, kFeatDims3, kFeatDims4> feat_conv4;
  LinearParams<param_t, kFeatDims4, kFeatDims5> feat_conv5;
  BatchNorm1dParams<param_t, kFeatDims1> feat_bn1;
  BatchNorm1dParams<param_t, kFeatDims2> feat_bn2;
  BatchNorm1dParams<param_t, kFeatDims3> feat_bn3;
  BatchNorm1dParams<param_t, kFeatDims4> feat_bn4;
  BatchNorm1dParams<param_t, kFeatDims5> feat_bn5;

  // Parameters for classification network
  // LinearParams<param_t, kClsDims0, kClsDims1> cls_fc1;
  // LinearParams<param_t, kClsDims1, kClsDims2> cls_fc2;
  LinearParams<param_t, kClsDims2, kClsDims3> cls_fc3;
  BatchNorm1dParams<param_t, kClsDims1> cls_bn1;
  BatchNorm1dParams<param_t, kClsDims2> cls_bn2;

  // Extracted feature
  value_t feature[kFeatDims5];

  if (op_mode == kModeInitWeights) {
    // Initialize the PointNet feature extraction network
    InitializeFeatNaive<param_t>(
      &feat_conv1, &feat_conv2, &feat_conv3, &feat_conv4, &feat_conv5,
      &feat_bn1, &feat_bn2, &feat_bn3, &feat_bn4, &feat_bn5,
      feat_params1, feat_params2, feat_params3, feat_params4, feat_params5);
    // Initialize the classification network
    InitializeClsNaive<param_t>(
      &cls_fc3, &cls_bn1, &cls_bn2,
      cls_params1, cls_params2, cls_params3);
  } else if (op_mode == kModeInference) {
    // Run the PointNet feature extraction
    InferenceFeatNaive<value_t, param_t, 1024>(
      point_cloud, num_points, feature,
      &feat_conv1, &feat_conv2, &feat_conv3, &feat_conv4, &feat_conv5,
      &feat_bn1, &feat_bn2, &feat_bn3, &feat_bn4, &feat_bn5);

    // Run the classification
    InferenceClsNaive<value_t, param_t>(
      feature, out_logits,
      &cls_fc3, &cls_bn1, &cls_bn2,
      cls_params1, cls_params2, cls_params3);
  }
}
```

ä¸Šè¨˜ã‚’é«˜ä½åˆæˆã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãªIPã‚³ã‚¢ãŒä½œã‚‰ã‚Œã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-ip-core.svg" width="50%" />](point-cloud-classification-images/pointnet-ip-core.svg)

ã“ã®IPã‚³ã‚¢ã‚’åˆ¥ã®IPã‚³ã‚¢ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ (å¾Œè¿°)ã€æ¬¡ã®ã‚ˆã†ãªãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã§ãã¾ã™ã€‚

[<img src="point-cloud-classification-images/board-design.svg" width="100%" />](point-cloud-classification-images/board-design.svg)

ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ã«å¯¾ã—ã¦ã€è«–ç†åˆæˆãŠã‚ˆã³é…ç½®é…ç·šã™ã‚‹ã“ã¨ã§ã€å›è·¯æƒ…å ±ã‚’è¡¨ã™ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ  (Bitstream) ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’FPGAã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã€PointNetã®å°‚ç”¨å›è·¯ãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆ

`PointNetClsTop`ãŒã€IPã‚³ã‚¢ã‚’è¡¨ã™æœ€ä¸Šä½ã®é–¢æ•°ã§ã™ã€‚
ãƒˆãƒƒãƒ—é–¢æ•° (Top function) ã¨ã‚ˆã°ã‚Œã¾ã™ã€‚
é–¢æ•°ã®å¼•æ•°ã¯ã€IPã‚³ã‚¢ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã¨ãªã‚Šã€åˆ¥ã®IPã‚³ã‚¢ã«æ¥ç¶šã•ã‚Œã¾ã™ (ä¸Šã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ã”è¦§ãã ã•ã„)ã€‚
HLSã§ã¯ã€é–¢æ•°ãã®ã‚‚ã®ãŒå›è·¯ (Verilog HDLã«ãŠã‘ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«) ã«ãªã‚Šã¾ã™ã€‚
é–¢æ•°ã®å†å¸°å‘¼ã³å‡ºã—ã¯ã§ãã¾ã›ã‚“ã€‚

ç‰¹å¾´æŠ½å‡ºç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯5ã¤ã®MLPã€ã¾ãŸã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¯3ã¤ã®MLPãŒå«ã¾ã‚Œã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰æ“ä½œã§ãã‚‹ã‚ˆã†ã«ã€DRAMä¸Šã®ãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚Œã¾ã™ã€‚
ã¾ãŸã€ç‚¹ç¾¤$\mathcal{P}$ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›(ãƒ­ã‚¸ãƒƒãƒˆ)ã‚‚åŒæ§˜ã«ã€DRAMãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚Œã¾ã™ã€‚

`feat_params1`ã‹ã‚‰`feat_params5`ã¾ã§ã¨ã€`cls_params1`ã‹ã‚‰`cls_params3`ã¾ã§ã®8ã¤ã®ãƒãƒ¼ãƒˆã¯ã€DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€IPã‚³ã‚¢å´ã‹ã‚‰èª­ã¿å–ã‚‹ãŸã‚ã«ä½¿ã„ã¾ã™ã€‚
`point_cloud`ã¯ç‚¹ç¾¤ã®èª­ã¿å‡ºã—ã€`out_logits`ã¯ãƒ­ã‚¸ãƒƒãƒˆã®æ›¸ãè¾¼ã¿ã®ãŸã‚ã«ä½¿ã„ã¾ã™ã€‚
`op_mode`ã¯å›è·¯ã®å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã€`num_points`ã¯ç‚¹ã®å€‹æ•°$N$ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã§ã™ã€‚

`#pragma HLS`ã‹ã‚‰å§‹ã¾ã‚‹è¡Œã¯ã€é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã«å¯¾ã—ã¦ã€C/C++ã‹ã‚‰RTLã«å¤‰æ›ã™ã‚‹éš›ã®ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆã¾ã™ (å¿…ãšã—ã‚‚å®ˆã£ã¦ãã‚Œã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“)ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãªã©ã¯C/C++ã§ã¯è¨˜è¿°ã§ãã¾ã›ã‚“ãŒã€ã“ã®ã‚ˆã†ãª**HLSãƒ—ãƒ©ã‚°ãƒ**ã‚’é©åˆ‡ãªå ´æ‰€ã«ç½®ãã“ã¨ã§ã€é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ãŒè‡ªå‹•çš„ã«ã“ã‚Œã‚‰ã®æœ€é©åŒ–ã‚’æ–½ã—ã¦ãã‚Œã¾ã™ã€‚

`#pragma HLS INLINE off`ã¨ã™ã‚‹ã¨ã€ãã®é–¢æ•°ãŒã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã•ã‚Œãªããªã‚Šã¾ã™ (å¿…ãšã€1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ä½œã‚‰ã‚Œã‚‹)ã€‚
å¤§ããªé–¢æ•°ã§ã‚ã‚Œã°ã€è‡ªå‹•çš„ã«ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€å¿µã®ãŸã‚ä»˜ä¸ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚ˆã†ãªçŠ¶æ³ã§ã¯ã€é–¢æ•°`B`ã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ã—ãªã„æ–¹ãŒã„ã„ã¨æ€ã„ã¾ã™ã€‚
åŒæ™‚ã«ä½¿ã‚ã‚Œãªã„ã®ã«ã‚‚é–¢ã‚ã‚‰ãšã€é–¢æ•°`A`ã®å†…éƒ¨ã«`B`ã®ã‚³ãƒ”ãƒ¼ãŒ3ã¤ä½œã‚‰ã‚Œã¦ã€ãƒªã‚½ãƒ¼ã‚¹ã®ç„¡é§„é£ã„ã¨ãªã‚Šã¾ã™ã€‚
é–¢æ•°`B`ã®ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–ã‚’æŠ‘åˆ¶ã—ã¦ã€`B`ã‚’1ã¤ã ã‘ä½œã‚Šã€ãã‚Œã‚’ä½¿ã„å›ã—ãŸæ–¹ãŒã„ã„ã§ã—ã‚‡ã†ã€‚
```C++
void B(const float x_in[10], float y_out[10])
{
#pragma HLS INLINE

  // ä½•ã‚‰ã‹ã®å‡¦ç†
}

void A(const float x_in[10], float y_out[10])
{
  float x0[10];
  float x1[10];
  B(x_in, x0);
  B(x0, x1);
  B(x1, y_out);
}
```

`#pragma HLS INTERFACE m_axi`ã¨ã€`#pragma HLS INTERFACE s_axilite`ã®è¨˜è¿°ãŒç›®ç«‹ã¡ã¾ã™ãŒã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆ (ä¾‹ãˆã°`feat_params1`) ã«å¯¾ã—ã¦ã“ã®2ã¤ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’è¨˜è¿°ã™ã‚‹ã¨ã€IPã‚³ã‚¢å´ã‹ã‚‰DRAMãƒãƒƒãƒ•ã‚¡ã‚’èª­ã¿æ›¸ãã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
èª­ã¿æ›¸ãã®éš›ã«ã¯ã€AXIã¨ã‚ˆã°ã‚Œã‚‹ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€`#pragma HLS INTERFACE m_axi`ã«ã‚ˆã£ã¦ãã‚Œã‚’æŒ‡å®šã§ãã¾ã™ (IPã‚³ã‚¢å´ãŒãƒã‚¹ã‚¿ãƒ¼ã«ãªã‚Šã¾ã™)ã€‚

ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã¯ã€å„ãƒãƒ¼ãƒˆã«å¯¾ã—ã¦ã€ãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¦ã€ãƒãƒ¼ãƒˆã¨ãƒãƒƒãƒ•ã‚¡ã‚’ç´ã¥ã‘ã¾ã™ã€‚
å„ãƒãƒ¼ãƒˆã«ã¯ã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€`#pragma HLS INTERFACE s_axilite`ã«ã‚ˆã£ã¦ãã‚Œã‚’å®Ÿç¾ã§ãã¾ã™ (IPã‚³ã‚¢å´ã‹ã‚‰ã¿ã‚‹ã¨ã‚¹ãƒ¬ãƒ¼ãƒ–ã§ã™)ã€‚
`op_mode`ã€`num_points`ã«å¯¾ã—ã¦ã‚‚ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚
`port=return`ã¨ã—ã¦ã„ã‚‹è¡Œã¯ã€IPã‚³ã‚¢ç”¨ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½œæˆã—ã€CPUå´ã‹ã‚‰IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ãŸã‚Šã€çŠ¶æ…‹ (ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ãªã®ã‹å‹•ä½œä¸­ã‹) ã‚’èª­ã¿å–ã£ãŸã‚Šã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ¬ã‚¸ã‚¹ã‚¿ã¯ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/OãŠã‚ˆã³AXI-Liteãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«ã‚ˆã£ã¦èª­ã¿æ›¸ãã•ã‚Œã¾ã™ã€‚

å„å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã‹ã‚‰ã¯ã€PyTorchã®ãƒ¢ãƒ‡ãƒ«ã§å®šç¾©ã—ãŸã€å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒèª­ã¿å‡ºã•ã‚Œã¾ã™ (ä¸€æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦ã€å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé€£çµã•ã‚Œã¾ã™)ã€‚

- `feat_params1`: `PointNetFeat::conv1` + `PointNetFeat::bn1`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `feat_params2`: `PointNetFeat::conv2` + `PointNetFeat::bn2`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `feat_params3`: `PointNetFeat::conv3` + `PointNetFeat::bn3`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `feat_params4`: `PointNetFeat::conv4` + `PointNetFeat::bn4`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `feat_params5`: `PointNetFeat::conv5` + `PointNetFeat::bn5`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `cls_params1`: `PointNetCls::fc1` + `PointNetCls::bn1`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `cls_params2`: `PointNetCls::fc2` + `PointNetCls::bn2`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `cls_params3`: `PointNetCls::fc3`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```C++
void PointNetClsTop(const int op_mode,
                    const float* point_cloud,
                    const int num_points,
                    float* out_logits,
                    const float* feat_params1,
                    const float* feat_params2,
                    const float* feat_params3,
                    const float* feat_params4,
                    const float* feat_params5,
                    const float* cls_params1,
                    const float* cls_params2,
                    const float* cls_params3)
{
  // ...
}
```

## å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‡¦ç†

`torch.nn.Conv1d`ãŠã‚ˆã³`torch.nn.Linear`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ã¯ã€é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
`Conv1d`ã¨ã‚ã‚Šã¾ã™ãŒã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã¯1ãªã®ã§ã€`Linear`ã¨å‹•ä½œãŒåŒã˜ã«ãªã‚Šã¾ã™ã€‚
ä»¥å¾Œã€`Conv1d`ã¨`Linear`ã‚’åŒä¸€è¦–ã—ã¾ã™ã€‚
å…¥åŠ›ã¨å‡ºåŠ›ã®æ¬¡å…ƒæ•°ã‚’$\mathrm{InDims}$ã€$\mathrm{OutDims}$ã¨ã™ã‚‹ã¨ã€é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®ã‚µã‚¤ã‚ºã¯$(\mathrm{OutDims}, \mathrm{InDims})$ã€$(\mathrm{OutDims})$ã¨ãªã‚Šã¾ã™ã€‚
å…¥åŠ›$\boldsymbol{x} \in \mathbb{R}^{\mathrm{InDims}}$ã€é‡ã¿$\boldsymbol{W} \in \mathbb{R}^{\mathrm{OutDims} \times \mathrm{InDims}}$ã€ãƒã‚¤ã‚¢ã‚¹$\boldsymbol{b} \in \mathbb{R}^{\mathrm{OutDims}}$ãŒã‚ã‚‹ã¨ãã€å‡ºåŠ›$\boldsymbol{y} \in \mathbb{R}^{\mathrm{OutDims}}$ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
$$
  \boldsymbol{y} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}
$$

`torch.nn.BatchNorm1d`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ã¯ã€å¹³å‡ã€æ¨™æº–åå·®ã€é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã®4ã¤ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚
å…¥å‡ºåŠ›ã®æ¬¡å…ƒã‚’$\mathrm{Dims}$ã¨ã™ã‚‹ã¨ã€ã“ã‚Œã‚‰4ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¯$(\mathrm{Dims})$ã§ã™ã€‚
å¹³å‡ã€æ¨™æº–åå·®ã€é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹$\boldsymbol{\mu}, \boldsymbol{\sigma}, \boldsymbol{w}, \boldsymbol{b} \in \mathbb{R}^{\mathrm{Dims}}$ãŒã‚ã‚‹ã¨ãã€å…¥åŠ›$\boldsymbol{x} \in \mathbb{R}^{\mathrm{Dims}}$ã«å¯¾ã—ã¦å‡ºåŠ›$\boldsymbol{y} \in \mathbb{R}^{\mathrm{Dims}}$ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
$$
  y_i = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \varepsilon}} \cdot w_i + b_i \quad (i = 1, \ldots, \mathrm{Dims})
$$
$\varepsilon$ã¯ã€ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ããŸã‚ã®å°ã•ãªæ­£ã®å€¤ã§ã™ã€‚
$x_i$ã¯ã€$\boldsymbol{x}$ã®ç¬¬$i$è¦ç´ ã§ã™ (ä»–ã‚‚åŒæ§˜)ã€‚
ä¸Šè¨˜ã‚’ã¿ã‚‹ã¨ã€$w_i / \sqrt{\sigma_i^2 + \varepsilon}$ã®éƒ¨åˆ†ã‚’å…ˆã«è¨ˆç®—ã§ãã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
$\boldsymbol{w}$ã¨$\boldsymbol{\sigma}$ã®ä¸¡æ–¹ã‚’ä½¿ã†å ´åˆã¨æ¯”ã¹ã¦ã€é™¤ç®—ãŠã‚ˆã³å¹³æ–¹æ ¹ã®è¨ˆç®—ã‚’çœç•¥ã§ãã¾ã™ã€‚
ã¾ãŸã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚
ç´°ã‹ã„è©±ã«ã¿ãˆã¾ã™ãŒã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®å¤§ããªFPGAä¸Šã«å®Ÿè£…ã™ã‚‹å ´åˆã¯é‡è¦ã§ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨ˆç®—ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚
$$
  y_i = \left( x_i - \mu_i \right) \cdot s_i + b_i \quad (i = 1, \ldots, \mathrm{Dims})
$$
ä¸Šè¨˜ã®$s_i$ã‚’ã€ã“ã“ã§ã¯**ã‚¹ã‚±ãƒ¼ãƒ«**ã¨å‘¼ã¶ã“ã¨ã«ã—ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€å¹³å‡$\boldsymbol{\mu}$ã€ãƒã‚¤ã‚¢ã‚¹$\boldsymbol{b}$ã€ã‚¹ã‚±ãƒ¼ãƒ«$\boldsymbol{s} \in \mathbb{R}^{\mathrm{Dims}}$ã®3ã¤ã«ãªã‚Šã¾ã™ã€‚
$\boldsymbol{s}$ã®è¨ˆç®—ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–æ™‚ã«ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ä¸Šã§è¡Œã†ã“ã¨ã«ã—ã¾ã™ã€‚

ãƒãƒƒãƒæ­£è¦åŒ–ã®å¾Œã«ReLUæ´»æ€§åŒ–ãŒè¨ˆç®—ã•ã‚Œã¾ã™ã€‚
å„å±¤ã‚’åˆ¥ã€…ã«å®Ÿè£…ã™ã‚‹ã‚ˆã‚Šã‚‚ã€ã¾ã¨ã‚ã¦ã—ã¾ã£ãŸæ–¹ãŒåŠ¹ç‡ãŒã‚ˆã„ã®ã§ã€ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã‚’æ¬¡ã®ã‚ˆã†ã«ã¾ã¨ã‚ã¾ã™ (**æœ€é©åŒ–ãã®3: è¨ˆç®—ã®ç°¡ç•¥åŒ–**)ã€‚
$$
  y_i = \max \left( 0, \left( x_i - \mu_i \right) \cdot s_i + b_i \right) \quad (i = 1, \ldots, \mathrm{Dims})
$$

æœ€å¾Œã«Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã§ã™ãŒã€å…ˆè¿°ã®é€šã‚Šã€å„ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´é‡$\boldsymbol{\psi}_i \in \mathbb{R}^{1024}$ã¨ã€ç¾åœ¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡$\boldsymbol{\phi} \in \mathbb{R}^{1024}$ã¨ã®ã€è¦ç´ ã”ã¨ã®$\max$ã«ç½®ãæ›ãˆã¾ã—ãŸã€‚
Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã®è¨ˆç®—ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
$$
  \phi_i = \max \left( \phi_i, \psi_i \right) \quad (i = 1, \ldots, 1024)
$$

ã•ã¦ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®`LinearParams<T, InDims_, OutDims_>`æ§‹é€ ä½“ã¨ã€`BatchNorm1dParams<T, Dims_>`æ§‹é€ ä½“ã¯ã€å…¨çµåˆå±¤ (`Conv1d`ãŠã‚ˆã³`Linear`) ã¨ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ (`BatchNorm1d`) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã‚Œãã‚Œã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚

```C++
// Parameters for fully-connected layers
template <typename T, int InDims_, int OutDims_>
struct LinearParams
{
  enum
  {
    InDims = InDims_,
    OutDims = OutDims_,
  };

  T weight[OutDims][InDims];
  T bias[OutDims];
};

// Parameters for 1D batch normalization layers
template <typename T, int Dims_>
struct BatchNorm1dParams
{
  enum
  {
    Dims = Dims_,
  };

  // `scale` is obtained by multiplying weights and reciprocal of the
  // square root of the standard deviation (to reduce the computational cost)
  T scale[Dims];
  T bias[Dims];
  T mean[Dims];
};
```

`PointNetClsTop`å†…ã§ã¯ã€PyTorchã§å®šç¾©ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å„å±¤ã«å¯¾å¿œã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå®£è¨€ã•ã‚Œã¾ã™ã€‚

- `feat_conv1`: `PointNetFeat::conv1`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `feat_conv2`: `PointNetFeat::conv2`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `feat_conv3`: `PointNetFeat::conv3`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `feat_conv4`: `PointNetFeat::conv4`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `feat_conv5`: `PointNetFeat::conv5`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `feat_bn1`: `PointNetFeat::bn1`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `feat_bn2`: `PointNetFeat::bn2`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `feat_bn3`: `PointNetFeat::bn3`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `feat_bn4`: `PointNetFeat::bn4`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `feat_bn5`: `PointNetFeat::bn5`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `cls_fc3`: `PointNetCls::fc3`ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹
- `cls_bn1`: `PointNetCls::bn1`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«
- `cls_bn2`: `PointNetCls::bn2`ã®å¹³å‡ã€ãƒã‚¤ã‚¢ã‚¹ã€ã‚¹ã‚±ãƒ¼ãƒ«

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ã¦ã®å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€æ¨è«–ã‚’é–‹å§‹ã™ã‚‹å‰ã«äºˆã‚ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ç½®ã„ã¦ãŠãã¾ã™ã€‚
ä¸€æ–¹ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨çµåˆå±¤2ã¤ (`PointNetCls::fc1`ã€`PointNetCls::fc2`) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ã¯ç½®ã‹ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ããã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹ãŸã‚ã§ã™ã€‚
ã“ã‚Œã‚‰ã®å±¤ã«ã¤ã„ã¦ã¯ã€æ¨è«–æ™‚ã«DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å‡ºã—ã¾ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å–ã‚Šå‡ºã—ã¦ã€å‡ºåŠ›ã®ä¸€éƒ¨ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚
ä¸€éƒ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«ã€å°ã•ãªã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã‚’ç”¨æ„ã™ã‚Œã°ã‚ˆããªã‚Šã¾ã™ã€‚

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯ã€$N$å€‹å…¨ã¦ã®ç‚¹ã«å¯¾ã—ã¦ç‰¹å¾´æŠ½å‡ºã‚’è¡Œã†ãŸã‚ã«ã€$N$å›ã®é †ä¼æ’­ãŒèµ·ã“ã‚Šã¾ã™ã€‚
æ¨è«–æ™‚é–“ã®ãªã‹ã§å ã‚ã‚‹å‰²åˆãŒå¤§ãã„ã®ã§ã€1å›ã®é †ä¼æ’­ã«è¦ã™ã‚‹è¨ˆç®—æ™‚é–“ã‚’ã†ã¾ãçŸ­ç¸®ã§ãã‚Œã°ã€å…¨ä½“ã®æ¨è«–æ™‚é–“ã®å¤§å¹…ãªçŸ­ç¸®ã«ã¤ãªãŒã‚Šã¾ã™ (**ã‚¢ãƒ ãƒ€ãƒ¼ãƒ«ã®æ³•å‰‡**)ã€‚
ä¸€æ–¹ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é †ä¼æ’­ã¯1åº¦ã ã‘ã§ã€æ¨è«–æ™‚é–“ã®ãªã‹ã§ã¯ãã‚Œã»ã©é‡è¦ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã«äº‹å‰ã«æ ¼ç´ã™ã‚‹ã®ã¨æ¯”ã¹ã¦ã€æ¨è«–æ™‚ã«DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å‡ºã™ã¨ã€å±¤ã®è¨ˆç®—æ™‚é–“ã¯ä¼¸ã³ã¦ã—ã¾ã„ã¾ã™ãŒã€æ¨è«–æ™‚é–“ã«ä¸ãˆã‚‹å½±éŸ¿ã¯ãã‚Œã»ã©å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚

## ãƒ‡ãƒ¼ã‚¿å‹

Vitis HLSã§ã¯ã€ä»»æ„ç²¾åº¦ã®**å›ºå®š**å°æ•°ç‚¹æ•°å‹`ap_fixed`ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚
å˜ç²¾åº¦æµ®å‹•å°æ•°ç‚¹æ•°`float`ã‚„ã€åŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹æ•°`half`ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚
ã“ã“ã§ã¯ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«ã€å›ºå®šå°æ•°ç‚¹æ•°ã‚’ä½¿ã„ã¾ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰ (`ap_o_mode::AP_WRAP`) ã§ã¯ã€å€¤ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã—ãŸã¨ãã«æŠ˜ã‚Šè¿”ã—ã¾ã™ã€‚
ã“ã‚Œã ã¨ã€æœ€å¤§å€¤ã‹ã‚‰æ€¥ã«æœ€å°å€¤ã«ãªã£ãŸã‚Šã—ã¦å±ãªã£ã‹ã—ã„ã®ã§ã€æœ€å¤§å€¤ã‚ã‚‹ã„ã¯æœ€å°å€¤ã«ç•™ã¾ã‚Šç¶šã‘ã‚‹ã‚ˆã†ã«ã€é£½å’Œãƒ¢ãƒ¼ãƒ‰ (`ap_o_mode::AP_SAT`) ã«å¤‰æ›´ã—ã¦ã„ã¾ã™ã€‚
é£½å’Œãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã†å›ºå®šå°æ•°ç‚¹æ•°å‹ã‚’ã€`ap_fixed_sat`ã¨ã—ã¦å®šç¾©ã—ã¾ã—ãŸã€‚

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å…¥å‡ºåŠ›ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã§ãƒ“ãƒƒãƒˆå¹…ã‚’å¤‰ãˆã‚‹ãŸã‚ã«ã€å…¥å‡ºåŠ›ç”¨ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã«åˆ¥ã€…ã®å‹ã‚’ç”¨æ„ã—ã¾ã—ãŸ (`param_t`ãŠã‚ˆã³`value_t`)ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤åŸŸã«åˆã‚ã›ã¦ã€ãƒ“ãƒƒãƒˆå¹…ã‚’å‰Šæ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
ãƒ“ãƒƒãƒˆå¹…ã®å‰Šæ¸›ã‚„é‡å­åŒ–ã€å°æ•°ç‚¹å‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãªã©ã¯ã€ãã‚Œè‡ªä½“ãŒç«‹æ´¾ãªç ”ç©¶åˆ†é‡ã¨ãªã£ã¦ã„ã¾ã™ã€‚

```C++
// Value types
template <int _AP_W, int _AP_I>
using ap_fixed_sat = ap_fixed<
  _AP_W, _AP_I, ap_q_mode::AP_TRN, ap_o_mode::AP_SAT, 0>;

// Data type for values (layer inputs, outputs, and intermediate results)
using value_t = ap_fixed_sat<kValueBitWidth, kValueIntWidth>;
// Data type for network parameters
using param_t = ap_fixed_sat<kParamBitWidth, kParamIntWidth>;
```

## å‹•ä½œãƒ¢ãƒ¼ãƒ‰

ã•ã¦ã€ã“ã“ã§ç¤ºã™IPã‚³ã‚¢ã«ã¯ã€2ã¤ã®**å‹•ä½œãƒ¢ãƒ¼ãƒ‰** (Operation mode) ãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚

- é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ (`kModeInitWeights`): é‡ã¿ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã£ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã™ã‚‹ã€‚
- æ¨è«–ãƒ¢ãƒ¼ãƒ‰ (`kModeInference`): å…¥åŠ›ç‚¹ç¾¤ã‹ã‚‰ã€å„ã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹ã€‚

ã“ã‚Œã‚‰ã‚’é †ã«èª¬æ˜ã—ã¾ã™ã€‚

### é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã£ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã—ã¾ã™ã€‚
ä»¥ä¸‹ã«ç¤ºã™ã€`InitializeFeatNaive`ãŠã‚ˆã³`InitializeClsNaive`ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãã‚Œãã‚Œã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãŸã‚ã®é–¢æ•°ã§ã™ã€‚

```C++
// Naive implementation of the parameter initialization
// `T` is the type for parameters
template <typename T>
void InitializeFeatNaive(LinearParams<T, kFeatDims0, kFeatDims1>* conv1,
                         LinearParams<T, kFeatDims1, kFeatDims2>* conv2,
                         LinearParams<T, kFeatDims2, kFeatDims3>* conv3,
                         LinearParams<T, kFeatDims3, kFeatDims4>* conv4,
                         LinearParams<T, kFeatDims4, kFeatDims5>* conv5,
                         BatchNorm1dParams<T, kFeatDims1>* bn1,
                         BatchNorm1dParams<T, kFeatDims2>* bn2,
                         BatchNorm1dParams<T, kFeatDims3>* bn3,
                         BatchNorm1dParams<T, kFeatDims4>* bn4,
                         BatchNorm1dParams<T, kFeatDims5>* bn5,
                         const float* params1,
                         const float* params2,
                         const float* params3,
                         const float* params4,
                         const float* params5)
{
#pragma HLS INLINE off

  ReadBlockParamsNaive<T, kFeatDims0, kFeatDims1>(conv1, bn1, params1);
  ReadBlockParamsNaive<T, kFeatDims1, kFeatDims2>(conv2, bn2, params2);
  ReadBlockParamsNaive<T, kFeatDims2, kFeatDims3>(conv3, bn3, params3);
  ReadBlockParamsNaive<T, kFeatDims3, kFeatDims4>(conv4, bn4, params4);
  ReadBlockParamsNaive<T, kFeatDims4, kFeatDims5>(conv5, bn5, params5);
}

// Naive implementation of the parameter initialization
// `T` is the type for parameters
template <typename T>
void InitializeClsNaive(LinearParams<T, kClsDims2, kClsDims3>* fc3,
                        BatchNorm1dParams<T, kClsDims1>* bn1,
                        BatchNorm1dParams<T, kClsDims2>* bn2,
                        const float* params1,
                        const float* params2,
                        const float* params3)
{
#pragma HLS INLINE off

  ReadBatchNorm1dParamsNaive<T, kClsDims1>(
    bn1, params1, kClsDims0 * kClsDims1 + kClsDims1);
  ReadBatchNorm1dParamsNaive<T, kClsDims2>(
    bn2, params2, kClsDims1 * kClsDims2 + kClsDims2);
  ReadLinearParamsNaive<T, kClsDims2, kClsDims3>(
    fc3, params3, 0);
}
```

ã“ã‚Œã‚‰ã®é–¢æ•°ã®ãªã‹ã§ã¯ã€`ReadBlockParamsNaive`ã€`ReadLinearParamsNaive`ã€ãã—ã¦`ReadBatchNorm1dParamsNaive`ã®3ã¤ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™ã€‚
å„é–¢æ•°ã¯æ¬¡ã®ã‚ˆã†ãªå‹•ä½œã§ã™ (è©³ç´°ã¯ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ã”å‚ç…§ãã ã•ã„)ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã«ã¯`float`å‹ã§ç½®ã‹ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚’å›ºå®šå°æ•°ç‚¹æ•°å‹ã«ç›´ã™å‡¦ç†ã‚‚å«ã¾ã‚Œã¾ã™ã€‚

- `ReadLinearParamsNaive<T, InDims, OutDims>`: DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€å…¨çµåˆå±¤ (`Conv1d`ãŠã‚ˆã³`Linear`) ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’èª­ã¿å–ã‚‹ã€‚
é‡ã¿ã®ã‚µã‚¤ã‚ºã¯`(OutDims, InDims)`ã€ãƒã‚¤ã‚¢ã‚¹ã®ã‚µã‚¤ã‚ºã¯`(OutDims)`ã§ã‚ã‚‹ã€‚
2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ (é…åˆ—ã®ã‚µã‚¤ã‚ºã¯`OutDims * InDims + OutDims`)ã€‚
- `ReadBatchNorm1dParamsNaive<T, Dims>`: DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ (`BatchNorm1d`) ã®ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã‚’èª­ã¿å–ã‚‹ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¯`(Dims)`ã§ã‚ã‚‹ã€‚
3ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ (é…åˆ—ã®ã‚µã‚¤ã‚ºã¯`3 * Dims`)ã€‚
- `ReadBlockParamsNaive<T, InDims, OutDims`: DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã€å…¨çµåˆå±¤ãŠã‚ˆã³ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿5ã¤ã‚’èª­ã¿å–ã‚‹ã€‚
5ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€1æ¬¡å…ƒã®é…åˆ—ã¨ã—ã¦é€£çµã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ (é…åˆ—ã®ã‚µã‚¤ã‚ºã¯`OutDims * InDims + 4 * OutDims`)ã€‚

### æ¨è«–ãƒ¢ãƒ¼ãƒ‰

å…¥åŠ›ç‚¹ç¾¤ã‹ã‚‰ã€å„ã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ä»¥ä¸‹ã«ç¤ºã™ã€`InferenceFeatNaive`ãŠã‚ˆã³`InferenceClsNaive`ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãã‚Œãã‚Œã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡¦ç†ã§ã™ã€‚

```C++
// Naive implementation of the PointNet feature extraction
// `T` is the type for layer input, output, and intermediate results
// `U` is the type for parameters
// `N` is the expected number of input points (e.g., 1024)
template <typename T, typename U, int N>
void InferenceFeatNaive(const float* point_cloud,
                        const int num_points,
                        T feature[kFeatDims5],
                        const LinearParams<U, kFeatDims0, kFeatDims1>* conv1,
                        const LinearParams<U, kFeatDims1, kFeatDims2>* conv2,
                        const LinearParams<U, kFeatDims2, kFeatDims3>* conv3,
                        const LinearParams<U, kFeatDims3, kFeatDims4>* conv4,
                        const LinearParams<U, kFeatDims4, kFeatDims5>* conv5,
                        const BatchNorm1dParams<U, kFeatDims1>* bn1,
                        const BatchNorm1dParams<U, kFeatDims2>* bn2,
                        const BatchNorm1dParams<U, kFeatDims3>* bn3,
                        const BatchNorm1dParams<U, kFeatDims4>* bn4,
                        const BatchNorm1dParams<U, kFeatDims5>* bn5)
{
#pragma HLS INLINE off

  // Zero-initialize the output feature
  VectorNdSetZero<T, kFeatDims5>(feature);

  // Compute the feature
  for (int i = 0; i < num_points; ++i) {
#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N
#pragma HLS LOOP_FLATTEN off

    // Input, output, and intermediate results
    T x0[kFeatDims0];
    T x1[kFeatDims1];
    T x2[kFeatDims1];
    T x3[kFeatDims2];
    T x4[kFeatDims2];
    T x5[kFeatDims3];
    T x6[kFeatDims3];
    T x7[kFeatDims4];
    T x8[kFeatDims4];
    T x9[kFeatDims5];
    T x10[kFeatDims5];

    // Read a point from a DDR memory
    ReadPointNaive<T>(point_cloud, i, x0);

    // Compute a point feature
    LinearNaive<T, U, kFeatDims0, kFeatDims1, false>(
      x0, x1, conv1->weight, conv1->bias);
    BatchNorm1dReLUNaive<T, U, kFeatDims1>(
      x1, x2, bn1->scale, bn1->bias, bn1->mean);
    LinearNaive<T, U, kFeatDims1, kFeatDims2, false>(
      x2, x3, conv2->weight, conv2->bias);
    BatchNorm1dReLUNaive<T, U, kFeatDims2>(
      x3, x4, bn2->scale, bn2->bias, bn2->mean);
    LinearNaive<T, U, kFeatDims2, kFeatDims3, false>(
      x4, x5, conv3->weight, conv3->bias);
    BatchNorm1dReLUNaive<T, U, kFeatDims3>(
      x5, x6, bn3->scale, bn3->bias, bn3->mean);
    LinearNaive<T, U, kFeatDims3, kFeatDims4, false>(
      x6, x7, conv4->weight, conv4->bias);
    BatchNorm1dReLUNaive<T, U, kFeatDims4>(
      x7, x8, bn4->scale, bn4->bias, bn4->mean);
    LinearNaive<T, U, kFeatDims4, kFeatDims5, false>(
      x8, x9, conv5->weight, conv5->bias);
    BatchNorm1dReLUNaive<T, U, kFeatDims5>(
      x9, x10, bn5->scale, bn5->bias, bn5->mean);

    // Update the output feature
    MaxPool1dNaive<T, kFeatDims5>(x10, feature);
  }
}

// Naive implementation of the classification network
// `T` is the type for layer input, output, and intermediate results
// `U` is the type for parameters
template <typename T, typename U>
void InferenceClsNaive(const T feature[kFeatDims5],
                       float* out_logits,
                       const LinearParams<U, kClsDims2, kClsDims3>* fc3,
                       const BatchNorm1dParams<U, kClsDims1>* bn1,
                       const BatchNorm1dParams<U, kClsDims2>* bn2,
                       const float* params1,
                       const float* params2,
                       const float* params3)
{
#pragma HLS INLINE off

  static_assert(kFeatDims5 == kClsDims0,
                "Feature dimension should be equal to the input dimension");

  // Input, output, and intermediate results
  T x0[kClsDims1];
  T x1[kClsDims1];
  T x2[kClsDims2];
  T x3[kClsDims2];
  T x4[kClsDims3];

  // Compute logits
  LinearNaiveDDR<T, U, kClsDims0, kClsDims1, false>(
    feature, x0, params1, 0);
  BatchNorm1dReLUNaive<T, U, kClsDims1>(
    x0, x1, bn1->scale, bn1->bias, bn1->mean);
  LinearNaiveDDR<T, U, kClsDims1, kClsDims2, false>(
    x1, x2, params2, 0);
  BatchNorm1dReLUNaive<T, U, kClsDims2>(
    x2, x3, bn2->scale, bn2->bias, bn2->mean);
  LinearNaive<T, U, kClsDims2, kClsDims3, false>(
    x3, x4, fc3->weight, fc3->bias);

  // Write the result
  WriteTensor1dNaive<T, kClsDims3>(out_logits, x4, 0);
}
```

`InferenceFeatNaive`ã§ã¯ã€DRAMã«ç½®ã‹ã‚ŒãŸç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ (`point_cloud`) ã‹ã‚‰ã€1ã¤ãšã¤ç‚¹ã‚’èª­ã¿å–ã‚Šã¾ã™ã€‚
å„ç‚¹ (`x0`) ã«å¯¾ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ãªç‰¹å¾´é‡ (`x10`) ã‚’è¨ˆç®—ã—ã€ç¾åœ¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡ (`feature`) ã‚’æ›´æ–°ã™ã‚‹å‡¦ç†ã‚’ã€ç‚¹ã®å€‹æ•° (`num_points`) ã ã‘ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚
`InferenceClsNaive`ã¯ã€ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡ (`feature`) ã‚’å—ã‘å–ã£ã¦ã€å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ (`x4`) ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’DRAMãƒãƒƒãƒ•ã‚¡ (`out_logits`) ã«æ›¸ãæˆ»ã—ã¾ã™ã€‚

`ReadPointNaive`ã¯ã€$i$ç•ªç›®ã®ç‚¹$\boldsymbol{p}_i$ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã‚‹ã‚‚ã®ã§ã™ã€‚
`LinearNaive`ã€`BatchNorm1dReLUNaive`ã€`MaxPool1dNaive`ã¯ã€åå‰ã®é€šã‚Šã€å…¨çµåˆå±¤ (`Conv1d`)ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨ReLUæ´»æ€§åŒ–ã€Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«å¯¾å¿œã—ã¾ã™ (å…ˆç¨‹ã®è¨ˆç®—å¼ã‚’å‚ç…§)ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å‡ºã—ã¦ã€å±¤ã®å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
`LinearNaiveDDR`ã‚‚å…¨çµåˆå±¤ã®é–¢æ•°ã§ã™ãŒã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°‘ã—ãšã¤å–ã‚Šå‡ºã—ã¤ã¤ã€å‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®é–¢æ•°ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚
HLSãƒ—ãƒ©ã‚°ãƒã‚’é™¤ã‘ã°ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å®Ÿè£…ã¨å¤§ä½“åŒã˜ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
è¡Œæ•°ã¯å¤šã„ã§ã™ãŒã€å‡¦ç†å†…å®¹ã¯å˜ç´”ã§ã™ã€‚

```C++
// Naive implementation of the fully-connected layer
// `T` is the type for values
// `TParam` is the type for weight and bias
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
// `ApplyReLU` is the flag to apply ReLU activation
template <typename T, typename TParam,
          int InDims, int OutDims, bool ApplyReLU>
void LinearNaive(const T x[InDims],
                 T y[OutDims],
                 const TParam weight[OutDims][InDims],
                 const TParam bias[OutDims])
{
#pragma HLS INLINE off

  for (int i = 0; i < OutDims; ++i) {
#pragma HLS PIPELINE off
    T val = bias[i];

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      val += x[j] * weight[i][j];
    }

    if (ApplyReLU)
      y[i] = val > T(0) ? val : T(0);
    else
      y[i] = val;
  }
}

// Naive implementation of the fully-connected layer
// Weight and bias parameters are stored on the DDR memory
template <typename T, typename TParam,
          int InDims, int OutDims, bool ApplyReLU>
void LinearNaiveDDR(const T x[InDims],
                    T y[OutDims],
                    const float* params,
                    const int offset)
{
  // `params` contains weight parameters of size (`OutDims`, `InDims`) and
  // bias parameters of size (`OutDims`) in a contiguous buffer

#pragma HLS INLINE off

  constexpr const int OffsetToBias = OutDims * InDims;

  TParam bias[OutDims];

  // Copy the bias parameters in advance
  for (int i = 0; i < OutDims; ++i) {
#pragma HLS PIPELINE II=1
    bias[i] = TParam(params[offset + OffsetToBias + i]);
  }

  for (int i = 0; i < OutDims; ++i) {
#pragma HLS PIPELINE off
    T val = bias[i];

    TParam weight[InDims];

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE II=1
      weight[j] = TParam(params[offset + i * InDims + j]);
    }

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      val += x[j] * weight[j];
    }

    if (ApplyReLU)
      y[i] = val > T(0) ? val : T(0);
    else
      y[i] = val;
  }
}

// Naive implementation of the 1D batch normalization and ReLU activation
// `T` is the type for values
// `TParam` is the type for parameters
// `Dims` is the number of input and output dimensions
template <typename T, typename TParam, int Dims>
void BatchNorm1dReLUNaive(const T x[Dims],
                          T y[Dims],
                          const TParam scale[Dims],
                          const TParam bias[Dims],
                          const TParam mean[Dims])
{
#pragma HLS INLINE off

  for (int i = 0; i < Dims; ++i) {
#pragma HLS PIPELINE
    // Batch normalization with the learned parameters
    T val = (x[i] - mean[i]) * scale[i] + bias[i];
    // ReLU activation
    y[i] = val > T(0) ? val : T(0);
  }
}

// Naive implementation of the 1D max-pooling layer
// `T` is the type for values
// `Dims` is the number of input and output dimensions
// `y` must be properly initialized
template <typename T, int Dims>
void MaxPool1dNaive(const T x[Dims], T y[Dims])
{
  // `x` is of size (1, `Dims`)
  // `y` is of size (1, `Dims`)

#pragma HLS INLINE off

  for (int i = 0; i < Dims; ++i) {
#pragma HLS PIPELINE
    y[i] = x[i] > y[i] ? x[i] : y[i];
  }
}
```

`LinearNaiveDDR`ã§ã¯ã€å…¨çµåˆå±¤ã®ãƒã‚¤ã‚¢ã‚¹é … `bias`ã¨ã€å‡ºåŠ›1è¦ç´ åˆ†ã®è¨ˆç®—ã«å¿…è¦ãªé‡ã¿ `weight`ã ã‘ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªä¸Šã«ä¿æŒã—ã¾ã™ã€‚
å…¥å‡ºåŠ›ã®æ¬¡å…ƒã‚’$\mathrm{InDims}, \mathrm{OutDims}$ã¨ã™ã‚Œã°ã€`bias`ã®ã‚µã‚¤ã‚ºã¯$\mathrm{OutDims}$ã€`weight`ã®ã‚µã‚¤ã‚ºã¯$\mathrm{InDims}$ã¨ãªã‚Šã¾ã™ã€‚

ä¸Šè¨˜ã®é–¢æ•°ã®ãƒ«ãƒ¼ãƒ—ã«ã¯`#pragma HLS PIPELINE`ãŒä»˜åŠ ã•ã‚Œã¦ãŠã‚Šã€ãƒ«ãƒ¼ãƒ—å†…éƒ¨ã®å‡¦ç†ãŒè‡ªå‹•çš„ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã•ã‚Œã¾ã™ (**æœ€é©åŒ–ãã®4: ãƒ«ãƒ¼ãƒ—ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–**)ã€‚
`#pragma HLS PIPELINE off`ã¨ã™ã‚‹ã¨ã€ã“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ãŒæŠ‘åˆ¶ã•ã‚Œã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’ã€ä»¥ä¸‹ã®å›³ã«ç¤ºã—ã¾ã™ã€‚

[<img src="point-cloud-classification-images/pipelined-execution.svg" width="70%" />](point-cloud-classification-images/pipelined-execution.svg)

ãƒ«ãƒ¼ãƒ—ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ãªã„å ´åˆã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é †ã«å®Ÿè¡Œã—ã¾ã™ (å›³ã®ä¸Šéƒ¨)ã€‚
ä¸€æ–¹ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã§ã¯ã€ãƒ«ãƒ¼ãƒ—å†…éƒ¨ã®å‡¦ç†ã‚’åˆ†å‰² (å›³ã®å ´åˆã¯4åˆ†å‰²) ã—ã€ãã‚Œãã‚Œã®å‡¦ç†ã‚’æ™‚é–“çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¾ã™ (å›³ã®ä¸‹éƒ¨)ã€‚
è¤‡æ•°ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ã®ã§ã€ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã¯ã€æœ€ã‚‚æ™‚é–“ã®æ›ã‹ã‚‹å‡¦ç† (å›³ã®å ´åˆã¯å‡¦ç†3) ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã¾ã™ã€‚
ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å‡¦ç†ã‚’ã€ãªã‚‹ã¹ãå‡ç­‰ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã®åŠ¹æœãŒå¢—ã—ã¾ã™ã€‚
ä¸Šè¨˜ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«ã€æœ€å†…ãƒ«ãƒ¼ãƒ—ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã™ã‚‹ã¨ã€å‡¦ç†æ™‚é–“ã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ã€‚
2é‡ãƒ«ãƒ¼ãƒ—ã®ã†ã¡å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã™ã‚‹ã¨ã€å†…å´ã®ãƒ«ãƒ¼ãƒ—ã¯å…¨ã¦å±•é–‹ã•ã‚Œã¦ã€1é‡ãƒ«ãƒ¼ãƒ—ã«ç›´ã•ã‚Œã‚‹ã®ã§ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ãŒå¤§å¹…ã«å¢—ãˆã¦ã—ã¾ã„ã¾ã™ã€‚
å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã«ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã‚’é©ç”¨ã—ãªã„æ–¹ãŒã„ã„ã¨æ€ã„ã¾ã™ã€‚

ä¸Šè¨˜ã®IPã‚³ã‚¢ã¯ã€`hls/src/top_naive.cpp`ã«ã‚ã‚Šã¾ã™ã€‚

## ä¸¦åˆ—åŒ– (ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã®æ´»ç”¨)

ã“ã®IPã‚³ã‚¢ã‚‚æ­£ã—ãå‹•ä½œã™ã‚‹ã®ã§ã™ãŒã€æ˜ã‚‰ã‹ã«ãƒŠã‚¤ãƒ¼ãƒ–ãª (å…¨ãå·¥å¤«ã—ã¦ã„ãªã„ç´ æœ´ãª) å®Ÿè£…ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (Data parallelism) ã‚’æ´»ã‹ã—ã¦ã€å„å±¤ã®è¨ˆç®—ã‚’ä¸¦åˆ—åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã† (**æœ€é©åŒ–ãã®5: ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§**)ã€‚

å…¨çµåˆå±¤ã®è¨ˆç®—ã‚’ã‚‚ã†ä¸€åº¦ã¿ã¦ã¿ã¾ã™ã€‚
$$
  \boldsymbol{y} = \boldsymbol{W} \boldsymbol{x} + \boldsymbol{b}
$$
å‡ºåŠ›$\boldsymbol{y}$ã®å„è¦ç´ $y_i$ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
$$
  y_i = \sum_j W_{i, j} x_j + b_i
$$
$B$å€‹ã®å‡ºåŠ›è¦ç´ $y_i, y_{i + 1}, \ldots, y_{i + B - 1}$ã®é–“ã«ã¯ä¾å­˜ãŒãªã„ã®ã§ (ãã‚Œãã‚Œã®è¦ç´ ã¯äº’ã„ã«ä¾å­˜ã›ãšç‹¬ç«‹ã«è¨ˆç®—ã§ãã‚‹ã®ã§)ã€ä¸¦åˆ—ã«è¨ˆç®—ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
$$
  \begin{eqnarray}
    y_i &=& \sum_j W_{i, j} x_j + b_i \\
    y_{i + 1} &=& \sum_j W_{i + 1, j} x_j + b_{i + 1} \\
    &\vdots& \\
    y_{i + B - 1} &=& \sum_j W_{i + B - 1, j} x_j + b_{i + B - 1}
  \end{eqnarray}
$$
$W_{i, j} x_j, W_{i + 1, j} x_j, \ldots, W_{i + B - 1, j} x_j$ã®$B$å€‹ã®ç©ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã‚ã‘ã§ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€$j$ (å…¥åŠ›æ¬¡å…ƒ) ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã¯ãã®ã¾ã¾ã«ã—ã¦ã€$i$ (å‡ºåŠ›æ¬¡å…ƒ) ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
$B$å€‹ã®å‡ºåŠ›ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ã®ã§ã€$B$å€ã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™ (ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚‚$B$å€ã«ãªã‚Šã¾ã™)ã€‚

ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã«ã€è¤‡æ•°ã®å‡ºåŠ›è¦ç´ $y_i, y_{i + 1}, \ldots, y_{i + B - 1}$ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã—ã¾ã™ã€‚
$$
  \begin{eqnarray}
    y_i &=& \max \left( 0, \left( x_i - \mu_i \right) \cdot s_i + b_i \right) \\
    y_{i + 1} &=& \max \left( 0, \left( x_{i + 1} - \mu_{i + 1} \right) \cdot s_{i + 1} + b_{i + 1} \right) \\
    &\vdots& \\
    y_{i + B - 1} &=& \max \left( 0, \left( x_{i + B - 1} - \mu_{i + B - 1} \right) \cdot s_{i + B - 1} + b_{i + B - 1} \right)
  \end{eqnarray}
$$

Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã¤ã„ã¦ã‚‚å…¨ãåŒã˜ã§ã€è¤‡æ•°ã®å‡ºåŠ›è¦ç´ $\phi_i, \phi_{i + 1}, \ldots, \phi_{i + B - 1}$ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã—ã¾ã™ã€‚
$$
  \begin{eqnarray}
    \phi_i &=& \max \left( \phi_i, \psi_i \right) \\
    \phi_{i + 1} &=& \max \left( \phi_{i + 1}, \psi_{i + 1} \right) \\
    &\vdots& \\
    \phi_{i + B - 1} &=& \max \left( \phi_{i + B - 1}, \psi_{i + B - 1} \right)
  \end{eqnarray}
$$

`LinearNaive`ã€`LinearNaiveDDR`ã€`BatchNorm1dReLUNaive`ã€`MaxPool1dNaive`ãŒã€å„å±¤ã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ã—ãŸã€‚
ä¸¦åˆ—åŒ–ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ `LinearOpt1`ã€`LinearOpt1DDR`ã€`BatchNorm1dReLUOpt1`ã€`MaxPool1dOpt1`ã«ç½®ãæ›ãˆã¾ã™ (åå‰ã‚’`Naive`ã‹ã‚‰`Opt1`ã«ã—ã¾ã™)ã€‚
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¼•æ•°ã¨ã—ã¦`B`ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ (`B`ä¸¦åˆ—)ã€‚

```C++
// Parallel implementation of the fully-connected layer
// Matrix-vector multiplication is parallelized along the output dimension
// `T` is the type for values
// `TParam` is the type for weight and bias
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
// `ApplyReLU` is the flag to apply ReLU activation
// `B` is the block size for the output dimension
template <typename T, typename TParam,
          int InDims, int OutDims, bool ApplyReLU, int B>
void LinearOpt1(const T x[InDims],
                T y[OutDims],
                const TParam weight[OutDims][InDims],
                const TParam bias[OutDims])
{
#pragma HLS INLINE off

  // `OutDims` must be a multiple of `B`
  static_assert(OutDims % B == 0, "`OutDims` must be a multiple of `B`");

  for (int i0 = 0; i0 < OutDims; i0 += B) {
#pragma HLS PIPELINE off
    T vals[B];
#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
        int i = i0 + i1;
        T last = (j == 0) ? T(bias[i]) : vals[i1];
        vals[i1] = last + x[j] * weight[i][j];
      }
    }

    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      if (ApplyReLU)
        y[i] = vals[i1] > T(0) ? vals[i1] : T(0);
      else
        y[i] = vals[i1];
    }
  }
}

// Parallel implementation of the fully-connected layer
// Weight and bias parameters are stored on the DDR memory
// Matrix-vector multiplication is parallelized along the output dimension
template <typename T, typename TParam,
          int InDims, int OutDims, bool ApplyReLU, int B>
void LinearOpt1DDR(const T x[InDims],
                   T y[OutDims],
                   const float* params,
                   const int offset)
{
  // `params` contains weight parameters of size (`OutDims`, `InDims`) and
  // bias parameters of size (`OutDims`) in a contiguous buffer

#pragma HLS INLINE off

  // `OutDims` must be a multiple of `B`
  static_assert(OutDims % B == 0, "`OutDims` must be a multiple of `B`");
  // `B` must be larger than 1
  static_assert(B > 1, "`B` must be larger than 1");

  constexpr const int BHalf = B / 2;
  constexpr const int OffsetToBias = OutDims * InDims;

  TParam bias[OutDims];
#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1

  // Copy the bias parameters in advance
  for (int i = 0; i < OutDims; ++i) {
#pragma HLS PIPELINE II=1
    bias[i] = TParam(params[offset + OffsetToBias + i]);
  }

  for (int i0 = 0; i0 < OutDims; i0 += B) {
#pragma HLS PIPELINE off
    T vals[B];
#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1
    TParam weight[B][InDims];
#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1

    // Copy the weight parameters for `B` outputs
    const int offset0 = offset + i0 * InDims;
    for (int i1 = 0; i1 < B; ++i1) {
      for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE II=1
        weight[i1][j] = TParam(params[offset0 + i1 * InDims + j]);
      }
    }

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
        int i = i0 + i1;
        if (i < OutDims) {
          T last = (j == 0) ? T(bias[i]) : vals[i1];
          vals[i1] = last + x[j] * weight[i1][j];
        }
      }
    }

    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      if (i < OutDims) {
        if (ApplyReLU)
          y[i] = vals[i1] > T(0) ? vals[i1] : T(0);
        else
          y[i] = vals[i1];
      }
    }
  }
}

// Parallel implementation of the 1D batch normalization and ReLU activation
// `T` is the type for values
// `TParam` is the type for parameters
// `Dims` is the number of input and output dimensions
// `B` is the block size for the output dimension
template <typename T, typename TParam, int Dims, int B>
void BatchNorm1dReLUOpt1(const T x[Dims],
                         T y[Dims],
                         const TParam scale[Dims],
                         const TParam bias[Dims],
                         const TParam mean[Dims])
{
  // `scale` is the multiplication of the weight and reciprocal of the
  // standard deviation (to reduce the on-chip memory consumption)

#pragma HLS INLINE off

  static_assert(Dims % B == 0, "`Dims` must be a multiple of `B`");

  for (int i0 = 0; i0 < Dims; i0 += B) {
#pragma HLS PIPELINE
    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      // Batch normalization with the learned parameters
      T val = (x[i] - mean[i]) * scale[i] + bias[i];
      // ReLU activation
      y[i] = val > T(0) ? val : T(0);
    }
  }
}

// Parallel implementation of the 1D max-pooling layer
// `T` is the type for values
// `Dims` is the number of input and output dimensions
// `B` is the block size for the output dimension
// `y` must be properly initialized
template <typename T, int Dims, int B>
void MaxPool1dOpt1(const T x[Dims], T y[Dims])
{
#pragma HLS INLINE off

  static_assert(Dims % B == 0, "`Dims` must be a multiple of `B`");

  for (int i0 = 0; i0 < Dims; i0 += B) {
#pragma HLS PIPELINE
    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      y[i] = x[i] > y[i] ? x[i] : y[i];
    }
  }
}
```

`LinearOpt1`ã¨`LinearNaive`ã‚’æ¯”ã¹ã¦ã¿ã‚‹ã¨ã€`j` (å…¥åŠ›æ¬¡å…ƒ) ã®ãƒ«ãƒ¼ãƒ—ã¯ãã®ã¾ã¾ã§ã€`i` (å‡ºåŠ›æ¬¡å…ƒ) ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ãŒã€`i0`ã¨`i1`ã®2ã¤ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚
`i0`ã¯`B`åˆ»ã¿ã€`i1`ã¯`i0`ã‹ã‚‰`i0 + B - 1`ã¾ã§1ã¤ãšã¤å¢—ãˆã¦ã‚†ãã¾ã™ã€‚
`i1`ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚° (`#pragma HLS UNROLL`) ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ«ãƒ¼ãƒ—ã®ä¸­èº«ãŒå®Œå…¨ã«å±•é–‹ã•ã‚Œã¾ã™ã€‚
`i1`ã®ãƒ«ãƒ¼ãƒ—è‡ªä½“ã¯ç„¡ããªã£ã¦ã€`i0`ã‹ã‚‰`i0 + B - 1`ã¾ã§ã®å‡¦ç†ãŒä¸¦åˆ—ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
æœ€åˆã®ãƒ«ãƒ¼ãƒ—ã«æ³¨ç›®ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```C++
    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
        int i = i0 + i1;
        T last = (j == 0) ? T(bias[i]) : vals[i1];
        vals[i1] = last + x[j] * weight[i][j];
      }
    }
```

```C++
    for (int j = 0; j < InDims; ++j) {
  #pragma HLS PIPELINE
      T last0 = (j == 0) ? T(bias[i0 + 0]) : vals[0];
      T last1 = (j == 0) ? T(bias[i0 + 1]) : vals[1];
      // ...
      T lastB1 = (j == 0) ? T(bias[i0 + B - 1]) : vals[B - 1];

      vals[0] = last0 + x[j] * weight[i0 + 0][j];
      vals[1] = last1 + x[j] * weight[i0 + 1][j];
      // ...
      vals[B - 1] = lastB1 + x[j] * weight[i0 + B - 1][j];
    }
```

ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã«ã€`vals`ã¨ã„ã†ã€ã‚µã‚¤ã‚º`B`ã®ä¸€æ™‚é…åˆ—ã‚’æ–°ãŸã«ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚
ã“ã®é…åˆ—ã«ã¯ã€å‡ºåŠ›`y[i0]`ã‹ã‚‰`y[i0 + B - 1]`ã¾ã§ã®è¨ˆç®—çµæœã‚’ä¿æŒã—ã¾ã™ã€‚
`vals`ã®å„è¦ç´ ã¯ã€ãƒã‚¤ã‚¢ã‚¹é …`bias[i0]`ã‹ã‚‰`bias[i0 + B - 1]`ã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚
ãã®å¾Œã€`j`ã®ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã£ã¦ã€`x[j] * weight[i0][j]`ã‹ã‚‰`x[j] * weight[i0 + B - 1][j]`ãŒã€`vals`ã®å„è¦ç´ ã«é †ã«åŠ ç®—ã•ã‚Œã¾ã™ã€‚
ä¸Šè¨˜ã®è¨ˆç®—å¼ã¨å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

ãƒ«ãƒ¼ãƒ—ã‚’å±•é–‹ã™ã‚‹ã¨ã€`vals[0]`ã‹ã‚‰`vals[B - 1]`ã¾ã§ã®å…¨è¦ç´ ã€ãã‚Œã‹ã‚‰`bias[i0]`ã‹ã‚‰`bias[i0 + B - 1]`ã¾ã§ã€ãã—ã¦`weight[i0][j]`ã‹ã‚‰`weight[i0 + B - 1][j]`ã¾ã§ã®`B`å€‹ã®è¦ç´ ã«ã€1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ã€é…åˆ—`bias`ã€`vals`ã€`weight`ã®ãƒãƒ¼ãƒˆæ•°ã‚’`B`ä»¥ä¸Šã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

`vals`ã«ã¤ã„ã¦ã¯ã€`#pragma HLS ARRAY_PARTITION type=complete`ã‚’ä½¿ã£ã¦ã€é…åˆ—ã‚’å€‹ã€…ã®è¦ç´ ã«å®Œå…¨ã«åˆ†è§£ã—ã¦ã„ã¾ã™ã€‚
åˆ†å‰²ã—ãªã„å ´åˆã¯ãƒãƒ¼ãƒˆãŒ2ã¤ã—ã‹ãªã„ã®ã§ã€åŒæ™‚ã«2ã¤ã®è¦ç´ ã‚’èª­ã¿å‡ºã™ (ã‚ã‚‹ã„ã¯1è¦ç´ ã‚’èª­ã¿å‡ºã—ã¦ã€åˆ¥ã®1è¦ç´ ã¸æ›¸ãè¾¼ã‚€) ã“ã¨ã—ã‹ã§ãã¾ã›ã‚“ã€‚
å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€é…åˆ—ã®å…¨ã¦ã®è¦ç´ ã‚’åŒæ™‚ã«èª­ã¿æ›¸ãã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ãªãŠã€å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª (BlockRAM) ã§ã¯ãªãã€ãƒ•ãƒªãƒƒãƒ—ãƒ•ãƒ­ãƒƒãƒ— (FF) ã‚’ä½¿ã£ã¦é…åˆ—ãŒå®Ÿè£…ã•ã‚Œã¾ã™ã€‚

`B`å€‹ã®è¦ç´ ã‚’ã‚‚ã¤é…åˆ—`vals`ã‚’ã€å®Œå…¨ã«åˆ†å‰²ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

[<img src="point-cloud-classification-images/complete-partition.svg" width="50%" />](point-cloud-classification-images/complete-partition.svg)

`LinearOpt1`å†…ã«ã¯è¨˜è¿°ã•ã‚Œã¦ã„ã¾ã›ã‚“ãŒã€`weight`ã¨`bias`ã«ã¤ã„ã¦ã¯ã€åˆ¥ã®å ´æ‰€ã§ã€`vals`ã¨åŒæ§˜ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
`weight`ã¨`bias`ã‹ã‚‰ã€1ã‚µã‚¤ã‚¯ãƒ«ã§`B`å€‹ã®**é€£ç¶šã—ãŸ**è¦ç´  (`bias[i0]`ã‹ã‚‰`bias[i0 + B - 1]`ã¾ã§ã€ãã—ã¦`weight[i0][j]`ã‹ã‚‰`weight[i0 + B - 1][j]`ã¾ã§) ã‚’èª­ã¿å‡ºã™ãŸã‚ã«ã¯ã€æ¬¡ã®ã‚ˆã†ã«**ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²**ã—ã¾ã™ã€‚
`weight`ã¯2æ¬¡å…ƒé…åˆ—ã§ã™ãŒã€æœ€åˆã®æ¬¡å…ƒã«å¯¾ã—ã¦åˆ†å‰²ã—ãŸã„ã®ã§ã€`dim=1`ã‚’æŒ‡å®šã—ã¾ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒª (BlockRAM) 1ã¤ã«ã¤ããƒãƒ¼ãƒˆãŒ2ã¤ä»˜ã„ã¦ãŠã‚Šã€1ã‚µã‚¤ã‚¯ãƒ«ã§2è¦ç´ ã®èª­ã¿å‡ºã— (ã‚ã‚‹ã„ã¯1ã¤ã®æ›¸ãå‡ºã—ã¨1ã¤ã®èª­ã¿å‡ºã—) ãŒã§ãã¾ã™ã€‚
`B`å€‹ã®è¦ç´ ã‚’1ã‚µã‚¤ã‚¯ãƒ«ã§èª­ã¿å‡ºã™ãŸã‚ã«ã¯ã€é…åˆ—ã‚’`BHalf = B / 2`å€‹ã«åˆ†å‰²ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚
```C++
  constexpr const int BHalf = B / 2;
  TParam weight[OutDims][InDims];
#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1
  TParam bias[OutDims];
#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1
```

ç°¡å˜ãªä¾‹ã¨ã—ã¦ã€2æ¬¡å…ƒé…åˆ—`w[8][4]`ã‚’ã€æœ€åˆã®æ¬¡å…ƒã§4ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰² (`factor=4 dim=1`) ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
4åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒ8ã¤ã«å¢—ãˆã‚‹ã®ã§ã€8ã¤ã®é€£ç¶šã—ãŸè¦ç´  (ä¾‹ãˆã°`w[0][j]`ã‹ã‚‰`w[7][j]`ã¾ã§) ã‚’ã¾ã¨ã‚ã¦èª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã§ã¯ã€åˆ†å‰²ã•ã‚ŒãŸãã‚Œãã‚Œã®é…åˆ—ã«å¯¾ã—ã¦é †ã«ã€å…ˆé ­ã®è¦ç´ ã‹ã‚‰ (`w[0][0]`ã€`w[1][0]`ã€`w[2][0]`ã®é †ã«) è©°ã‚ã¦ã„ãã¾ã™ã€‚
å…¨ã¦ã®é…åˆ—ã«è¦ç´ ãŒå…¥ã£ãŸã‚‰ã€ã¾ãŸæœ€åˆã®é…åˆ—ã«æˆ»ã£ã¦ã€è¦ç´ ã‚’é †ã«è©°ã‚ã¦ã„ãã¾ã™ã€‚
ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨å›³ã®ã‚ˆã†ãªé…ç½®ã«ãªã‚Šã¾ã™ã€‚
é€£ç¶šã™ã‚‹è¦ç´  (`w[0][0]`ã€`w[1][0]`ã€`w[2][0]`ã€`w[3][0]`ãªã©) ãŒåˆ¥ã€…ã®é…åˆ—ã«æ ¼ç´ã•ã‚Œã‚‹ã®ã§ã€ã“ã‚Œã‚‰ã‚’ä¸€åº¦ã«å–ã‚Šå‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨ã€é…åˆ—ã®ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€é…åˆ—ã®é€£ç¶šã™ã‚‹è¦ç´ ã«å¯¾ã™ã‚‹ä¸¦åˆ—å‡¦ç†ã‚’ã€å®¹æ˜“ã«å®Ÿç¾ã§ãã¾ã™ã€‚
ã“ã®ã“ã¨ã‹ã‚‰ã€`#pragma HLS UNROLL`ã¨`#pragma HLS ARRAY_PARTITION`ã¯ã€ã‚»ãƒƒãƒˆã§ä½¿ã†å ´é¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã¨ã€é…åˆ—ã®åˆ†å‰²æ•°ã¯æƒãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ä¿‚æ•°`B`ã§ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‚‰ã€é…åˆ—ã¯`B / 2`å€‹ (`B`å€‹ã§ã‚‚ã‚ˆã„) ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã—ãªã„ã¨ã€`B`ä¸¦åˆ—ã«ãªã‚Šã¾ã›ã‚“ã€‚
ã¾ãŸã€ãƒ«ãƒ¼ãƒ—ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã®ã«ã€é…åˆ—ã‚’ä¸€åˆ‡åˆ†å‰²ã—ãªã‘ã‚Œã°ã€ä¸¦åˆ—å‡¦ç†ã«ãªã‚Šã¾ã›ã‚“ã€‚

[<img src="point-cloud-classification-images/cyclic-partition.svg" width="60%" />](point-cloud-classification-images/cyclic-partition.svg)

æœ€åˆã®æ¬¡å…ƒã§2ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰² (`factor=2 dim=1`) ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
2åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒ4ã¤ã«å¢—ãˆã‚‹ã®ã§ã€4ã¤ã®é€£ç¶šã—ãŸè¦ç´  (ä¾‹ãˆã°`w[0][j]`ã‹ã‚‰`w[3][j]`ã€ã‚ã‚‹ã„ã¯`w[4][j]`ã‹ã‚‰`w[7][j]`ã¾ã§) ã‚’ã¾ã¨ã‚ã¦èª­ã¿å‡ºã›ã¾ã™ã€‚

[<img src="point-cloud-classification-images/cyclic-partition3.svg" width="60%" />](point-cloud-classification-images/cyclic-partition3.svg)

2ç•ªç›®ã®æ¬¡å…ƒã§2ã¤ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰² (`factor=2 dim=2`) ã™ã‚Œã°ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
ä»Šåº¦ã¯ã€2ç•ªç›®ã®æ¬¡å…ƒã«ã¤ã„ã¦ã€4ã¤ã®é€£ç¶šã—ãŸè¦ç´  (ä¾‹ãˆã°`w[i][0]`ã‹ã‚‰`w[i][3]`ã¾ã§) ã«1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

[<img src="point-cloud-classification-images/cyclic-partition2.svg" width="50%" />](point-cloud-classification-images/cyclic-partition2.svg)

ã“ã‚Œã‚‰ã‚’è€ƒãˆã‚‹ã¨ã€`weight`ã¨`bias`ã«ã¤ã„ã¦ã¯ä¸Šè¨˜ã®ãƒ—ãƒ©ã‚°ãƒã‚’ä½¿ãˆã°ã‚ˆã„ã¨åˆ†ã‹ã‚Šã¾ã™ã€‚

ã•ã¦ã€2ã¤ç›®ã®ãƒ«ãƒ¼ãƒ—ã«æ³¨ç›®ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
1ã¤ç›®ã®ãƒ«ãƒ¼ãƒ—ã§è¨ˆç®—ã•ã‚ŒãŸ`B`å€‹ã®è¦ç´ ã‚’ã€å‡ºåŠ›`y`ã«æ›¸ãè¾¼ã‚€éƒ¨åˆ†ã§ã™ã€‚

```C++
    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      if (ApplyReLU)
        y[i] = vals[i1] > T(0) ? vals[i1] : T(0);
      else
        y[i] = vals[i1];
    }
```

ã“ã®ãƒ«ãƒ¼ãƒ—ã‚‚ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```C++
    if (ApplyReLU) {
      y[i0 + 0] = vals[0] > T(0) ? vals[0] : T(0);
      y[i0 + 1] = vals[1] > T(0) ? vals[1] : T(0);
      // ...
      y[i0 + B - 1] = vals[B - 1] > T(0) ? vals[B - 1] : T(0);
    } else {
      y[i0 + 0] = vals[0];
      y[i0 + 1] = vals[1];
      // ...
      y[i0 + B - 1] = vals[B - 1];
    }
```

å‡ºåŠ›`y[i0]`ã‹ã‚‰`y[i0 + B - 1]`ã¾ã§ã®ã€é€£ç¶šã™ã‚‹`B`å€‹ã®è¦ç´ ã«1ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
`LinearOpt1`å†…ã«ã¯è¨˜è¼‰ã•ã‚Œã¾ã›ã‚“ãŒã€é…åˆ—`y`ã‚‚ã€æ¬¡ã®ã‚ˆã†ã«ã‚µã‚¤ã‚¯ãƒªãƒƒã‚¯åˆ†å‰²ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚
```C++
  constexpr const int BHalf = B / 2;
  T y[OutDims];
#pragma HLS ARRAY_PARTITION variable=y type=cyclic factor=BHalf dim=1
```

ãªãŠã€å…¥åŠ›`x`ã«ã¤ã„ã¦ã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§1ã¤ã®è¦ç´ ã«ã—ã‹ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ãŸã‚ã€åˆ†å‰²ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
`LinearOpt1`ã‚’ä½¿ã£ã¦ã€å…¨çµåˆå±¤ã®å‡¦ç†ã‚’`B`ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã«ã¯ã€å¼•æ•°ã§ã‚ã‚‹é‡ã¿`weight`ã€ãƒã‚¤ã‚¢ã‚¹`bias`ã€å‡ºåŠ›`y`ã‚’ã€å‡ºåŠ›ã®æ¬¡å…ƒã§`B / 2`å€‹ã«åˆ†å‰²ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ (`B`ãŒ2ã§ã‚ã‚Œã°åˆ†å‰²ã®å¿…è¦ã¯ãªã„)ã€‚

ä»¥ä¸ŠãŒ`LinearOpt1`ã®ä¸»ãªå¤‰æ›´ç‚¹ã§ã™ã€‚
`LinearOpt1DDR`ã«ã¤ã„ã¦ã‚‚ã€`B`å€‹ã®å‡ºåŠ›ã‚’ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€åŒæ§˜ã®å¤‰æ›´ãŒãªã•ã‚Œã¦ã„ã¾ã™ã€‚
å…¨çµåˆå±¤ã®ãƒã‚¤ã‚¢ã‚¹é …`bias`ã¨ã€å‡ºåŠ›ã®`B`è¦ç´ åˆ†ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«å¿…è¦ãªé‡ã¿`weight`ã‚’ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ä¸Šã«è»¢é€ã—ã¦ã„ã¾ã™ã€‚
`LinearNaiveDDR`ã¨ã¯ç•°ãªã‚Šã€é‡ã¿ã‚’ä¿æŒã™ã‚‹ãƒãƒƒãƒ•ã‚¡`weight`ã¯ã€2æ¬¡å…ƒé…åˆ—ã¨ãªã£ã¦ã„ã¾ã™ã€‚
`B`å€‹ã®å¿…è¦ãªè¦ç´ ã‚’å–ã‚Šå‡ºã™ãŸã‚ã«ã€`bias`ã¨`weight`ã¯`BHalf = B / 2`å€‹ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚

`BatchNorm1dReLUOpt1`ã¨`MaxPool1dOpt1`ã«ã¤ã„ã¦ã‚‚ã€`i` (å‡ºåŠ›æ¬¡å…ƒ) ã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ—ãŒã€`i0`ã¨`i1`ã®2ã¤ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚
`i1`ã®ãƒ«ãƒ¼ãƒ—ã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã€`B`å€‹ã®å‡ºåŠ›ãŒä¸¦åˆ—ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
`BatchNorm1dReLUOpt1`ã‚’ä½¿ã£ã¦ã€ãƒãƒƒãƒæ­£è¦åŒ–ã¨ReLUæ´»æ€§åŒ–ã‚’`B`ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã«ã¯ã€é–¢æ•°ã®å…¥åŠ›`x`ã€å‡ºåŠ›`y`ã¨ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ã‚¹ã‚±ãƒ¼ãƒ«`scale`ã€ãƒã‚¤ã‚¢ã‚¹`bias`ã€å¹³å‡`mean`) ã‚’`B / 2`å€‹ã«åˆ†å‰²ã—ã¾ã™ã€‚
`MaxPool1dOpt1`ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã§ã€`B`ä¸¦åˆ—ã§Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã«ã€é–¢æ•°ã®å…¥åŠ›`x`ã¨`y`ã‚’`B / 2`å€‹ã«åˆ†å‰²ã—ã¾ã™ (`x`ã¯å„ç‚¹ã«å¯¾ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´é‡ã§ã€`y`ã¯ç‚¹ç¾¤å…¨ä½“ã‚’è¡¨ã™ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç‰¹å¾´é‡)ã€‚

å„å±¤ã‚’`B`ä¸¦åˆ—ã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã®ã€é…åˆ—ã®åˆ†å‰²ã®ãƒ«ãƒ¼ãƒ«ã‚’æ¬¡ã«ã¾ã¨ã‚ã¾ã™ã€‚
2ä¸¦åˆ—ã®å ´åˆã¯ã€åˆ†å‰²ã®å¿…è¦ãŒãªã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

- `LinearOpt1`: é‡ã¿`weight`ã€ãƒã‚¤ã‚¢ã‚¹`bias`ã€å‡ºåŠ›`y`ã‚’ã€å‡ºåŠ›ã®æ¬¡å…ƒã§`B / 2`å€‹ã«åˆ†å‰² (å…¥åŠ›`x`ã¯åˆ†å‰²ã®å¿…è¦ãªã—)
- `LinearOpt1DDR`: å‡ºåŠ›`y`ã‚’`B / 2`å€‹ã«åˆ†å‰² (å…¥åŠ›`x`ã¯åˆ†å‰²ã®å¿…è¦ãªã—)
- `BatchNorm1dReLUOpt1`: å…¥åŠ›`x`ã¨å‡ºåŠ›`y`ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ã‚¹ã‚±ãƒ¼ãƒ«`scale`ã€ãƒã‚¤ã‚¢ã‚¹`bias`ã€å¹³å‡`mean`) ã‚’ã€`B / 2`å€‹ã«åˆ†å‰²
- `MaxPool1dOpt1`: å…¥åŠ›`x`ã¨å‡ºåŠ›`y`ã‚’ã€`B / 2`å€‹ã«åˆ†å‰²

ã“ã‚Œã‚‰ã®ä¸¦åˆ—åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–å‡¦ç†ã‚’æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã¾ã™ã€‚
`InferenceFeatNaive`ã¨`InferenceClsNaive`ã‹ã‚‰ã€ãã‚Œãã‚Œ`InferenceFeatOpt1`ã¨`InferenceClsOpt1`ã«ãªã‚Šã¾ã™ã€‚
é–¢æ•°ã®å¼•æ•°ã¯å¤‰æ›´ã—ã¾ã›ã‚“ã€‚
ãªãŠã€`InitializeFeatNaive`ã¨`InitializeClsNaive` (é‡ã¿ã®åˆæœŸåŒ–é–¢æ•°) ã¯ã€ãã®ã¾ã¾ä½¿ã†ã“ã¨ã«ã—ã¾ã™ (é–¢æ•°åã ã‘ã€`InitializeFeatOpt1`ã€`InitializeClsOpt1`ã¨ã—ã¾ã—ãŸ)ã€‚

```C++
// Parallel implementation of the PointNet feature extraction
// `T` is the type for layer input, output, and intermediate results
// `U` is the type for parameters
// `N` is the expected number of input points (e.g., 1024)
template <typename T, typename U, int N>
void InferenceFeatOpt1(const float* point_cloud,
                       const int num_points,
                       T feature[kFeatDims5],
                       const LinearParams<U, kFeatDims0, kFeatDims1>* conv1,
                       const LinearParams<U, kFeatDims1, kFeatDims2>* conv2,
                       const LinearParams<U, kFeatDims2, kFeatDims3>* conv3,
                       const LinearParams<U, kFeatDims3, kFeatDims4>* conv4,
                       const LinearParams<U, kFeatDims4, kFeatDims5>* conv5,
                       const BatchNorm1dParams<U, kFeatDims1>* bn1,
                       const BatchNorm1dParams<U, kFeatDims2>* bn2,
                       const BatchNorm1dParams<U, kFeatDims3>* bn3,
                       const BatchNorm1dParams<U, kFeatDims4>* bn4,
                       const BatchNorm1dParams<U, kFeatDims5>* bn5)
{
#pragma HLS INLINE off

  // Zero-initialize the output feature
  VectorNdSetZero<T, kFeatDims5>(feature);

  // Compute the feature
  for (int i = 0; i < num_points; ++i) {
#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N
#pragma HLS LOOP_FLATTEN off

    // Input, output, and intermediate results
    T x0[kFeatDims0];
    T x1[kFeatDims1];
    T x2[kFeatDims1];
    T x3[kFeatDims2];
    T x4[kFeatDims2];
    T x5[kFeatDims3];
    T x6[kFeatDims3];
    T x7[kFeatDims4];
    T x8[kFeatDims4];
    T x9[kFeatDims5];
    T x10[kFeatDims5];

#pragma HLS ARRAY_PARTITION variable=x3 type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=x5 type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=x7 type=cyclic factor=8 dim=1
#pragma HLS ARRAY_PARTITION variable=x9 type=cyclic factor=64 dim=1

    // Read a point from a DDR memory
    ReadPointNaive<T>(point_cloud, i, x0);

    // Compute a point feature
    LinearOpt1<T, U, kFeatDims0, kFeatDims1, false, 2>(
      x0, x1, conv1->weight, conv1->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims1, 2>(
      x1, x2, bn1->scale, bn1->bias, bn1->mean);
    LinearOpt1<T, U, kFeatDims1, kFeatDims2, false, 8>(
      x2, x3, conv2->weight, conv2->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims2, 2>(
      x3, x4, bn2->scale, bn2->bias, bn2->mean);
    LinearOpt1<T, U, kFeatDims2, kFeatDims3, false, 8>(
      x4, x5, conv3->weight, conv3->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims3, 2>(
      x5, x6, bn3->scale, bn3->bias, bn3->mean);
    LinearOpt1<T, U, kFeatDims3, kFeatDims4, false, 16>(
      x6, x7, conv4->weight, conv4->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims4, 2>(
      x7, x8, bn4->scale, bn4->bias, bn4->mean);
    LinearOpt1<T, U, kFeatDims4, kFeatDims5, false, 128>(
      x8, x9, conv5->weight, conv5->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims5, 2>(
      x9, x10, bn5->scale, bn5->bias, bn5->mean);

    // Update the output feature
    MaxPool1dOpt1<T, kFeatDims5, 2>(x10, feature);
  }
}

// Parallel implementation of the classification network
// `T` is the type for layer input, output, and intermediate results
// `U` is the type for parameters
template <typename T, typename U>
void InferenceClsOpt1(const T feature[kFeatDims5],
                      float* out_logits,
                      const LinearParams<U, kClsDims2, kClsDims3>* fc3,
                      const BatchNorm1dParams<U, kClsDims1>* bn1,
                      const BatchNorm1dParams<U, kClsDims2>* bn2,
                      const float* params1,
                      const float* params2,
                      const float* params3)
{
#pragma HLS INLINE off

  static_assert(kFeatDims5 == kClsDims0,
                "Feature dimension should be equal to the input dimension");

  // Input, output, and intermediate results
  T x0[kClsDims1];
  T x1[kClsDims1];
  T x2[kClsDims2];
  T x3[kClsDims2];
  T x4[kClsDims3];

#pragma HLS ARRAY_PARTITION variable=x0 type=cyclic factor=8 dim=1
#pragma HLS ARRAY_PARTITION variable=x2 type=cyclic factor=4 dim=1

  // Compute logits
  LinearOpt1DDR<T, U, kClsDims0, kClsDims1, false, 16>(
    feature, x0, params1, 0);
  BatchNorm1dReLUOpt1<T, U, kClsDims1, 2>(
    x0, x1, bn1->scale, bn1->bias, bn1->mean);
  LinearOpt1DDR<T, U, kClsDims1, kClsDims2, false, 8>(
    x1, x2, params2, 0);
  BatchNorm1dReLUOpt1<T, U, kClsDims2, 2>(
    x2, x3, bn2->scale, bn2->bias, bn2->mean);
  LinearOpt1<T, U, kClsDims2, kClsDims3, false, 2>(
    x3, x4, fc3->weight, fc3->bias);

  // Write the result
  WriteTensor1dNaive<T, kClsDims3>(out_logits, x4, 0);
}
```

å„å±¤ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™éš›ã«ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¼•æ•°ã«ä¸¦åˆ—åŒ–åº¦ã‚‚æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®4ç•ªç›®ã®å…¨çµåˆå±¤ (PyTorchã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹`PointNetFeat::conv4`) ã¯16ä¸¦åˆ—ã€æœ€å¾Œã®å…¨çµåˆå±¤ (`PointNetFeat::conv5`) ã¯128ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
ä¸€æ–¹ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¯ã€2ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚
å„å±¤ã®ä¸¦åˆ—åº¦ã‚’ã©ã®ã‚ˆã†ã«æ±ºå®šã—ãŸã®ã‹ã«ã¤ã„ã¦ã¯ã€å¾Œè¿°ã—ã¾ã™ã€‚

ç¶šã„ã¦ã€IPã‚³ã‚¢ã®æœ€ä¸Šä½é–¢æ•°`PointNetClsTop`ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚

```C++
void PointNetClsTop(const int op_mode,
                    const float* point_cloud,
                    const int num_points,
                    float* out_logits,
                    const float* feat_params1,
                    const float* feat_params2,
                    const float* feat_params3,
                    const float* feat_params4,
                    const float* feat_params5,
                    const float* cls_params1,
                    const float* cls_params2,
                    const float* cls_params3)
{
#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params2 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params3 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params4 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params5 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params1 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params2 offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0

#pragma HLS INTERFACE s_axilite port=op_mode bundle=control
#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control
#pragma HLS INTERFACE s_axilite port=num_points bundle=control
#pragma HLS INTERFACE s_axilite port=out_logits bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params2 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params3 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params4 bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params5 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params1 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params2 bundle=control
#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control
#pragma HLS INTERFACE s_axilite port=return bundle=control

  // Parameters for feature extraction
  LinearParams<param_t, kFeatDims0, kFeatDims1> feat_conv1;
  LinearParams<param_t, kFeatDims1, kFeatDims2> feat_conv2;
  LinearParams<param_t, kFeatDims2, kFeatDims3> feat_conv3;
  LinearParams<param_t, kFeatDims3, kFeatDims4> feat_conv4;
  LinearParams<param_t, kFeatDims4, kFeatDims5> feat_conv5;
  BatchNorm1dParams<param_t, kFeatDims1> feat_bn1;
  BatchNorm1dParams<param_t, kFeatDims2> feat_bn2;
  BatchNorm1dParams<param_t, kFeatDims3> feat_bn3;
  BatchNorm1dParams<param_t, kFeatDims4> feat_bn4;
  BatchNorm1dParams<param_t, kFeatDims5> feat_bn5;

#pragma HLS ARRAY_PARTITION variable=feat_conv2.weight type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv2.bias type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv3.weight type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv3.bias type=cyclic factor=4 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv4.weight type=cyclic factor=8 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv4.bias type=cyclic factor=8 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv5.weight type=cyclic factor=64 dim=1
#pragma HLS ARRAY_PARTITION variable=feat_conv5.bias type=cyclic factor=64 dim=1

  // Parameters for classification network
  // LinearParams<param_t, kClsDims0, kClsDims1> cls_fc1;
  // LinearParams<param_t, kClsDims1, kClsDims2> cls_fc2;
  LinearParams<param_t, kClsDims2, kClsDims3> cls_fc3;
  BatchNorm1dParams<param_t, kClsDims1> cls_bn1;
  BatchNorm1dParams<param_t, kClsDims2> cls_bn2;

  // Extracted feature
  value_t feature[kFeatDims5];

  if (op_mode == kModeInitWeights) {
    // Initialize the PointNet feature extraction network
    InitializeFeatOpt1<param_t>(
      &feat_conv1, &feat_conv2, &feat_conv3, &feat_conv4, &feat_conv5,
      &feat_bn1, &feat_bn2, &feat_bn3, &feat_bn4, &feat_bn5,
      feat_params1, feat_params2, feat_params3, feat_params4, feat_params5);
    // Initialize the classification network
    InitializeClsOpt1<param_t>(
      &cls_fc3, &cls_bn1, &cls_bn2,
      cls_params1, cls_params2, cls_params3);
  } else if (op_mode == kModeInference) {
    // Run the PointNet feature extraction
    InferenceFeatOpt1<value_t, param_t, 1024>(
      point_cloud, num_points, feature,
      &feat_conv1, &feat_conv2, &feat_conv3, &feat_conv4, &feat_conv5,
      &feat_bn1, &feat_bn2, &feat_bn3, &feat_bn4, &feat_bn5);

    // Run the classification
    InferenceClsOpt1<value_t, param_t>(
      feature, out_logits,
      &cls_fc3, &cls_bn1, &cls_bn2,
      cls_params1, cls_params2, cls_params3);
  }
}
```

é–¢æ•°ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã¯å…¨ãåŒä¸€ã§ã™ã€‚
ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€å±¤ã®å…¥å‡ºåŠ›ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒãƒƒãƒ•ã‚¡ (`feat_conv5.weight`ã€`feat_conv5.bias`ã€`x3`ã€`x5`ãªã©) ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã«ã€`#pragma HLS ARRAY_PARTITION`ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
é…åˆ—ã®åˆ†å‰²æ•° (`factor`) ã«ã¤ã„ã¦ã¯ã€ä¸Šè¿°ã®ãƒ«ãƒ¼ãƒ«ã«å‰‡ã£ã¦ã„ã¾ã™ã€‚
ä¾‹ãˆã°ã€`InferenceFeatOpt1`ã¨`PointNetClsTop`ã‚’ã¿ã‚‹ã¨ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€å¾Œã®å…¨çµåˆå±¤ã‚’128ä¸¦åˆ—ã§å®Ÿè¡Œã—ãŸã„ã®ã§ã€å‡ºåŠ›ç”¨ã®ãƒãƒƒãƒ•ã‚¡`x10`ã¨ã€å…¨çµåˆå±¤ã®2ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿`feat_conv5.weight`ã€`feat_conv5.bias`ã‚’64åˆ†å‰²ã—ã¦ã„ã¾ã™ (è¨˜è¿°ã™ã‚‹å ´æ‰€ãŒæ•£ã‚‰ã°ã£ã¦ã„ã‚‹ã®ãŒé›£ç‚¹ã§ã™)ã€‚
åŒæ§˜ã«ã€`InferenceClsOpt1`ã¨`PointNetClsTop`ã‚’ã¿ã‚‹ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€åˆã®å…¨çµåˆå±¤ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§ã€å‡ºåŠ›ç”¨ã®ãƒãƒƒãƒ•ã‚¡`x0`ã¯8åˆ†å‰²ã—ã¦ã„ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã¨Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°ã¯2ä¸¦åˆ—ãªã®ã§ã€é…åˆ—ã‚’åˆ†å‰²ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

å…ˆè¿°ã®ã‚ˆã†ã«ã€é…åˆ—ã‚’åˆ†å‰²ã™ã‚‹ã¨ãƒãƒ¼ãƒˆæ•°ãŒå¢—ãˆã¦ã€ä¸€åº¦ã«å¤šãã®è¦ç´ ã‚’èª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ãŒã€è²´é‡ãªã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®æ¶ˆè²»ã‚‚å¢—ãˆã¾ã™ã€‚
ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®æ¶ˆè²»ã‚’æŠ‘ãˆã¤ã¤ã€ãªã‚‹ã¹ãä¸¦åˆ—åº¦ã‚’ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
æ¨è«–æ™‚é–“ã®çŸ­ç¸®ã«æœ€ã‚‚åŠ¹æœãŒã‚ã‚‹éƒ¨åˆ† (ä¾‹ãˆã°ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€å¾Œã®å…¨çµåˆå±¤) ã®ä¸¦åˆ—åº¦ã‚’ä¸Šã’ã¦ã€åŠ¹æœãŒã‚ã¾ã‚Šãªã„éƒ¨åˆ† (ä¾‹ãˆã°ãƒãƒƒãƒæ­£è¦åŒ–å±¤) ã®ä¸¦åˆ—åº¦ã¯ä¸‹ã’ã¦ã„ã¾ã™ã€‚

ã“ã“ã§ã€å„å±¤ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã™ (å‹•ä½œå‘¨æ³¢æ•°ã¯150MHz)ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

| å±¤ | `InferenceFeatNaive` | `InferenceFeatOpt1` |
| :-- | :-- | :-- |
| å…¨çµåˆå±¤1 (`PointNetFeat::conv1`) | 577 (3.843us) | 321 (2.138us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetFeat::bn1`) | 68 (0.453us) | 36 (0.240us) |
| å…¨çµåˆå±¤2 (`PointNetFeat::conv2`) | 4,481 (29.84us) | 569 (3.790us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetFeat::bn2`) | 68 (0.453us) | 36 (0.240us) |
| å…¨çµåˆå±¤3 (`PointNetFeat::conv3`) | 4,481 (29.84us) | 569 (3.790us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetFeat::bn3`) | 68 (0.453us) | 36 (0.240us) |
| å…¨çµåˆå±¤4 (`PointNetFeat::conv4`) | 8,961 (59.68us) | 569 (3.790us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetFeat::bn4`) | 132 (0.879us) | 68 (0.453us) |
| å…¨çµåˆå±¤5 (`PointNetFeat::conv5`) | 137,217 (914.0us) | 1,081 (7.199us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetFeat::bn5`) | 1,028 (6.846us) | 516 (3.437us) |
| Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ | 1,026 (6.833us) | 514 (3.423us) |
| å…¨ä½“ (1å›åˆ†) | 158,149 (1.053ms) | 4,357 (29.02us) |
| å…¨ä½“ (1024å›åˆ†) | 161,945,604 (1.079s) | 4,462,596 (29.72ms) |

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã€ã‚„ã¯ã‚Šæœ€å¾Œã®å…¨çµåˆå±¤ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ã„ã¾ã™ã€‚
128ä¸¦åˆ—ã«ã™ã‚‹ã“ã¨ã§ã€å®Ÿè¡Œæ™‚é–“ã‚’126.9å€ (137,217ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰1,081ã‚µã‚¤ã‚¯ãƒ«) å‰Šæ¸›ã§ãã¦ã„ã¾ã™ã€‚
4ã¤ç›®ã®å…¨çµåˆå±¤ã«ã¤ã„ã¦ã‚‚ã€16ä¸¦åˆ—ã«ã™ã‚‹ã“ã¨ã§ã€å®Ÿè¡Œæ™‚é–“ãŒ15.75å€ (8,961ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰569ã‚µã‚¤ã‚¯ãƒ«) çŸ­ããªã‚Šã¾ã—ãŸã€‚
å…¨çµåˆå±¤ã‚„ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã€Maxãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã«ã¿ã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ã‹ã—ã¦ã€æ¨è«–æ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã—ãŸã€‚
ã¾ãŸåˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

| å±¤ | `InferenceClsNaive` | `InferenceClsOpt1` |
| :-- | :-- | :-- |
| å…¨çµåˆå±¤1 (`PointNetCls::fc1`) | 1,056,279 (7.035ms) | 558,071 (3.717ms) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetCls::bn1`) | 516 (3.437us) | 260 (1.732us) |
| å…¨çµåˆå±¤2 (`PointNetCls::fc2`) | 266,007 (1.772ms) | 148,183 (987.0us) |
| ãƒãƒƒãƒæ­£è¦åŒ–å±¤ + ReLU (`PointNetCls::bn2`) | 260 (1.732us) | 132 (0.879us) |
| å…¨çµåˆå±¤3 (`PointNetCls::fc3`) | 10,481 (69.80us) | 5,261 (35.04us) |
| å…¨ä½“ | 1,333,605 (8.882ms) | 711,969 (4.742ms) |

æœ€åˆã®å…¨çµåˆå±¤ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸãŒã€å®Ÿè¡Œæ™‚é–“ã¯1.89å€ (1,056,279ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰558,071ã‚µã‚¤ã‚¯ãƒ«) ã—ã‹çŸ­ããªã£ã¦ã„ã¾ã›ã‚“ã€‚
å‰è¿°ã®ã‚ˆã†ã«ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€åˆã®å…¨çµåˆå±¤2ã¤ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«ç½®ãã®ã§ã¯ãªãã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’è»¢é€ã—ã¦ã„ã¾ã™ã€‚
è¡Œåˆ—ã®ç©ã‚„åŠ ç®—ã¯16ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§ã™ãŒã€ãƒ‡ãƒ¼ã‚¿è»¢é€éƒ¨åˆ†ã®å®Ÿè¡Œæ™‚é–“ã¯çŸ­ç¸®ã•ã‚Œãªã„ã®ã§ã€ã“ã®ã‚ˆã†ãªçµæœã«ãªã£ã¦ã„ã¾ã™ã€‚
2ã¤ç›®ã®å…¨çµåˆå±¤ã«é–¢ã—ã¦ã‚‚åŒæ§˜ã«ã€8ä¸¦åˆ—ã‚’æŒ‡å®šã—ãŸã®ã§ã™ãŒã€å®Ÿè¡Œæ™‚é–“ã¯1.80å€ (266,007ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰148,183ã‚µã‚¤ã‚¯ãƒ«) ã®å‰Šæ¸›ã«ç•™ã¾ã£ã¦ã„ã¾ã™ã€‚

ç¾åœ¨ã®å®Ÿè£…ã§ã¯ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹…ã¯32ãƒ“ãƒƒãƒˆã§ã€1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ã`float`ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ãšã¤è»¢é€ã—ã¦ã„ã¾ã™ã€‚
å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹…ã‚’åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã®å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã§ãã¾ã™ã€‚
å¾Œã»ã©ã€ãƒãƒ¼ãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã‹ã‚‰64ãƒ“ãƒƒãƒˆã«åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã§`float`ã®ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤è»¢é€ã™ã‚‹ã‚ˆã†ã«ã€æ”¹å–„ã—ã¾ã™ã€‚

IPã‚³ã‚¢ã®å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã«ã¯2ã¤ã‚ã‚Šã¾ã™ãŒã€ã“ã®ã†ã¡é‡ã¿ã®åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã«ã¤ã„ã¦ã¯ã€å…¨ãæ‰‹ã‚’åŠ ãˆã¦ã„ã¾ã›ã‚“ã€‚
é‡ã¿ã®åˆæœŸåŒ–ã¯ã€IPã‚³ã‚¢ã®åˆ©ç”¨é–‹å§‹å‰ã«ä¸€åº¦ã ã‘è¡Œã‚ã‚Œã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã¨ã¯å…¨ãé–¢ä¿‚ãªã„ãŸã‚ã§ã™ã€‚

ä»¥ä¸Šã§æ¨è«–ã®ä¸¦åˆ—åŒ–ãŒæ¸ˆã¿ã¾ã—ãŸã€‚
è©³ã—ãã¯`hls/src/top_opt1.cpp`ã‚’ã”å‚ç…§ãã ã•ã„ã€‚

## ä¸¦åˆ—åŒ–ãã®2 (ã‚¿ã‚¹ã‚¯ä¸¦åˆ—æ€§ã®æ´»ç”¨)

å„å±¤ã®è¨ˆç®—ã¯ä¸¦åˆ—åŒ–ã§ãã¾ã—ãŸãŒã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®éƒ¨åˆ†ã«ã¯ã€ã¾ã é«˜é€ŸåŒ–ã®ä½™åœ°ãŒæ®‹ã•ã‚Œã¦ã„ã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–å‡¦ç†ã‚’ã€ã‚‚ã†ä¸€åº¦ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```C++
  // Compute the feature
  for (int i = 0; i < num_points; ++i) {
#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N
#pragma HLS LOOP_FLATTEN off

    // ...

    // Read a point from a DDR memory
    ReadPointNaive<T>(point_cloud, i, x0);

    // Compute a point feature
    LinearOpt1<T, U, kFeatDims0, kFeatDims1, false, 2>(
      x0, x1, conv1->weight, conv1->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims1, 2>(
      x1, x2, bn1->scale, bn1->bias, bn1->mean);
    LinearOpt1<T, U, kFeatDims1, kFeatDims2, false, 8>(
      x2, x3, conv2->weight, conv2->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims2, 2>(
      x3, x4, bn2->scale, bn2->bias, bn2->mean);
    LinearOpt1<T, U, kFeatDims2, kFeatDims3, false, 8>(
      x4, x5, conv3->weight, conv3->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims3, 2>(
      x5, x6, bn3->scale, bn3->bias, bn3->mean);
    LinearOpt1<T, U, kFeatDims3, kFeatDims4, false, 16>(
      x6, x7, conv4->weight, conv4->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims4, 2>(
      x7, x8, bn4->scale, bn4->bias, bn4->mean);
    LinearOpt1<T, U, kFeatDims4, kFeatDims5, false, 128>(
      x8, x9, conv5->weight, conv5->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims5, 2>(
      x9, x10, bn5->scale, bn5->bias, bn5->mean);

    // Update the output feature
    MaxPool1dOpt1<T, kFeatDims5, 2>(x10, feature);
  }
```

ãƒ«ãƒ¼ãƒ—ã®å†…éƒ¨ã‚’ã¿ã‚‹ã¨ã€æœ€åˆã«ã€DRAMã«ç½®ã‹ã‚ŒãŸç‚¹ç¾¤`point_cloud`ã‹ã‚‰`i`ç•ªç›®ã®ç‚¹ã‚’å–ã£ã¦ãã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡`x0`ã«æ ¼ç´ã—ã¦ã„ã¾ã™ã€‚
ç¶šã„ã¦ã€ã“ã®`x0`ãŒãƒã‚±ãƒ„ãƒªãƒ¬ãƒ¼ã®ã‚ˆã†ã«ã€è¤‡æ•°ã®é–¢æ•°ã«æ¸¡ã•ã‚Œã¦ã„ãã¾ã™ã€‚
ä¾‹ãˆã°ã€æœ€åˆã®å…¨çµåˆå±¤ã«ã‚ˆã£ã¦`x0`ã‹ã‚‰`x1`ã€ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã«ã‚ˆã£ã¦`x1`ã‹ã‚‰`x2`ã€æ¬¡ã®å…¨çµåˆå±¤ã«ã‚ˆã£ã¦`x2`ã‹ã‚‰`x3`ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã‚ã‚‹å±¤ã®é–¢æ•° (ä¾‹ãˆã°`LinearOpt1(x4, x5)`) ã¯ã€ãã®ä¸€ã¤å‰ã®é–¢æ•°ã®å‡ºåŠ› (`x4`) ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€å‡ºåŠ› (`x5`) ã‚’æ¬¡ã®é–¢æ•°ã«å¼•ãæ¸¡ã—ã¾ã™ã€‚
å…¨ã¦ã®é–¢æ•°ãŒã€å…¥å‡ºåŠ›ã‚’ä»‹ã—ã¦ã€æ•°ç ã¤ãªãã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚
é–¢æ•°ã®å®Ÿè¡Œã®æµã‚Œã‚’å›³ã«ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

[<img src="point-cloud-classification-images/dataflow-optimization-before.svg" width="80%" />](point-cloud-classification-images/dataflow-optimization-before.svg)

å…ˆç¨‹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¨åŒæ§˜ã«ã€è¤‡æ•°ã®ç‚¹ã«ã¤ã„ã¦å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã§ãã¾ã™ã€‚

[<img src="point-cloud-classification-images/dataflow-optimization-after.svg" width="90%" />](point-cloud-classification-images/dataflow-optimization-after.svg)

ä¾‹ãˆã°ã€1ã¤ç›®ã®ç‚¹ã«å¯¾ã—ã¦ã€æœ€å¾Œã®å…¨çµåˆå±¤ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹é–“ã«ã€2ã¤ç›®ã®ç‚¹ã«å¯¾ã—ã¦ã€ãã®ä¸€ã¤å‰ã®ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã‚’è¨ˆç®—ã™ã‚‹ã¨ã„ã†ã‚ˆã†ã«ã€è¤‡æ•°ã®ç‚¹ã«å¯¾ã™ã‚‹å‡¦ç†ã‚’æ™‚é–“çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¾ã™ã€‚
ä»¥å‰ã¯ã€ãƒ«ãƒ¼ãƒ—å†…ã®å‡¦ç†ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¦ã€ãƒ«ãƒ¼ãƒ—ã®è¤‡æ•°ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã¾ã—ãŸã€‚
ãã—ã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ã€ä¸»ã«ä¹—ç®—ã‚„åŠ ç®—ã§ã—ãŸã€‚
ã“ã“ã§ã¯ã€å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ä¸€ã¤ã®é–¢æ•° (ã‚¿ã‚¹ã‚¯) ã«å¯¾å¿œã™ã‚‹ã®ã§ã€ã‚ˆã‚Šç²—ç²’åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¨ã„ãˆã¾ã™ã€‚
ã“ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã¯ã€Vitis HLSã§ã¯**ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–** (Dataflow optimization) ã¨ã‚ˆã°ã‚Œã¦ã„ã¾ã™ (**æœ€é©åŒ–ãã®6: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–**)ã€‚
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ã„ã‚ã„ã‚ãªæ¡ä»¶ãŒã‚ã‚Šã¾ã™ãŒã€ä»Šå›ã®å ´åˆã¯å¤§ä¸ˆå¤«ã§ã™ã€‚

ä»¥å‰è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’ãªã‚‹ã¹ãå‡ç­‰ã«æƒãˆã‚‹ã“ã¨ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŠ¹æœãŒå¢—ã—ã¾ã™ã€‚
å„å±¤ã®è¨ˆç®—æ™‚é–“ã‚’ã€ãªã‚‹ã¹ãå‡ä¸€ã«ã—ãŸã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚
è¨ˆç®—æ™‚é–“ã¯ã€ä¸Šã®è¡¨ã«ã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’åˆ©ç”¨ã™ã‚‹å‰ã¯ã€å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•° (ç‰¹ã«å…¨çµåˆå±¤) ã«ã¯ã€ã‹ãªã‚Šã®ã°ã‚‰ã¤ããŒã‚ã‚Šã¾ã—ãŸã€‚
å…¨çµåˆå±¤5ã¤ã ã‘æŠœãå‡ºã—ã¦ã¿ã‚‹ã¨ã€577ã€4,481ã€4,481ã€8,961ã€137,217ã¨ãªã£ã¦ã„ã¾ã™ã€‚
ãã‚Œãã‚Œã®å±¤ã‚’ã€2ã€8ã€8ã€16ã€128ä¸¦åˆ—ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ (`InferenceFeatOpt1`ã‚’å‚ç…§)ã€321ã€569ã€569ã€569ã€1,081ã‚µã‚¤ã‚¯ãƒ«ã«å‰Šæ¸›ã•ã‚Œã€ã°ã‚‰ã¤ãã‚‚ã‹ãªã‚ŠæŠ‘ãˆã‚‰ã‚Œã¾ã—ãŸã€‚
æœ€å¾Œã®å…¨çµåˆå±¤ã‚’256ä¸¦åˆ—ã«ã™ã‚Œã°ã€ã•ã‚‰ã«å‡ç­‰ã«ãªã‚Šã¾ã™ãŒã€å›è·¯ãŒè¤‡é›‘ã«ãªã‚Šéãã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸã€‚

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯æœ€ã‚‚æ™‚é–“ã®é•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸ã«ã‚ˆã£ã¦æ€§èƒ½ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚
ä»Šå›ã®å ´åˆã¯ã€æœ€å¾Œã®å…¨çµåˆå±¤ (1,081ã‚µã‚¤ã‚¯ãƒ«) ã«ã‚ˆã£ã¦æ€§èƒ½ãŒæ±ºã¾ã‚Šã¾ã™ã€‚
ä»–ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ã€1,081ã‚µã‚¤ã‚¯ãƒ«ä»¥ä¸‹ã§ã‚ã‚Œã°ã€ä½•ã‚µã‚¤ã‚¯ãƒ«ã§ã‚ã‚ã†ã¨ã‚‚æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚
ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ä»–ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã«é–¢ã—ã¦ã¯ã€1,081ã‚µã‚¤ã‚¯ãƒ«ã‚’è¶…ãˆãªã„ç¯„å›²ã§ã€ãªã‚‹ã¹ãä¸¦åˆ—åº¦ã‚’è½ã¨ã—ã¾ã—ãŸã€‚

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã“ã®ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’äºˆã‚è€ƒæ…®ã—ãŸã†ãˆã§ã€å„å±¤ã®ä¸¦åˆ—åº¦ã‚’æŒ‡å®šã—ã¾ã—ãŸã€‚
åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸¦åˆ—åº¦ã¯ã€ä½•ã¨ãªãæ±ºã‚ã¦ã„ã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ–½ã—ãŸå®Ÿè£…ã‚’ã€æ¬¡ã«ç¤ºã—ã¾ã™ã€‚
`InferenceFeatOpt1`ã‹ã‚‰ã€`InferenceFeatOpt2`ã¨ã—ã¾ã—ãŸã€‚

```C++
// Parallel implementation of the PointNet feature extraction
// `T` is the type for layer input, output, and intermediate results
// `U` is the type for parameters
// `N` is the expected number of input points (e.g., 1024)
template <typename T, typename U, int N>
void InferenceFeatOpt2(...)
{
#pragma HLS INLINE off

  // Zero-initialize the output feature
  VectorNdSetZero<T, kFeatDims5>(feature);

  // Compute the feature
  for (int i = 0; i < num_points; ++i) {
#pragma HLS LOOP_TRIPCOUNT min=N max=N avg=N
#pragma HLS LOOP_FLATTEN off
#pragma HLS DATAFLOW

#pragma HLS STABLE variable=point_cloud
#pragma HLS STABLE variable=num_points
#pragma HLS STABLE variable=feature
#pragma HLS STABLE variable=conv1
#pragma HLS STABLE variable=conv2
#pragma HLS STABLE variable=conv3
#pragma HLS STABLE variable=conv4
#pragma HLS STABLE variable=conv5
#pragma HLS STABLE variable=bn1
#pragma HLS STABLE variable=bn2
#pragma HLS STABLE variable=bn3
#pragma HLS STABLE variable=bn4
#pragma HLS STABLE variable=bn5

    // Input, output, and intermediate results
    // ...

    // Read a point from a DDR memory
    ReadPointNaive<T>(point_cloud, i, x0);

    // Compute a point feature
    LinearOpt1<T, U, kFeatDims0, kFeatDims1, false, 2>(
      x0, x1, conv1->weight, conv1->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims1, 2>(
      x1, x2, bn1->scale, bn1->bias, bn1->mean);
    LinearOpt1<T, U, kFeatDims1, kFeatDims2, false, 8>(
      x2, x3, conv2->weight, conv2->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims2, 2>(
      x3, x4, bn2->scale, bn2->bias, bn2->mean);
    LinearOpt1<T, U, kFeatDims2, kFeatDims3, false, 8>(
      x4, x5, conv3->weight, conv3->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims3, 2>(
      x5, x6, bn3->scale, bn3->bias, bn3->mean);
    LinearOpt1<T, U, kFeatDims3, kFeatDims4, false, 16>(
      x6, x7, conv4->weight, conv4->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims4, 2>(
      x7, x8, bn4->scale, bn4->bias, bn4->mean);
    LinearOpt1<T, U, kFeatDims4, kFeatDims5, false, 128>(
      x8, x9, conv5->weight, conv5->bias);
    BatchNorm1dReLUOpt1<T, U, kFeatDims5, 2>(
      x9, x10, bn5->scale, bn5->bias, bn5->mean);

    // Update the output feature
    MaxPool1dOpt1<T, kFeatDims5, 2>(x10, feature);
  }
}
```

`InferenceFeatOpt1`ã¨ç•°ãªã‚‹ã®ã¯HLSãƒ—ãƒ©ã‚°ãƒã®éƒ¨åˆ†ã ã‘ã§ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã®å…ˆé ­éƒ¨åˆ†ã«ã¯`#pragma HLS DATAFLOW`ã®è¨˜è¿°ãŒã‚ã‚Šã€ãƒ«ãƒ¼ãƒ—ã®ä¸­èº«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã—ã¾ã™ã€‚
`#pragma HLS STABLE`ã®éƒ¨åˆ†ã¯ã€ãƒ«ãƒ¼ãƒ—ã®å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã™ã‚‹ã«ã‚ãŸã£ã¦ã€ãã®å¤‰æ•°ã«ã¤ã„ã¦åŒæœŸã‚’ã¨ã‚‹å¿…è¦ãŒãªã„ã€ã¨ã„ã†ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ç‚¹ç¾¤ãªã©ã€ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œä¸­ã¯å¤‰åŒ–ã—ãªã„å¤‰æ•°ã«ä»˜ä¸ã—ã¦ã„ã¾ã™ã€‚
ã“ã®è¨˜è¿°ãŒãªã„ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãŒã†ã¾ãæ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚

ã“ã®2ç¨®é¡ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ¿å…¥ã™ã‚‹ã ã‘ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’ã„ã¨ã‚‚ç°¡å˜ã«å®Ÿç¾ã§ãã¾ã™ã€‚
é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã¯ç´ æ™´ã‚‰ã—ã„ã¨æ€ã„ã¾ã™ã€‚
`PointNetClsTop` (ãƒˆãƒƒãƒ—é–¢æ•°) ã‚„åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«– (`InferenceClsOpt1`) ã«ã¤ã„ã¦ã¯ä»¥å‰ã¨å…¨ãåŒã˜ã§ã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å‰²æ„›ã—ã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’ã¿ã¦ã¿ã¾ã™ã€‚
`InferenceFeatOpt1`ã§ã¯ã€1ã¤ã®ç‚¹ã«å¯¾ã™ã‚‹é †ä¼æ’­ã«4,357ã‚µã‚¤ã‚¯ãƒ« (29.02us) è¦ã—ã¦ã„ã¾ã—ãŸãŒã€`InferenceFeatOpt2`ã§ã‚‚4,344ã‚µã‚¤ã‚¯ãƒ« (28.93us) ã§ã€ã»ã¼å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚
ä¸€æ–¹ã€1,024å€‹ã®ç‚¹ã«å¯¾ã™ã‚‹å‡¦ç†æ™‚é–“ã‚’ã¿ã¦ã¿ã‚‹ã¨ã€`InferenceFeatOpt1`ã§ã¯4,462,596ã‚µã‚¤ã‚¯ãƒ« (29.72ms) ã§ã—ãŸãŒã€`InferenceFeatOpt2`ã§ã¯1,112,259ã‚µã‚¤ã‚¯ãƒ« (7.408ms) ã«å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¦ã‚‚ã€å„å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹è¨ˆç®—æ™‚é–“ (ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·) ã¯å¤‰åŒ–ã—ã¾ã›ã‚“ãŒã€å˜ä½æ™‚é–“ã‚ãŸã‚Šã«å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ•° (ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ) ã¯æ”¹å–„ã™ã‚‹ã®ã§ã€ãã‚Œã«ä¼´ã£ã¦å…¨ä½“ã®æ€§èƒ½ã‚‚å‘ä¸Šã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚

ã“ã‚Œã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã¯çµ‚ã‚ã‚Šã§ã™ã€‚
è©³ã—ãã¯`hls/src/top_opt2.cpp`ã‚’ã”è¦§ãã ã•ã„ã€‚

## å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆå¹…ã®æ‹¡å¼µ

åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨çµåˆå±¤éƒ¨åˆ†ã§ã¯ã€ç©å’Œæ¼”ç®—ã‚’ä¸¦åˆ—åŒ–ã—ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å…¨ä½“ã®å‡¦ç†æ™‚é–“ã¯ãã‚Œã»ã©çŸ­ç¸®ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚
DRAMã‹ã‚‰ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»¢é€ã®ã‚µã‚¤ã‚¯ãƒ«æ•°ãŒã€å¤‰åŒ–ã—ã¦ã„ãªã„ãŸã‚ã§ã™ã€‚
ãã“ã§æœ€å¾Œã®æœ€é©åŒ–ã¨ã—ã¦ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®ãƒ“ãƒƒãƒˆå¹…ã‚’32ã‹ã‚‰64ã«åºƒã’ã¦ã€1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ã2ã¤ã®`float`ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã§ãã‚‹ã‚ˆã†ã«ã€å®Ÿè£…ã‚’ä¿®æ­£ã—ã¦ã¿ã¾ã—ã‚‡ã† (**æœ€é©åŒ–ãã®7: ãƒ‡ãƒ¼ã‚¿è»¢é€**)ã€‚

æœ€åˆã«ã€IPã‚³ã‚¢ã®æœ€ä¸Šä½é–¢æ•°`PointNetClsTop`ã‹ã‚‰ä¿®æ­£ã—ã¾ã™ã€‚
ä¿®æ­£å‰ã¯ã€æ¬¡ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã—ãŸã€‚
```C++
void PointNetClsTop(const int op_mode,
                    const float* point_cloud,
                    const int num_points,
                    float* out_logits,
                    const float* feat_params1,
                    const float* feat_params2,
                    const float* feat_params3,
                    const float* feat_params4,
                    const float* feat_params5,
                    const float* cls_params1,
                    const float* cls_params2,
                    const float* cls_params3)
{
  // ...
}
```

ã“ã‚Œã‚’ã€æ¬¡ã®ã‚ˆã†ã«64ãƒ“ãƒƒãƒˆå¹…ã«ã—ã¾ã™ã€‚
```C++
void PointNetClsTop(const int op_mode,
                    const ap_uint<64>* point_cloud,
                    const int num_points,
                    ap_uint<64>* out_logits,
                    const ap_uint<64>* feat_params1,
                    const ap_uint<64>* feat_params2,
                    const ap_uint<64>* feat_params3,
                    const ap_uint<64>* feat_params4,
                    const ap_uint<64>* feat_params5,
                    const ap_uint<64>* cls_params1,
                    const ap_uint<64>* cls_params2,
                    const ap_uint<64>* cls_params3)
{
  // ...
}
```

`ap_uint`ã¯ã€Vitis HLSã§æä¾›ã•ã‚Œã¦ã„ã‚‹ã€ä»»æ„ãƒ“ãƒƒãƒˆé•·ã®ç¬¦å·ãªã—æ•´æ•°å‹ã§ã™ã€‚
ã“ã“ã§ã¯64ãƒ“ãƒƒãƒˆã¨ã—ã¦ã„ã¾ã™ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã«ã¤ããƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‰ãªã‘ã‚Œã°ã„ã‘ãªã„ã®ã§ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã«é–¢ã™ã‚‹éƒ¨åˆ†ã‚’å…¨ã¦ä¿®æ­£ã—ã¾ã™ã€‚
DRAMã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã—ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã™ã‚‹ã€é‡ã¿åˆæœŸåŒ–é–¢æ•°`InitializeFeatOpt1`ã€`InitializeClsOpt1`ã‚‚æ¬¡ã®ã‚ˆã†ã«ç›´ã—ã¦ã€æ–°ãŸã«`InitializeFeatOpt3`ã€`InitializeClsOpt3`ã¨ã—ã¾ã™ã€‚
å˜ã«ã€é–¢æ•°ã®å¼•æ•°ã‚’`float*`ã‹ã‚‰`ap_uint<64>*`ã«å¤‰æ›´ã—ãŸã ã‘ã§ã™ã€‚
```C++
// Parallel implementation of the parameter initialization
// `T` is the type for parameters
template <typename T>
void InitializeFeatOpt3(LinearParams<T, kFeatDims0, kFeatDims1>* conv1,
                        LinearParams<T, kFeatDims1, kFeatDims2>* conv2,
                        LinearParams<T, kFeatDims2, kFeatDims3>* conv3,
                        LinearParams<T, kFeatDims3, kFeatDims4>* conv4,
                        LinearParams<T, kFeatDims4, kFeatDims5>* conv5,
                        BatchNorm1dParams<T, kFeatDims1>* bn1,
                        BatchNorm1dParams<T, kFeatDims2>* bn2,
                        BatchNorm1dParams<T, kFeatDims3>* bn3,
                        BatchNorm1dParams<T, kFeatDims4>* bn4,
                        BatchNorm1dParams<T, kFeatDims5>* bn5,
                        const ap_uint<64>* params1,
                        const ap_uint<64>* params2,
                        const ap_uint<64>* params3,
                        const ap_uint<64>* params4,
                        const ap_uint<64>* params5)
{
#pragma HLS INLINE off

  ReadBlockParamsOpt2<T, kFeatDims0, kFeatDims1>(conv1, bn1, params1);
  ReadBlockParamsOpt1<T, kFeatDims1, kFeatDims2>(conv2, bn2, params2);
  ReadBlockParamsOpt1<T, kFeatDims2, kFeatDims3>(conv3, bn3, params3);
  ReadBlockParamsOpt1<T, kFeatDims3, kFeatDims4>(conv4, bn4, params4);
  ReadBlockParamsOpt1<T, kFeatDims4, kFeatDims5>(conv5, bn5, params5);
}

// Parallel implementation of the parameter initialization
// `T` is the type for parameters
template <typename T>
void InitializeClsOpt3(LinearParams<T, kClsDims2, kClsDims3>* fc3,
                       BatchNorm1dParams<T, kClsDims1>* bn1,
                       BatchNorm1dParams<T, kClsDims2>* bn2,
                       const ap_uint<64>* params1,
                       const ap_uint<64>* params2,
                       const ap_uint<64>* params3)
{
#pragma HLS INLINE off

  ReadBatchNorm1dParamsOpt1<T, kClsDims1>(
    bn1, params1, kClsDims0 * kClsDims1 + kClsDims1);
  ReadBatchNorm1dParamsOpt1<T, kClsDims2>(
    bn2, params2, kClsDims1 * kClsDims2 + kClsDims2);
  ReadLinearParamsOpt1<T, kClsDims2, kClsDims3>(
    fc3, params3, 0);
}
```

æœ€åˆã®å®Ÿè£…ã§ã¯`ReadLinearParamsNaive`ã€`ReadBatchNorm1dParamsNaive`ã€`ReadBlockParamsNaive`ã‚’ä½¿ã£ã¦ã„ã¾ã—ãŸãŒã€ã“ã“ã§ã¯æ–°ãŸã«`ReadLinearParamsOpt1`ã€`ReadBatchNorm1dParamsOpt1`ã€`ReadBlockParamsOpt1`ã€`ReadBlockParamsOpt2`ã®4ç¨®é¡ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚
è©³ã—ãä¸­èº«ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
```C++
// Parallel implementation of the parameter initialization
// Read the parameters for a linear layer from a DDR memory and
// store them to BRAM buffers
// `T` is the type for parameters
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
template <typename T, int InDims, int OutDims>
void ReadLinearParamsOpt1(LinearParams<T, InDims, OutDims>* linear,
                          const ap_uint<64>* params,
                          const int offset)
{
#pragma HLS INLINE
  // `params` contains weight parameters of size (`OutDims`, `InDims`) and
  // bias parameters of size (`OutDims`) in a contiguous buffer

  static_assert(InDims % 2 == 0, "`InDims` must be a multiple of 2");
  static_assert(OutDims % 2 == 0, "`OutDims` must be a multiple of 2");
  assert(offset % 2 == 0);

  ReadTensor2dOpt1<T, OutDims, InDims>(linear->weight, params, offset);
  ReadTensor1dOpt1<T, OutDims>(linear->bias, params,
                               offset + InDims * OutDims);
}

// Parallel implementation of the parameter initialization
// Read the parameters for a 1D batch normalization layer from a DDR memory and
// store them to BRAM buffers
// `T` is the type for parameters
// `Dims` is the number of input and output dimensions
template <typename T, int Dims>
void ReadBatchNorm1dParamsOpt1(BatchNorm1dParams<T, Dims>* bn,
                               const ap_uint<64>* params,
                               const int offset)
{
#pragma HLS INLINE
  // `params` contains scale parameters of size (`Dims`),
  // bias of size (`Dims`), and mean of size (`Dims`) in a contiguous buffer

  static_assert(Dims % 2 == 0, "`Dims` must be a multiple of 2");
  assert(offset % 2 == 0);

  ReadTensor1dOpt1<T, Dims>(bn->scale, params, offset);
  ReadTensor1dOpt1<T, Dims>(bn->bias, params, offset + Dims);
  ReadTensor1dOpt1<T, Dims>(bn->mean, params, offset + Dims * 2);
}

// Parallel implementation of the parameter initialization
// Read the parameters for a linear and 1D batch normalization layer
// from a DDR memory and store them to BRAM buffers
// `T` is the type for parameters
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
template <typename T, int InDims, int OutDims>
void ReadBlockParamsOpt1(LinearParams<T, InDims, OutDims>* linear,
                         BatchNorm1dParams<T, OutDims>* bn,
                         const ap_uint<64>* params)
{
#pragma HLS INLINE

  static_assert(InDims % 2 == 0, "`InDims` must be a multiple of 2");
  static_assert(OutDims % 2 == 0, "`OutDims` must be a multiple of 2");

  ReadTensor2dOpt1<T, OutDims, InDims>(linear->weight, params, 0);
  ReadTensor1dOpt1<T, OutDims>(linear->bias, params, InDims * OutDims);
  ReadTensor1dOpt1<T, OutDims>(bn->scale, params,
                               InDims * OutDims + OutDims);
  ReadTensor1dOpt1<T, OutDims>(bn->bias, params,
                               InDims * OutDims + OutDims * 2);
  ReadTensor1dOpt1<T, OutDims>(bn->mean, params,
                               InDims * OutDims + OutDims * 3);
}

// Parallel implementation of the parameter initialization
// Read the parameters for a linear and 1D batch normalization layer
// from a DDR memory and store them to BRAM buffers
// `T` is the type for parameters
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
template <typename T, int InDims, int OutDims>
void ReadBlockParamsOpt2(LinearParams<T, InDims, OutDims>* linear,
                         BatchNorm1dParams<T, OutDims>* bn,
                         const ap_uint<64>* params)
{
#pragma HLS INLINE

  static_assert(InDims == 3, "`InDims` must be 3");
  static_assert(OutDims % 2 == 0, "`OutDims` must be a multiple of 2");

  ReadTensor2dOpt2<T, OutDims, InDims>(linear->weight, params, 0);
  ReadTensor1dOpt1<T, OutDims>(linear->bias, params, InDims * OutDims);
  ReadTensor1dOpt1<T, OutDims>(bn->scale, params,
                               InDims * OutDims + OutDims);
  ReadTensor1dOpt1<T, OutDims>(bn->bias, params,
                               InDims * OutDims + OutDims * 2);
  ReadTensor1dOpt1<T, OutDims>(bn->mean, params,
                               InDims * OutDims + OutDims * 3);
}
```

åŸºæœ¬çš„ã«ã¯å…ƒã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã¨åŒã˜ã§ã™ãŒã€å¼•æ•°ã®å‹ãŒ`float*`ã‹ã‚‰`ap_uint<64>*`ã«å¤‰ã‚ã£ã¦ã„ã¾ã™ã€‚
é–¢æ•°ã®ä¸­èº«ã‚‚å˜ç´”ã§ã€æŒ‡å®šã—ãŸã‚ªãƒ•ã‚»ãƒƒãƒˆã‹ã‚‰ã€æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ã“ã¨ã‚’ç¹°ã‚Šè¿”ã™ã ã‘ã§ã™ã€‚
ä¾‹ãˆã°ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹ã¨ãã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã®é †ã«èª­ã¿å–ã‚Šã¾ã™ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ä¸Šã«ã¯äºˆã‚ã€æ­£ã—ã„ä½ç½®ã«ã“ã®é †ã§ä¸¦ã¹ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ä¸­ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹é–¢æ•°`ReadTensor1dOpt1`ã€`ReadTensor2dOpt1`ã€`ReadTensor2dOpt2`ã¯æ¬¡ã®é€šã‚Šã§ã™ã€‚
```C++
union conv32_t
{
  std::uint32_t u32;
  int i32;
  float f;
};

// Interpret float as std::uint32_t
inline std::uint32_t FloatToU32(const float f)
{
  conv32_t conv;
  conv.f = f;
  return conv.u32;
}

// Interpret std::uint32_t as float
inline float U32ToFloat(const std::uint32_t u32)
{
  conv32_t conv;
  conv.u32 = u32;
  return conv.f;
}

// Read a 1D tensor from a DDR memory
template <typename T, int D0>
void ReadTensor1dNaive(T tensor[D0],
                       const float* src,
                       const int offset)
{
#pragma HLS INLINE off

  for (int i = 0; i < D0; ++i) {
#pragma HLS PIPELINE II=1
    tensor[i] = T(src[offset + i]);
  }
}

// Read a 1D tensor from a DDR memory
template <typename T, int D0>
void ReadTensor1dOpt1(T tensor[D0],
                      const ap_uint<64>* src,
                      const int offset)
{
#pragma HLS INLINE off

  static_assert(D0 % 2 == 0, "`D0` must be a multiple of 2");
  assert(offset % 2 == 0);

  constexpr const int D0Over2 = D0 / 2;
  const int offset2 = offset / 2;

  for (int i = 0; i < D0Over2; ++i) {
#pragma HLS PIPELINE II=1
    const ap_uint<64> tensor_data = src[offset2 + i];
    tensor[i * 2 + 0] = T(U32ToFloat(tensor_data.range(31, 0)));
    tensor[i * 2 + 1] = T(U32ToFloat(tensor_data.range(63, 32)));
  }
}

// Read a 2D tensor from a DDR memory
template <typename T, int D0, int D1>
void ReadTensor2dNaive(T tensor[D0][D1],
                       const float* src,
                       const int offset)
{
#pragma HLS INLINE off

  for (int i = 0; i < D0; ++i) {
    for (int j = 0; j < D1; ++j) {
#pragma HLS PIPELINE II=1
      const int idx = i * D1 + j;
      tensor[i][j] = T(src[offset + idx]);
    }
  }
}

// Read a 2D tensor from a DDR memory
template <typename T, int D0, int D1>
void ReadTensor2dOpt1(T tensor[D0][D1],
                      const ap_uint<64>* src,
                      const int offset)
{
#pragma HLS INLINE off

  static_assert(D1 % 2 == 0, "`D1` must be a multiple of 2");
  assert(offset % 2 == 0);

  constexpr const int D1Over2 = D1 / 2;
  const int offset2 = offset / 2;

  for (int i = 0; i < D0; ++i) {
    for (int j = 0; j < D1Over2; ++j) {
#pragma HLS PIPELINE II=1
      const int idx = i * D1Over2 + j;
      const ap_uint<64> tensor_data = src[offset2 + idx];
      tensor[i][j * 2 + 0] = T(U32ToFloat(tensor_data.range(31, 0)));
      tensor[i][j * 2 + 1] = T(U32ToFloat(tensor_data.range(63, 32)));
    }
  }
}

// Read a 2D tensor of size (`D0`, 3) from a DDR memory
template <typename T, int D0, int D1>
void ReadTensor2dOpt2(T tensor[D0][D1],
                      const ap_uint<64>* src,
                      const int offset)
{
#pragma HLS INLINE off

  static_assert(D0 % 2 == 0, "`D0` must be a multiple of 2");
  static_assert(D1 == 3, "`D1` must be 3");
  assert(offset % 2 == 0);

  constexpr const int Iter = D0 * D1 / (2 * 3);
  const int offset2 = offset / 2;

  for (int i = 0; i < Iter; ++i) {
#pragma HLS PIPELINE
    const int src_idx = i * 3;
    const int dst_idx = i * 2;
    const ap_uint<64> tensor_data0 = src[offset2 + src_idx + 0];
    const ap_uint<64> tensor_data1 = src[offset2 + src_idx + 1];
    const ap_uint<64> tensor_data2 = src[offset2 + src_idx + 2];
    tensor[dst_idx + 0][0] = T(U32ToFloat(tensor_data0.range(31, 0)));
    tensor[dst_idx + 0][1] = T(U32ToFloat(tensor_data0.range(63, 32)));
    tensor[dst_idx + 0][2] = T(U32ToFloat(tensor_data1.range(31, 0)));
    tensor[dst_idx + 1][0] = T(U32ToFloat(tensor_data1.range(63, 32)));
    tensor[dst_idx + 1][1] = T(U32ToFloat(tensor_data2.range(31, 0)));
    tensor[dst_idx + 1][2] = T(U32ToFloat(tensor_data2.range(63, 32)));
  }
}
```

æ¯”è¼ƒã§ãã‚‹ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ãšã¤èª­ã¿å–ã‚‹ã€å…ƒã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã‚‚è¼‰ã›ã¾ã—ãŸã€‚
å„é–¢æ•°ã®å‹•ä½œã‚’ã¾ã¨ã‚ã¾ã™ã€‚

- `ReadTensor1dOpt1<T, D0>(tensor, src, offset)`: æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡`src`ã®ã€`float`ã§`offset`å€‹åˆ†ã ã‘ãšã‚‰ã—ãŸå ´æ‰€ã‹ã‚‰ (`src`ã«`4 * offset`ãƒã‚¤ãƒˆåˆ†ã ã‘è¶³ã—ãŸã‚¢ãƒ‰ãƒ¬ã‚¹ã‹ã‚‰)ã€`D0`å€‹åˆ†ã®`float`ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‹ã€‚
èª­ã¿å–ã£ãŸãƒ‡ãƒ¼ã‚¿ã¯`float`ã‹ã‚‰`T`å‹ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸ1æ¬¡å…ƒã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡`tensor` (ã‚µã‚¤ã‚º`(D0)`)ã«2ã¤ãšã¤æ ¼ç´ã™ã‚‹ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã§2ã¤ãšã¤èª­ã¿å–ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º`D0`ã¯å¶æ•°ã¨ä»®å®šã—ã¦ã„ã‚‹ã€‚
- `ReadTensor2dOpt1<T, D0, D1>(tensor, src, offset)`: æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡`src`ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã£ã¦ã€2æ¬¡å…ƒã®ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡`tensor` (ã‚µã‚¤ã‚º`(D0, D1)`)ã«æ ¼ç´ã™ã‚‹ã€‚
1ã‚µã‚¤ã‚¯ãƒ«ã§2ã¤ãšã¤èª­ã¿å–ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º`D1`ã¯å¶æ•°ã¨ä»®å®šã—ã¦ã„ã‚‹ã€‚
- `ReadTensor2dOpt2<T, D0, D1>(tensor, src, offset)`: `D1`ãŒ3ã§ã‚ã‚‹å ´åˆã®å°‚ç”¨ã®å®Ÿè£…ã€‚
3ã‚µã‚¤ã‚¯ãƒ«æ›ã‘ã¦ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡`src`ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’6ã¤èª­ã¿å–ã£ãŸå¾Œã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡`tensor`ã«æ ¼ç´ã—ã¦ã„ãã€‚
å®Ÿè£…ã‚’ç°¡ç•¥åŒ–ã™ã‚‹ãŸã‚ã€ã‚µã‚¤ã‚ºã«é–¢ã—ã¦ã¯ã€`D1`ã¯3ã€`D0`ã¯å¶æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ä»®å®šã—ã¦ã„ã‚‹ (è¦ç´ æ•°ãŒå¶æ•°)ã€‚

`ReadTensor2dOpt2`ãŠã‚ˆã³`ReadBlockParamsOpt2`ã¯ã€ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹æœ€åˆã®å…¨çµåˆå±¤ã®é‡ã¿ã‚’è»¢é€ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™ (`InitializeFeatOpt3`ã‚’å‚ç…§)ã€‚
æœ€åˆã®å…¨çµåˆå±¤ã¯ã€3æ¬¡å…ƒã®ç‚¹ã®åº§æ¨™ã‚’64æ¬¡å…ƒã®ç‰¹å¾´ã«å¤‰æ›ã™ã‚‹ã®ã§ã€é‡ã¿ã®ã‚µã‚¤ã‚ºã¯`(64, 3)`ã¨ãªã‚Šã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚ŠãŸã„ã®ã«ã€2ç•ªç›®ã®æ¬¡å…ƒãŒå¥‡æ•°ã§ã€å®Ÿè£…ä¸Šã®éƒ½åˆãŒæ‚ªã„ã®ã§ã€å°‚ç”¨ã®é–¢æ•°ã‚’ç”¨æ„ã—ãŸã‚ã‘ã§ã™ã€‚
`ReadTensor2dOpt2`ã§ã¯ã€é‡ã¿ã‚’6ã¤ãšã¤èª­ã¿å–ã‚‹ã“ã¨ã§å¯¾å‡¦ã—ã¦ã„ã¾ã™ã€‚
åˆ¥ã®å¯¾å‡¦æ³•ã¨ã—ã¦ã¯ã€é‡ã¿ã®ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’`(64, 3)`ã‹ã‚‰`(64, 4)`ã«åºƒã’ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ (4ç•ªç›®ã®æ¬¡å…ƒã¯å˜ã«ä½¿ã‚ãªã„)ã€‚

`ReadBlockParamsOpt1`ã¨`ReadBlockParamsOpt2`ã®é•ã„ã¯ã€`ReadTensor2dOpt1`ã¨`ReadTensor2dOpt2`ã®ã©ã¡ã‚‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ã ã‘ã§ã™ã€‚
2ã¤ã®é–¢æ•°ã¯ã€C++17ã«ç”¨æ„ã•ã‚ŒãŸ`if constexpr`æ–‡ã‚’ä½¿ãˆã°ã€1ã¤ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ãŒã€ä»Šå›ã¯C++14ã¾ã§ã®æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€åˆ¥ã€…ã«ã—ã¦ã„ã¾ã™ã€‚

`ap_uint`å‹ã«ã¯`range()`ã¨ã„ã†ä¾¿åˆ©ãªãƒ¡ã‚½ãƒƒãƒ‰ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€æŒ‡å®šã—ãŸãƒ“ãƒƒãƒˆã®éƒ¨åˆ†ã‚’è‡ªç”±ã«å–ã‚Šå‡ºã›ã¾ã™ã€‚
`range(31, 0)`ã§ä¸‹ä½32ãƒ“ãƒƒãƒˆã€`range(63, 32)`ã§ä¸Šä½32ãƒ“ãƒƒãƒˆã‚’å–ã‚Šå‡ºã—ã¦ã„ã¾ã™ã€‚

`U32ToFloat()`ã€`FloatToU32()`ã¯ã€ãƒ“ãƒƒãƒˆè¡¨ç¾ã‚’ç¶­æŒã—ãŸã¾ã¾ã€åˆ¥ã®å‹ã«è§£é‡ˆã™ã‚‹ãŸã‚ã®é–¢æ•°ã§ã™ (`float`ã¨ç¬¦å·ãªã—32ãƒ“ãƒƒãƒˆæ•´æ•°)ã€‚
`tensor_data.range(31, 0)`ã¯32ãƒ“ãƒƒãƒˆã®ç¬¦å·ãªã—æ•´æ•°å‹ (`unsigned int`ã‚„`ap_uint<32>`) ã§ã™ãŒã€å®Ÿéš›ã«ã¯`float`ã®ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ã€`U32ToFloat()`ã‚’ä½¿ã£ã¦`float`ã«è§£é‡ˆã—ç›´ã—ã¦ã„ã¾ã™ã€‚
2ã¤ã®é–¢æ•°ã¯ã€å…±ç”¨ä½“ã‚’ä½¿ã£ã¦å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
C++20ã§ã‚ã‚Œã°ã€`std::bit_cast`ã§åŒç­‰ã®å‡¦ç†ãŒã§ãã¾ã™ã€‚

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–ã«ç€ç›®ã—ã¾ã™ (`InferenceFeatOpt2`ã‚’å‚ç…§)ã€‚
`i`ç•ªç›®ã®ç‚¹ã‚’DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰èª­ã¿å–ã‚‹`ReadPointNaive`ã‚‚ã€64ãƒ“ãƒƒãƒˆå¹…ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¾ã™ã€‚
ä¿®æ­£å¾Œã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’`ReadPointOpt1`ã¨ã—ã¾ã—ãŸã€‚

```C++
// Read a point from a DDR memory
template <typename T>
void ReadPointNaive(const float* point_cloud,
                    const int idx,
                    T x[3])
{
#pragma HLS INLINE off

  for (int i = 0; i < 3; ++i) {
#pragma HLS PIPELINE II=1
    x[i] = T(point_cloud[idx * 3 + i]);
  }
}

// Read a point from a DDR memory
template <typename T>
void ReadPointOpt1(const ap_uint<64>* point_cloud,
                   const int idx,
                   T x[3])
{
#pragma HLS INLINE off

  const ap_uint<64> point_data0 = point_cloud[idx * 2 + 0];
  const ap_uint<64> point_data1 = point_cloud[idx * 2 + 1];
  x[0] = T(U32ToFloat(point_data0.range(31, 0)));
  x[1] = T(U32ToFloat(point_data0.range(63, 32)));
  x[2] = T(U32ToFloat(point_data1.range(31, 0)));
}
```

`ReadPointNaive`ã§ã¯ã€DRAMãƒãƒƒãƒ•ã‚¡`point_cloud`ã®ã‚µã‚¤ã‚ºãŒ$(N, 3)$ã§ã‚ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã—ãŸã€‚
ä¸€æ–¹`ReadPointOpt1`ã§ã¯ã€å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºãŒ$(N, 4)$ã§ã‚ã‚‹ã¨ã—ã¾ã™ (4ç•ªç›®ã®æ¬¡å…ƒã«ã¤ã„ã¦ã¯ä½¿ã‚ãªã„)ã€‚
`i`ç•ªç›®ã®ç‚¹ã‚’èª­ã¿å–ã‚‹ã¨ãã¯ã€ãƒãƒƒãƒ•ã‚¡ã®`idx * 2 + 0`ç•ªç›®ã¨`idx * 2 + 1`ç•ªç›®ã‚’å‚ç…§ã™ã‚Œã°ã‚ˆã„ã§ã™ã€‚

æœ€å¾Œã«ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–ã‚’ç›´ã—ã¾ã™ (`InferenceClsOpt1`ã‚’å‚ç…§)ã€‚
ç‚¹ç¾¤ã®ç‰¹å¾´é‡ã‹ã‚‰ã€ç‰©ä½“ã®å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—ã—ã€`WriteTensor1dNaive`ã«ã‚ˆã‚ŠDRAMãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã‚“ã§ã„ã¾ã™ã€‚
`WriteTensor1dNaive`ã‚’ã€64ãƒ“ãƒƒãƒˆå¹…ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¾ã™ã€‚
ä¿®æ­£å¾Œã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’`WriteTensor1dOpt1`ã¨ã—ã¾ã—ãŸã€‚

```C++
// Write a 1D tensor to a DDR memory
template <typename T, int D0>
void WriteTensor1dNaive(float* dst,
                        const T tensor[D0],
                        const int offset)
{
#pragma HLS INLINE off

  for (int i = 0; i < D0; ++i) {
#pragma HLS PIPELINE II=1
    dst[offset + i] = static_cast<float>(tensor[i]);
  }
}

// Write a 1D tensor to a DDR memory
template <typename T, int D0>
void WriteTensor1dOpt1(ap_uint<64>* dst,
                       const T tensor[D0],
                       const int offset)
{
#pragma HLS INLINE off

  static_assert(D0 % 2 == 0, "`D0` must be a multiple of 2");
  assert(offset % 2 == 0);

  constexpr const int D0Over2 = D0 / 2;
  const int offset2 = offset / 2;

  for (int i = 0; i < D0Over2; ++i) {
#pragma HLS PIPELINE II=1
    ap_uint<64> tensor_data;
    tensor_data.range(31, 0) = FloatToU32(
      static_cast<float>(tensor[i * 2 + 0]));
    tensor_data.range(63, 32) = FloatToU32(
      static_cast<float>(tensor[i * 2 + 1]));
    dst[offset2 + i] = tensor_data;
  }
}
```

ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡`tensor`ã«ç½®ã‹ã‚ŒãŸã‚µã‚¤ã‚º`(D0)`ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€1ã‚µã‚¤ã‚¯ãƒ«ã«2ã¤ãšã¤ã€DRAMã«æ›¸ãæˆ»ã—ã¦ã„ã¾ã™ã€‚
å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€`D0`ã¯å¶æ•°ã§ã‚ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚
2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã¯`T`å‹ã§ã™ãŒã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰åˆ©ç”¨ã—ã‚„ã™ã„ã‚ˆã†ã«`float`ã«ç›´ã—ã€æ›´ã«`FloatToU32`ã‚’ä½¿ã£ã¦ã€ãƒ“ãƒƒãƒˆè¡¨ç¾ã‚’ç¶­æŒã—ãŸã¾ã¾32ãƒ“ãƒƒãƒˆã®ç¬¦å·ãªã—æ•´æ•°å‹ã«å†è§£é‡ˆã—ã¦ã„ã¾ã™ã€‚
ã“ã‚Œã‚‰2ã¤ã‚’ã€`ap_uint<64>`å‹ã®ä¸Šä½32ãƒ“ãƒƒãƒˆã¨ä¸‹ä½32ãƒ“ãƒƒãƒˆã«è©°ã‚ã¦ã€DRAMãƒãƒƒãƒ•ã‚¡ã«æ›¸ãæˆ»ã—ã¦ã„ã¾ã™ã€‚

æœ€åˆã®2ã¤ã®å…¨çµåˆå±¤ (`LinearOpt1DDR`) ã‚‚ç›´ã—ã¦ã€æ–°ãŸã«`LinearOpt2DDR`ã‚’ä½œã‚Šã¾ã™ã€‚
é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®è»¢é€éƒ¨åˆ†ã‚’å¤‰æ›´ã—ã¾ã™ã€‚
è»¢é€ã«è¦ã™ã‚‹ã‚µã‚¤ã‚¯ãƒ«æ•°ãŒåŠåˆ†ã»ã©ã«ãªã‚‹ã®ã§ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã®å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚
å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹ãŸã‚ã€å…¥å‡ºåŠ›ã®æ¬¡å…ƒãŒã„ãšã‚Œã‚‚å¶æ•°ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
```C++
// Parallel implementation of the fully-connected layer
// Weight and bias parameters are stored on the DDR memory
// Matrix-vector multiplication is parallelized along the output dimension
// `T` is the type for values
// `TParam` is the type for weight and bias
// `InDims` is the number of input dimensions
// `OutDims` is the number of output dimensions
// `ApplyReLU` is the flag to apply ReLU activation
// `B` is the block size for the output dimension
template <typename T, typename TParam,
          int InDims, int OutDims, bool ApplyReLU, int B>
void LinearOpt2DDR(const T x[InDims],
                   T y[OutDims],
                   const ap_uint<64>* params,
                   const int offset)
{
  // `x` is of size (1, `InDims`)
  // `y` is of size (1, `OutDims`)
  // `params` contains weight parameters of size (`OutDims`, `InDims`) and
  // bias parameters of size (`OutDims`) in a contiguous buffer

#pragma HLS INLINE off

  // `OutDims` must be a multiple of `B`
  static_assert(OutDims % B == 0, "`OutDims` must be a multiple of `B`");
  // `B` must be larger than 1
  static_assert(B > 1, "`B` must be larger than 1");
  // `InDims` must be a multiple of 2
  static_assert(InDims % 2 == 0, "`InDims` must be a multiple of 2");
  // `OutDims` must be a multiple of 2
  static_assert(OutDims % 2 == 0, "`OutDims` must be a multiple of 2");
  // `offset` must be a multiple of 2
  assert(offset % 2 == 0);

  constexpr const int BHalf = B / 2;
  constexpr const int OffsetToBias = OutDims * InDims / 2;
  constexpr const int InDims2 = InDims / 2;
  constexpr const int OutDims2 = OutDims / 2;
  const int offset2 = offset / 2;

  TParam bias[OutDims];
#pragma HLS ARRAY_PARTITION variable=bias type=cyclic factor=BHalf dim=1

  // Copy the bias parameters in advance
  for (int i = 0; i < OutDims2; ++i) {
#pragma HLS PIPELINE II=1
    const ap_uint<64> bias_data = params[offset2 + OffsetToBias + i];
    bias[i * 2 + 0] = TParam(U32ToFloat(bias_data.range(31, 0)));
    bias[i * 2 + 1] = TParam(U32ToFloat(bias_data.range(63, 32)));
  }

  for (int i0 = 0; i0 < OutDims; i0 += B) {
#pragma HLS PIPELINE off
    T vals[B];
#pragma HLS ARRAY_PARTITION variable=vals type=complete dim=1
    TParam weight[B][InDims];
#pragma HLS ARRAY_PARTITION variable=weight type=cyclic factor=BHalf dim=1

    // Copy the weight parameters for `B` outputs
    const int offset0 = offset2 + i0 * InDims2;
    for (int i1 = 0; i1 < B; ++i1) {
      for (int j = 0; j < InDims2; ++j) {
#pragma HLS PIPELINE
        const ap_uint<64> weight_data = params[offset0 + i1 * InDims2 + j];
        weight[i1][j * 2 + 0] = TParam(
          U32ToFloat(weight_data.range(31, 0)));
        weight[i1][j * 2 + 1] = TParam(
          U32ToFloat(weight_data.range(63, 32)));
      }
    }

    for (int j = 0; j < InDims; ++j) {
#pragma HLS PIPELINE
      for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
        int i = i0 + i1;
        if (i < OutDims) {
          T last = (j == 0) ? T(bias[i]) : vals[i1];
          vals[i1] = last + x[j] * weight[i1][j];
        }
      }
    }

    for (int i1 = 0; i1 < B; ++i1) {
#pragma HLS UNROLL
      int i = i0 + i1;
      if (i < OutDims) {
        if (ApplyReLU)
          y[i] = vals[i1] > T(0) ? vals[i1] : T(0);
        else
          y[i] = vals[i1];
      }
    }
  }
}
```

2ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®å…¥å‡ºåŠ›ã«é–¢é€£ã™ã‚‹éƒ¨åˆ†ã‚’ä¿®æ­£ã—ã¾ã—ãŸã€‚
`InferenceFeatOpt2`ã¨`InferenceClsOpt1`ã«å¯¾ã—ã¦ã€ä¿®æ­£ã‚’æ–½ã—ãŸã‚‚ã®ã‚’`InferenceFeatOpt3`ã€`InferenceClsOpt3`ã¨ã—ã¾ã™ã€‚
`InferenceFeatOpt3`ã§ã¯ã€ç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚‹éš›ã«ã€`ReadPointNaive`ã®ä»£ã‚ã‚Šã«`ReadPointOpt1`ã‚’ä½¿ã£ã¦ã„ã¾ã™ (ä»–ã¯åŒã˜)ã€‚
ã¾ãŸ`InferenceClsOpt3`ã§ã¯ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’æ›¸ãè¾¼ã‚€éš›ã«ã€`WriteTensor1dNaive`ã§ã¯ãªã`WriteTensor1dOpt1`ã‚’ä½¿ã„ã€æœ€åˆã®2ã¤ã®å…¨çµåˆå±¤ã«ã¤ã„ã¦ã¯ã€`LinearOpt1DDR`ã®ä»£ã‚ã‚Šã«`LinearOpt2DDR`ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚
```C++
template <typename T, typename U, int N>
void InferenceFeatOpt3(...)
{
#pragma HLS INLINE off

  // Zero-initialize the output feature
  VectorNdSetZero<T, kFeatDims5>(feature);

  // Compute the feature
  for (int i = 0; i < num_points; ++i) {
    // ...

    // Read a point from a DDR memory
    ReadPointOpt1<T>(point_cloud, i, x0);

    // Compute a point feature
    // ...

    // Update the output feature
    MaxPool1dOpt1<T, kFeatDims5, 2>(x10, feature);
  }
}

template <typename T, typename U>
void InferenceClsOpt3(...)
{
#pragma HLS INLINE off

  // ...

  // Compute logits
  LinearOpt2DDR<T, U, kClsDims0, kClsDims1, false, 16>(
    feature, x0, params1, 0);
  BatchNorm1dReLUOpt1<T, U, kClsDims1, 2>(
    x0, x1, bn1->scale, bn1->bias, bn1->mean);
  LinearOpt2DDR<T, U, kClsDims1, kClsDims2, false, 8>(
    x1, x2, params2, 0);
  BatchNorm1dReLUOpt1<T, U, kClsDims2, 2>(
    x2, x3, bn2->scale, bn2->bias, bn2->mean);
  LinearOpt1<T, U, kClsDims2, kClsDims3, false, 2>(
    x3, x4, fc3->weight, fc3->bias);

  // Write the result
  WriteTensor1dOpt1<T, kClsDims3>(out_logits, x4, 0);
}
```

å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆå¹…ã«ã‚ˆã£ã¦ã€ã©ã®ç¨‹åº¦å®Ÿè¡Œæ™‚é–“ã‚’å‰Šæ¸›ã§ããŸã§ã—ã‚‡ã†ã‹ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯`InferenceFeatOpt2`ã®å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã¯1,112,259 (7.408ms)ã€æ–°ãŸã«ç”¨æ„ã—ãŸ`InferenceFeatOpt3`ã¯1,112,254 (7.408ms) ã§ã—ãŸã€‚
ã»ã¼ä¸€ç·’ã§ã™ã€‚
åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é–¢ã—ã¦ã¯ã€ãƒãƒ¼ãƒˆå¹…32ãƒ“ãƒƒãƒˆç”¨ã®`InferenceClsOpt1`ã¯711,969ã‚µã‚¤ã‚¯ãƒ« (4.742ms) ã§ã—ãŸãŒã€64ãƒ“ãƒƒãƒˆç”¨ã®`InferenceClsOpt3`ã§ã¯383,885ã‚µã‚¤ã‚¯ãƒ« (2.557ms) ã«å‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚
ãƒãƒ¼ãƒˆå¹…ã‚’2å€ã«åºƒã’ãŸã“ã¨ã§ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¨è«–æ™‚é–“ã‚’1.85å€çŸ­ç¸®ã§ããŸã‚ã‘ã§ã™ã€‚

å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£… (`InferenceFeatNaive` + `InferenceClsNaive`) ã¨ã€ã“ã“ã«ç¤ºã™å®Ÿè£… (`InferenceFeatOpt3` + `InferenceClsOpt3`) ã¨ã§ã€å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•°ã¯ã©ã®ç¨‹åº¦å¤‰åŒ–ã—ãŸã§ã—ã‚‡ã†ã‹ã€‚
ä¸ŠãŒãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã€ä¸‹ãŒæœ€é©åŒ–æ¸ˆã¿ã®å®Ÿè£…ã§ã®çµæœã§ã™ã€‚
ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã§ã¯ã€æ¨è«–ã«163,279,213ã‚µã‚¤ã‚¯ãƒ« (1.087s) è¦ã—ã¦ã„ã¾ã™ãŒã€æœ€é©åŒ–ã«ã‚ˆã£ã¦1,496,143ã‚µã‚¤ã‚¯ãƒ« (9.964ms) ã«ã¾ã§å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãŠã‚ˆã109å€ã®å·®ã§ã™ã­ã€‚

[<img src="point-cloud-classification-images/pointnet-naive-clock-cycles.png" width="80%" />](point-cloud-classification-images/pointnet-naive-clock-cycles.png)

[<img src="point-cloud-classification-images/pointnet-opt3-clock-cycles.png" width="80%" />](point-cloud-classification-images/pointnet-opt3-clock-cycles.png)

ä»¥ä¸Šã§ã€é«˜ä½åˆæˆã®å®Ÿè£…ãŒã§ãã‚ãŒã‚Šã¾ã—ãŸã€‚
`hls/src/top_opt3.cpp`ã‚’ã”è¦§ãã ã•ã„ã€‚

## ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æº–å‚™

é«˜ä½åˆæˆã®å®Ÿè£…ãŒã§ããŸã®ã§ã€Vitis HLSã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€IPã‚³ã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚
ä»Šå›ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç’°å¢ƒã§ä½œæ¥­ã—ã¦ã„ã¾ã™ (è©¦ã™äººã¯ã„ãªã„ã¨æ€ã„ã¾ã™ãŒæ›¸ã„ã¦ãŠãã¾ã™)ã€‚

- Ubuntu 20.04.5 LTS
- Intel(R) Xeon(R) E-2186G CPU @ 3.80GHz
- 64GB DRAM
- Vivado ML Edition 2022.1 (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å ´æ‰€ã¯`/tools/Xilinx`ä»¥ä¸‹)
- CMake 3.16.3

ã¾ãŸã€å¯¾è±¡ã®FPGAãƒœãƒ¼ãƒ‰ã¯ã€Xilinx ZCU104 Evaluation Board (XCZU7EV-2FFVC1156)ã§ã™ã€‚

ä»Šå›ç”¨æ„ã—ãŸGitHubãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«`make`ã™ã‚‹ã ã‘ã§ã€è‡ªå‹•çš„ã«IPã‚³ã‚¢ã‚’ä½œæˆã§ãã¾ã™ã€‚
Tclã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨CMakeã‚’çµ„ã¿åˆã‚ã›ã¦å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚
ä¸Šã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ã«ã€Vitis HLSã«ã¯GUIãŒç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ãŒã€Tclã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ãˆã°ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä¸Šã§ã®ãƒãƒƒãƒå‡¦ç†ãŒå¯èƒ½ã§ã™ã€‚
é©å½“ãªå ´æ‰€ã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ãŸã‚‰ã€`hls`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»ã£ã¦ã€ä½œæ¥­ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™ã—ã¾ã™ã€‚
ç¶šã„ã¦CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆã—ã€æ‰€æœ›ã®IPã‚³ã‚¢ã‚’`make`ã§ä½œæˆã—ã¾ã™ã€‚

```
# äºˆã‚Vivadoã¨Vitis HLSã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«sourceã™ã‚‹
> source /tools/Xilinx/Vivado/2022.1/settings64.sh

# GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
> git clone git@github.com:sterngerlach/advent_2022_point_cloud_classification.git
> cd advent_2022_point_cloud_classification

# ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
> cd hls
> mkdir build
> mkdir work

> cd build

# CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆ
# settings64.shã«ã‚ˆã£ã¦CMakeãŒæ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã®CMakeã‚’ä½¿ã†
> /usr/bin/cmake ..

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã‹ã‚‰IPã‚³ã‚¢ã‚’ä½œæˆ
# workãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ä½œã‚‰ã‚Œã‚‹
> make pointnet_naive_150_csynth_export

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) IPã‚³ã‚¢ã‚’ä½œæˆ
> make pointnet_opt1_csynth_export

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸIPã‚³ã‚¢ã‚’ä½œæˆ
> make pointnet_opt2_csynth_export

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸIPã‚³ã‚¢ã‚’ä½œæˆ
> make pointnet_opt3_csynth_export
```

IPã‚³ã‚¢ã‚’ä½œæˆã—ãŸã‚‰ã€GUIã‚’èµ·å‹•ã—ã¦ã€åˆæˆçµæœã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã† (ä¸Šã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ãªç”»é¢ãŒé–‹ãã¾ã™)ã€‚

```
> cd hls/work

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ç”¨ã®Vitis HLSãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’GUIã§é–‹ã
> vitis_hls -p pointnet_naive_150

# ä»–ã‚‚åŒæ§˜
> vitis_hls -p pointnet_opt1
> vitis_hls -p pointnet_opt2
> vitis_hls -p pointnet_opt3
```

Vitis HLSã‚’ä½¿ã†ã®ã¯ã“ã“ã¾ã§ã§ã€ã“ã‚Œä»¥é™ã¯ã€Vivadoã‚’ä½¿ã£ãŸä½œæ¥­ã«ç§»ã‚Šã¾ã™ã€‚
ç¶šã„ã¦ã€ã“ã®IPã‚³ã‚¢ã‚’ã€åˆ¥ã®IPã‚³ã‚¢ã¨çµ„ã¿åˆã‚ã›ã¦ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ç”¨æ„ã—ã¾ã™ã€‚
ä»Šå›ã¯ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã®ä½œæˆã«ã¤ã„ã¦ã¯çœç•¥ã—ã¾ã™ã€‚
æœ€åˆã«ã€`vivado`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»ã£ã¦ã€ä½œæ¥­ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™ã—ã¾ã™ã€‚
ç¶šã„ã¦CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆã—ã€æ‰€æœ›ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’`make`ã§ä½œæˆã—ã¾ã™ã€‚

```
# ä½œæ¥­ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
> cd vivado
> mkdir build
> mkdir work
> mkdir bitstream

> cd build

# CMakeãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹æˆ
# settings64.shã«ã‚ˆã£ã¦CMakeãŒæ›¸ãæ›ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã®CMakeã‚’ä½¿ã†
# Vitis HLSã«ã‚ˆã‚‹IPã‚³ã‚¢ã®åˆæˆãŒçµ‚ã‚ã£ã¦ã„ãªã„ã¨ã‚¨ãƒ©ãƒ¼
> /usr/bin/cmake ..

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã®IPã‚³ã‚¢ã‹ã‚‰ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆ
> make pointnet_naive_150_create

# æœ€é©åŒ–æ¸ˆã¿ã®IPã‚³ã‚¢ã‹ã‚‰ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆ
> make pointnet_opt1_create
> make pointnet_opt2_create
> make pointnet_opt3_create
```

ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆã—ãŸã‚‰ã€GUIã‚’èµ·å‹•ã—ã¦ã€ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```
> cd vivado/work
> vivado -project pointnet_naive_150/pointnet_naive_150.xpr
> vivado -project pointnet_opt1/pointnet_opt1.xpr
> vivado -project pointnet_opt2/pointnet_opt2.xpr
> vivado -project pointnet_opt3/pointnet_opt3.xpr
```

[<img src="point-cloud-classification-images/pointnet-opt3-vivado.png" width="80%" />](point-cloud-classification-images/pointnet-opt3-vivado.png)

å·¦å´ã®Flow Navigatorã‹ã‚‰ã€ã€ŒOpen Block Designã€ã‚’é¸æŠã™ã‚‹ã¨ã€ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-opt3-vivado2.png" width="80%" />](point-cloud-classification-images/pointnet-opt3-vivado2.png)

ãƒ–ãƒ­ãƒƒã‚¯å›³ã‚’æ‹¡å¤§ã—ãŸã‚‚ã®ãŒä»¥ä¸‹ã§ã™ã€‚

[<img src="point-cloud-classification-images/board-design.svg" width="100%" />](point-cloud-classification-images/board-design.svg)

ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã«å¯¾ã—ã¦ã€è«–ç†åˆæˆã¨é…ç½®é…ç·šã‚’è¡Œã„ã€å›è·¯æƒ…å ±ã‚’ã¾ã¨ã‚ãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ  (Bitstream) ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚
ãƒã‚·ãƒ³ã®ã‚¹ãƒšãƒƒã‚¯ã«ã‚‚ã‚ˆã‚Šã¾ã™ãŒã€ã“ã¡ã‚‰ã®ç’°å¢ƒã§ã¯ã€1ã¤ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã®è«–ç†åˆæˆã¨é…ç½®é…ç·šã«ã€30åˆ†ä»¥ä¸Šæ›ã‹ã‚Šã¾ã—ãŸ (8ã‚³ã‚¢ã‚’ä½¿ã£ãŸå ´åˆ)ã€‚
ä»Šå›ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã¯ã€ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚‚å…¥ã‚Œã¦ã‚ã‚‹ã®ã§ã€ã“ã®ä½œæ¥­ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ (è©¦ã—ã¦ã¿ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™)ã€‚

```
> cd vivado/build
> make pointnet_naive_150_impl && make pointnet_naive_150_copy_bitstream
> make pointnet_opt1_impl && make pointnet_opt1_copy_bitstream
> make pointnet_opt2_impl && make pointnet_opt2_copy_bitstream
> make pointnet_opt3_impl && make pointnet_opt3_copy_bitstream
```

ã‚‚ã†ä¸€åº¦GUIã‚’èµ·å‹•ã—ã¦ã€åˆæˆæ¸ˆã¿ã®å›è·¯ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
å·¦å´ã®Flow Navigatorã‹ã‚‰ã€ã€ŒOpen Implemented Designã€ã‚’é¸æŠã—ã¾ã™ã€‚
å€‹äººçš„ã«ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã®ãƒãƒ³ãƒãƒƒã‚¿ãƒ³ã®ã‚ˆã†ã«ã¿ãˆã¦ã€ç¾ã—ã„ã¨æ€ã„ã¾ã™ã€‚
GUIä¸Šã§ã€ãƒªã‚½ãƒ¼ã‚¹ã®ä½¿ç”¨ç‡ (Utilization) ã‚„ã€é›»åŠ›æ¶ˆè²»ã®è¦‹ç©ã‚‚ã‚Š (Power)ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚° (Timing) ãªã©ã‚’ç¢ºèªã§ãã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-opt3-vivado3.png" width="80%" />](point-cloud-classification-images/pointnet-opt3-vivado3.png)

`vivado/bitstream`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã«ã€ç”Ÿæˆã•ã‚ŒãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒã‚³ãƒ”ãƒ¼ã•ã‚Œã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ  (æ‹¡å¼µå­`.bit`) ã®ä»–ã«ã€Hardware Handoffãƒ•ã‚¡ã‚¤ãƒ« (æ‹¡å¼µå­`.hwh`) ã‚‚ã‚ã‚Šã¾ã™ã€‚
Handoffãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€å›è·¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¾ã™ã€‚
FPGAãƒœãƒ¼ãƒ‰ã«ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ã¯ã€2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚»ãƒƒãƒˆã§å¿…è¦ã«ãªã‚Šã¾ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’èª­ã¿ç›´ã›ã°ã€å‹•ã‹ã™å›è·¯ã‚’ä½•åº¦ã§ã‚‚åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã¨ã„ã†ã®ãŒã€ASICã«å¯¾ã™ã‚‹FPGAã®å¤§ããªåˆ©ç‚¹ã§ã™ã€‚
ã•ã¦ã€ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’`scp`ãªã©ã§FPGAãƒœãƒ¼ãƒ‰ä¸Šã«è»¢é€ã™ã‚Œã°ã€å›è·¯ã‚’å‹•ã‹ã™æº–å‚™ãŒæ•´ã„ã¾ã™ã€‚

```
> cd vivado/bitstream
> ls
-rw-rw-r-- 1 x x  19M Dec 14 23:34 pointnet_naive_150.bit
-rw-rw-r-- 1 x x 363K Dec 14 23:34 pointnet_naive_150.hwh
-rw-rw-r-- 1 x x  19M Dec 15 00:01 pointnet_opt1.bit
-rw-rw-r-- 1 x x 363K Dec 15 00:01 pointnet_opt1.hwh
-rw-rw-r-- 1 x x  19M Dec 14 23:20 pointnet_opt2.bit
-rw-rw-r-- 1 x x 363K Dec 14 23:20 pointnet_opt2.hwh
-rw-rw-r-- 1 x x  19M Dec 15 18:07 pointnet_opt3.bit
-rw-rw-r-- 1 x x 363K Dec 15 18:07 pointnet_opt3.hwh
```

## å›è·¯ã‚’å‹•ã‹ã™

ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ç”¨æ„ã§ããŸã®ã§ã€ã„ã‚ˆã„ã‚ˆå›è·¯ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚
ä»Šå›ä½¿ç”¨ã™ã‚‹FPGAãƒœãƒ¼ãƒ‰ã€Xilinx ZCU104 Evaluation Kitã¯ã€SoC (System-on-Chip) ã¨ã‚ˆã°ã‚Œã¦ã„ã¾ã™ã€‚
FPGAã®ä»–ã«ã€ã‚¯ã‚¢ãƒƒãƒ‰ã‚³ã‚¢ ARM Cortex-A53 CPU (1.2GHz)ã€2GBã®DRAMã‚„ã€æ§˜ã€…ãªå‘¨è¾ºå›è·¯ãŒçµ±åˆã•ã‚Œã¦ã„ã¦ã€LinuxãŒå‹•ä½œã—ã¾ã™ã€‚
ã“ã“ã§ã¯OSã¨ã—ã¦ã€Ubuntu 20.04ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸPynq Linux 2.7ã‚’ä½¿ã„ã¾ã™ã€‚
Pynq Linuxã«ã¯`pynq`ã¨ã‚ˆã°ã‚Œã‚‹Pythonã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä»˜å±ã—ã¦ãŠã‚Šã€Pythonã‹ã‚‰FPGAé–¢é€£ã®å‡¦ç†ã‚’ç°¡å˜ã«è¡Œãˆã¾ã™ã€‚

ä»¥ä¸‹ã‚’è©¦ã™ãŸã‚ã«ã¯ã€Pynq Linuxä¸Šã«ã€PyTorch 1.11.0ã‚„ã€TorchVision 0.12.0ã€NumPyã€SciPyã€H5pyã€Tqdmãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’äºˆã‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ã“ã“ã§ã¯èª¬æ˜ãŒé•·ããªã£ã¦ã—ã¾ã†ãŸã‚å‰²æ„›ã—ã¾ã™ã€‚
åŸºæœ¬çš„ã«ã¯`pip`ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚
ãªãŠã€Xilinx ZCU104ã€Pynq Linux 2.7ç”¨ã«ãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸPyTorch 1.11.0ã€TorchVision 0.12.0ã®Wheelãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€[ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/sterngerlach/pytorch-pynq-builds)ã«ç½®ã„ã¦ã‚ã‚Šã¾ã™ã€‚
ã“ã“ã¾ã§è‹¦åŠ´ã—ã¦ã€ãªãœFPGAä¸Šã§æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ãã†ã¨ã™ã‚‹ã®ã‹ã€ãŸã¾ã«è‡ªå•è‡ªç­”ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ã“ã‚Œä»¥é™ã¯C/C++ã§ã¯ãªãã€Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ãã¾ã™ã€‚

æœ€åˆã«ã€PyTorchã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã‚’å†æ²ã—ã¾ã™ (`net/model.py`)ã€‚
ä½•ã®æ»ã‚Šã‚‚ãªãã€ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã­ã€‚
```Python
class PointNetFeat(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = torch.nn.Conv1d(3, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 64, 1)
        self.conv3 = torch.nn.Conv1d(64, 64, 1)
        self.conv4 = torch.nn.Conv1d(64, 128, 1)
        self.conv5 = torch.nn.Conv1d(128, 1024, 1)
        self.bn1 = torch.nn.BatchNorm1d(64)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.bn3 = torch.nn.BatchNorm1d(64)
        self.bn4 = torch.nn.BatchNorm1d(128)
        self.bn5 = torch.nn.BatchNorm1d(1024)

    def forward(self, x: torch.Tensor):
        # `x` is of size [B, N, 3]
        N = x.shape[1]
        # `x` is of size [B, 3, N]
        x = x.transpose(1, 2)

        # `x` is of size [B, 1024, N]
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))

        # `x` is of size [B, 1024]
        x = torch.max(x, dim=2)[0]

        return x

class PointNetCls(torch.nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()

        # Feature extraction
        self.feat = PointNetFeat()

        # Classification network
        self.fc1 = torch.nn.Linear(1024, 512)
        self.fc2 = torch.nn.Linear(512, 256)
        self.fc3 = torch.nn.Linear(256, num_classes)
        self.bn1 = torch.nn.BatchNorm1d(512)
        self.bn2 = torch.nn.BatchNorm1d(256)

    def forward(self, x):
        # `x` is of size [B, N, 3]
        # `x` is of size [B, 1024]
        x = self.feat(x)

        # `x` is of size [B, `num_classes`]
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)

        return x
```

æ¬¡ã«ã€FPGAã§é«˜é€ŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç¤ºã—ã¾ã™ (`host/model_zcu104.py`)ã€‚
ãƒ¢ãƒ‡ãƒ«ã®åå‰ã¯`PointNetClsZCU104`ã§ã™ã€‚
ä¸Šè¨˜ã®CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ« (`PointNetCls`) ã¨ã€ä½¿ã„å‹æ‰‹ãŒåŒã˜ã«ãªã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚
```Python
from net.model import PointNetCls

# Split the 64-bit address
def split_address(addr: int) -> Tuple[int, int]:
    mask = (1 << 32) - 1
    return addr & mask, addr >> 32

# Allocate a contiguous buffer for torch.nn.Conv1d (torch.nn.Linear)
def allocate_linear_buffer(in_dims: int, out_dims: int) \
    -> pynq.buffer.PynqBuffer:
    buf_size = in_dims * out_dims + out_dims
    return pynq.allocate(shape=(buf_size,), dtype=np.float32, cacheable=False)

# Allocate a contiguous buffer for a block with torch.nn.Conv1d
# (torch.nn.Linear) and torch.nn.BatchNorm1d
def allocate_block_buffer(in_dims: int, out_dims: int) \
    -> pynq.buffer.PynqBuffer:
    buf_size = 0
    buf_size += in_dims * out_dims + out_dims
    buf_size += out_dims * 3
    return pynq.allocate(shape=(buf_size,), dtype=np.float32, cacheable=False)

# Write the torch.nn.Conv1d parameters to the contiguous buffer
def write_conv1d_params(buf: pynq.buffer.PynqBuffer,
                        layer: torch.nn.Conv1d,
                        offset: int = 0) -> int:
    if layer.kernel_size != (1,):
        raise RuntimeError(f"Kernel size should be 1")

    weight_size = layer.out_channels * layer.in_channels
    bias_size = layer.out_channels

    buf[offset:offset+weight_size] = layer.weight.data.view(-1)
    offset += weight_size
    buf[offset:offset+bias_size] = layer.bias.data.view(-1)
    offset += bias_size

    return offset

# Write the torch.nn.Linear parameters to the contiguous buffer
def write_linear_params(buf: pynq.buffer.PynqBuffer,
                        layer: torch.nn.Linear,
                        offset: int = 0) -> int:
    weight_size = layer.out_features * layer.in_features
    bias_size = layer.out_features

    buf[offset:offset+weight_size] = layer.weight.data.view(-1)
    offset += weight_size
    buf[offset:offset+bias_size] = layer.bias.data.view(-1)
    offset += bias_size

    return offset

# Write the torch.nn.BatchNorm1d parameters to the contiguous buffer
def write_batchnorm1d_params(buf: pynq.buffer.PynqBuffer,
                             layer: torch.nn.BatchNorm1d,
                             offset: int = 0) -> int:
    dims = layer.num_features

    # `scale` is the multiplication of the weight and reciprocal of the
    # standard deviation (to reduce the on-chip memory consumption)
    std_inv = torch.sqrt(layer.running_var.data + layer.eps)
    std_inv = torch.reciprocal(std_inv)
    scale = std_inv * layer.weight.data

    buf[offset:offset+dims] = scale.data.view(-1)
    offset += dims
    buf[offset:offset+dims] = layer.bias.data.view(-1)
    offset += dims
    buf[offset:offset+dims] = layer.running_mean.data.view(-1)
    offset += dims

    return offset

# Write the block (torch.nn.Conv1d and torch.nn.BatchNorm1d) parameters
# to the contiguous buffer
def write_conv_batchnorm1d_params(buf: pynq.buffer.PynqBuffer,
                                  conv: torch.nn.Conv1d,
                                  bn: torch.nn.BatchNorm1d):
    offset = 0
    offset = write_conv1d_params(buf, conv, offset)
    offset = write_batchnorm1d_params(buf, bn, offset)

# Write the block (torch.nn.Linear and torch.nn.BatchNorm1d) parameters
# to the contiguous buffer
def write_linear_batchnorm1d_params(buf: pynq.buffer.PynqBuffer,
                                    linear: torch.nn.Linear,
                                    bn: torch.nn.BatchNorm1d):
    offset = 0
    offset = write_linear_params(buf, linear, offset)
    offset = write_batchnorm1d_params(buf, bn, offset)

class PointNetClsZCU104(torch.nn.Module):
    # Operation modes (refer to hls/src/op_modes.hpp)
    MODE_INIT_WEIGHTS = 100
    MODE_INFERENCE = 101

    def __init__(self, model_cpu: PointNetCls,
                 overlay_path: str, num_points: int):
        super().__init__()

        # Load an overlay
        self.overlay = self.load_overlay(overlay_path)
        # Get the IP core module
        self.net_ip: pynq.DefaultIP = self.overlay.PointNetClsTop
        # Get the control registers of the IP core
        self.registers = self.net_ip.register_map

        # Check the data width of the AXI master interface
        net_ip_params = self.overlay.ip_dict["PointNetClsTop"]["parameters"]
        self.axi_m_addr_width = int(net_ip_params["C_M_AXI_GMEM0_ADDR_WIDTH"])
        self.axi_m_data_width = int(net_ip_params["C_M_AXI_GMEM0_DATA_WIDTH"])

        # Allocate buffers for PointNet feature extraction network
        self.buf_feat_params1 = allocate_block_buffer(3, 64)
        self.buf_feat_params2 = allocate_block_buffer(64, 64)
        self.buf_feat_params3 = allocate_block_buffer(64, 64)
        self.buf_feat_params4 = allocate_block_buffer(64, 128)
        self.buf_feat_params5 = allocate_block_buffer(128, 1024)

        # Allocate buffers for classification network
        self.buf_cls_params1 = allocate_block_buffer(1024, 512)
        self.buf_cls_params2 = allocate_block_buffer(512, 256)
        self.buf_cls_params3 = allocate_linear_buffer(256, 40)

        # Allocate a buffer for point cloud
        self.num_points = num_points
        if self.axi_m_data_width == 32:
            self.buf_point_cloud: pynq.buffer.PynqBuffer = pynq.allocate(
                shape=(self.num_points, 3), dtype=np.float32, cacheable=False)
        elif self.axi_m_data_width == 64:
            self.buf_point_cloud: pynq.buffer.PynqBuffer = pynq.allocate(
                shape=(self.num_points, 4), dtype=np.float32, cacheable=False)
        else:
            raise RuntimeError(f"Unexpected data width for AXI master")

        # Allocate a buffer for output logits
        self.buf_out_logits: pynq.buffer.PynqBuffer = pynq.allocate(
            shape=(40,), dtype=np.float32, cacheable=False)

        # Copy parameters for PointNet feature extraction network
        write_conv_batchnorm1d_params(self.buf_feat_params1,
            model_cpu.feat.conv1, model_cpu.feat.bn1)
        write_conv_batchnorm1d_params(self.buf_feat_params2,
            model_cpu.feat.conv2, model_cpu.feat.bn2)
        write_conv_batchnorm1d_params(self.buf_feat_params3,
            model_cpu.feat.conv3, model_cpu.feat.bn3)
        write_conv_batchnorm1d_params(self.buf_feat_params4,
            model_cpu.feat.conv4, model_cpu.feat.bn4)
        write_conv_batchnorm1d_params(self.buf_feat_params5,
            model_cpu.feat.conv5, model_cpu.feat.bn5)

        # Copy parameters for classification network
        write_linear_batchnorm1d_params(self.buf_cls_params1,
            model_cpu.fc1, model_cpu.bn1)
        write_linear_batchnorm1d_params(self.buf_cls_params2,
            model_cpu.fc2, model_cpu.bn2)
        write_linear_params(self.buf_cls_params3, model_cpu.fc3)

        # Set the physical addresses of the buffers
        self.registers.point_cloud_1, self.registers.point_cloud_2 = \
            split_address(self.buf_point_cloud.device_address)
        self.registers.out_logits_1, self.registers.out_logits_2 = \
            split_address(self.buf_out_logits.device_address)
        self.registers.feat_params1_1, self.registers.feat_params1_2 = \
            split_address(self.buf_feat_params1.device_address)
        self.registers.feat_params2_1, self.registers.feat_params2_2 = \
            split_address(self.buf_feat_params2.device_address)
        self.registers.feat_params3_1, self.registers.feat_params3_2 = \
            split_address(self.buf_feat_params3.device_address)
        self.registers.feat_params4_1, self.registers.feat_params4_2 = \
            split_address(self.buf_feat_params4.device_address)
        self.registers.feat_params5_1, self.registers.feat_params5_2 = \
            split_address(self.buf_feat_params5.device_address)
        self.registers.cls_params1_1, self.registers.cls_params1_2 = \
            split_address(self.buf_cls_params1.device_address)
        self.registers.cls_params2_1, self.registers.cls_params2_2 = \
            split_address(self.buf_cls_params2.device_address)
        self.registers.cls_params3_1, self.registers.cls_params3_2 = \
            split_address(self.buf_cls_params3.device_address)

        # Synchronize the buffers
        self.buf_feat_params1.sync_to_device()
        self.buf_feat_params2.sync_to_device()
        self.buf_feat_params3.sync_to_device()
        self.buf_feat_params4.sync_to_device()
        self.buf_feat_params5.sync_to_device()
        self.buf_cls_params1.sync_to_device()
        self.buf_cls_params2.sync_to_device()
        self.buf_cls_params3.sync_to_device()

        # Initialize the weights (transfer the weights to the on-chip buffers)
        self.registers.op_mode = PointNetClsZCU104.MODE_INIT_WEIGHTS
        self.registers.CTRL.AP_START = 1
        self.wait_for_ip()

    def load_overlay(self, overlay_path):
        overlay = pynq.Overlay(overlay_path)

        if not overlay.is_loaded():
            raise RuntimeError(f"Unable to load overlay: {overlay_path}")

        return overlay

    def wait_for_ip(self):
        while self.registers.CTRL.AP_DONE == 0:
            pass

    def forward(self, x: torch.Tensor):
        # `x` is of size [B, N, 3]
        if x.ndim != 3 or x.shape[2] != 3:
            raise RuntimeError(f"Unexpected shape of the input: {x.shape}")

        batch_size = x.shape[0]
        num_points = x.shape[1]

        # Reallocate the buffer for point cloud if necessary
        if num_points > self.num_points:
            self.num_points = num_points
            self.buf_point_cloud.freebuffer()
            if self.axi_m_data_width == 32:
                self.buf_point_cloud: pynq.buffer.PynqBuffer = pynq.allocate(
                    shape=(self.num_points, 3),
                    dtype=np.float32, cacheable=False)
            elif self.axi_m_data_width == 64:
                self.buf_point_cloud: pynq.buffer.PynqBuffer = pynq.allocate(
                    shape=(self.num_points, 4),
                    dtype=np.float32, cacheable=False)
            else:
                raise RuntimeError(f"Unexpected data width for AXI master")
            self.registers.point_cloud_1, self.registers.point_cloud_2 = \
                split_address(self.buf_point_cloud.device_address)

        # Allocate the Tensor for output
        out = torch.empty(size=(batch_size, 40),
                          dtype=x.dtype, device=x.device)

        # Run the inference
        self.registers.op_mode = PointNetClsZCU104.MODE_INFERENCE
        self.registers.num_points = num_points

        for i in range(batch_size):
            # Copy the input point cloud
            self.buf_point_cloud[:num_points, :3] = x[i].view(-1, 3)
            self.buf_point_cloud.sync_to_device()

            # Run the inference
            self.registers.CTRL.AP_START = 1
            self.wait_for_ip()

            # Copy the output logits
            self.buf_out_logits.sync_from_device()
            out[i, :] = torch.from_numpy(self.buf_out_logits)

        return out
```

### IPã‚³ã‚¢ã®åˆæœŸåŒ–

`PointNetClsZCU104`ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§åˆæœŸåŒ–ã—ã€IPã‚³ã‚¢ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
ã“ã®æ‰‹é †ã§è¡Œã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
å„æ‰‹é †ã«ã¤ã„ã¦ã€é †ç•ªã«èª¬æ˜ã—ã¾ã™ã€‚
è©³ã—ãã¯ã€[Pynqã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pynq.readthedocs.io/en/latest/)ã‚’ã”è¦§ãã ã•ã„ã€‚

1. ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒ­ãƒ¼ãƒ‰ (`load_overlay`)
2. DRAMãƒãƒƒãƒ•ã‚¡ã®ç¢ºä¿ (`allocate_block_buffer`ã€`pynq.allocate`)
3. DRAMãƒãƒƒãƒ•ã‚¡ã¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ (`write_conv_batchnorm1d_params`ã€`write_linear_batchnorm1d_params`ã€`write_linear_params`)
4. DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ã€ãƒãƒ¼ãƒˆã®ãƒ¬ã‚¸ã‚¹ã‚¿ã«å¯¾ã—ã¦è¨­å®š
5. DRAMãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’åŒæœŸ (`sync_to_device`)
6. é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã§ã€IPã‚³ã‚¢ã‚’å‹•ä½œã•ã›ã€DRAMãƒãƒƒãƒ•ã‚¡ã«ç½®ã‹ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ä¸Šã«ã‚³ãƒ”ãƒ¼
7. IPã‚³ã‚¢ã®å‹•ä½œçµ‚äº†ã‚’å¾…æ©Ÿ (`wait_for_ip`)

ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¯`pynq.Overlay`ã§ã‚ã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¸ãˆã¦ã€æŒ‡å®šã—ãŸãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
æ‹¡å¼µå­ãŒ`.bit`ã®ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ä»–ã«ã€`.hwh`ã®Handoffãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒ`path/to/X.bit`ã§ã‚ã‚Œã°ã€å¯¾å¿œã™ã‚‹HandoffãŒ`path/to/X.hwh`ã«ãªã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼ã¨ãªã‚Šã¾ã™ã€‚
`pynq.Overlay`ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹`self.overlay`ã‚’èµ·ç‚¹ã¨ã—ã¦ã€FPGAã«å¯¾ã™ã‚‹æ§˜ã€…ãªå‡¦ç†ã‚’è¡Œã£ã¦ã„ãã¾ã™ã€‚

ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ (ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã€è‡ªä½œã®IPã‚³ã‚¢`PointNetClsTop`ã‚’å–ã‚Šå‡ºã—ã¦ã€`self.net_ip`ã«æ ¼ç´ã—ã¾ã™ã€‚
IPã‚³ã‚¢ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£åã¯ã€ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã«ãŠã‘ã‚‹å„IPã®åå‰ã¨å¯¾å¿œã—ã¦ã„ã¾ã™ ([ã“ã¡ã‚‰ã®ç”»åƒ](point-cloud-classification-images/board-design.svg)ã‚’å‚ç…§ã€‚)
ä¾‹ãˆã°ã€å‰²è¾¼ã¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ© (AXI Interrupt Controller) ã«ã¯ã€`axi_intc_0`ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é€šã˜ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚
IPã‚³ã‚¢ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯`pynq.DefaultIP`ã¨ãªã£ã¦ã„ã¾ã™ã€‚
ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ã€è‡ªä½œã®IPã‚³ã‚¢ã‚’ã‚ˆã‚Šä¾¿åˆ©ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã€æ§˜ã€…ãªãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
ã•ã‚‰ã«ã€IPã‚³ã‚¢ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹`register_map` (`pynq.registers.RegisterMap`ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹) ã‚’å–ã‚Šå‡ºã—ã¦ã€`self.registers`ã«æ ¼ç´ã—ã¾ã™ã€‚

æ¬¡ã®3è¡Œã§ã€IPã‚³ã‚¢ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®ã‚¢ãƒ‰ãƒ¬ã‚¹å¹…ã¨ãƒ‡ãƒ¼ã‚¿å¹…ã‚’èª¿ã¹ã¦ã€`self.axi_m_addr_width`ãŠã‚ˆã³`self.axi_m_data_width`ã«æ ¼ç´ã—ã¾ã™ã€‚
å‰è€…ã¯64ã€å¾Œè€…ã¯32ã¾ãŸã¯64ã§ã™ (å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å‹ã‚’`ap_uint<64>*`ã¨ã—ãŸå ´åˆã¯64ã€`float*`ã®ã¾ã¾ã§ã‚ã‚Œã°32)ã€‚
å‰è¿°ã®é€šã‚Šã€ãƒãƒ¼ãƒˆå¹…ãŒ32ãƒ“ãƒƒãƒˆã§ã‚ã‚Œã°ã€ç‚¹ç¾¤ãƒãƒƒãƒ•ã‚¡ã®ã‚µã‚¤ã‚ºã¯$(N, 3)$ã§ã‚ˆã„ã®ã§ã™ãŒã€64ãƒ“ãƒƒãƒˆã®å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ãšã¤èª­ã¿å–ã‚‹é–¢ä¿‚ä¸Šã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’$(N, 4)$ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
`self.axi_m_data_width`ã‚’å‚ç…§ã™ã‚Œã°ã€ç‚¹ç¾¤ãƒãƒƒãƒ•ã‚¡ã®ã‚µã‚¤ã‚ºã‚’æ±ºå®šã§ãã¾ã™ã€‚

ç¶šã„ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å…¥å‡ºåŠ›ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã®ãƒãƒƒãƒ•ã‚¡ã¯å°‘ã—ç‰¹æ®Šãªã‚‚ã®ã§ã€Linuxã‚«ãƒ¼ãƒãƒ«ã®CMA (Contiguous Memory Allocator) ã¨ã„ã†æ©Ÿèƒ½ã«ã‚ˆã‚Šç¢ºä¿ã•ã‚Œã¾ã™ã€‚
é€šå¸¸ã®`malloc()`ã‚„`new`ã‚’ä½¿ã£ã¦ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã™ã‚‹ã¨ã€ãã®ãƒãƒƒãƒ•ã‚¡ã¸ã®ä»®æƒ³ã‚¢ãƒ‰ãƒ¬ã‚¹ã—ã‹åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚
ä¸€æ–¹ã€FPGAå´ã‹ã‚‰ã¯ã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒ•ã‚¡ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã®ã§ã€ä»®æƒ³ã‚¢ãƒ‰ãƒ¬ã‚¹ã ã‘ã§ãªãã€ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚‚äºˆã‚çŸ¥ã£ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

`allocate_linear_buffer`é–¢æ•°ã¯ã€ãã®åã®é€šã‚Šã€å…¨çµåˆå±¤ (å…¥åŠ›æ¬¡å…ƒ`in_dims`ã€å‡ºåŠ›æ¬¡å…ƒ`out_dims`) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
æœ€åˆã«ã€å…¨çµåˆå±¤ã®é‡ã¿ (`in_dims * out_dims`) ã¨ãƒã‚¤ã‚¢ã‚¹ (`out_dims`) ã®è¦ç´ æ•°ã‚’è¶³ã—ã¦ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’æ±ºå®šã—ã¾ã™ã€‚
ç¶šã„ã¦ã€`pynq.allocate`é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã€æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºãŠã‚ˆã³ãƒ‡ãƒ¼ã‚¿å‹`np.float32` (`float`) ã®ã€1æ¬¡å…ƒã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã®ãƒãƒƒãƒ•ã‚¡ã¯DRAMã®ç‰¹æ®Šãªé ˜åŸŸã«ç½®ã‹ã‚Œã¦ã€ãƒ¡ãƒ¢ãƒªä¸Šã§é€£ç¶šã—ã¦ã„ã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã¾ã™ã€‚
`allocate_block_buffer`é–¢æ•°ã¯ã€å…¨çµåˆå±¤ã¨ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¦ç´ æ•°ã‚’è¶³ã—åˆã‚ã›ã¦ã‚µã‚¤ã‚ºã‚’æ±ºå®šã—ã€`pynq.allocate`é–¢æ•°ã‚’ä½¿ã£ã¦ã€1æ¬¡å…ƒã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒãƒƒãƒ•ã‚¡ã¯`pynq.buffer.PynqBuffer`ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã™ãŒã€NumPyé…åˆ— (`np.ndarray`) ã¨åŒã˜ã‚ˆã†ã«åˆ©ç”¨ã§ãã¾ã™ã€‚
ä¾‹ãˆã°ã€`torch.from_numpy`é–¢æ•°ã«ã‚ˆã‚Šã€PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã§ãã¾ã™ã€‚

ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (`buf_feat_params1`ã‹ã‚‰`buf_feat_params5`) ã¨ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (`buf_cls_params1`ã‹ã‚‰`buf_cls_params3`) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
ãã®å¾Œã€å…¥åŠ› (ç‚¹ç¾¤) ã¨å‡ºåŠ› (ãƒ­ã‚¸ãƒƒãƒˆ) ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã‚‚ç¢ºä¿ã—ã¾ã™ã€‚
å…¥åŠ›ã«ã¤ã„ã¦ã¯ä¸Šè¿°ã®é€šã‚Šã€ãƒãƒ¼ãƒˆã®ãƒ“ãƒƒãƒˆå¹…ãŒ64ã§ã‚ã‚Œã°`(self.num_points, 4)`ã€32ã§ã‚ã‚Œã°`(self.num_points, 3)`ã¨ã—ã¾ã™ã€‚

DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—çµ‚ãˆãŸã‚‰ã€æ¬¡ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã¸ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã¯`PointNetCls`ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã€ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®å¼•æ•°`model_cpu`ã¨ã—ã¦æ¸¡ã•ã‚Œã¾ã™ã€‚
`write_conv1d_params`ã€`write_linear_params`ã¯ã€ãã‚Œãã‚Œ`torch.nn.Conv1d`ã€`torch.nn.Linear`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã«ä½¿ã‚ã‚Œã¾ã™ã€‚
`write_conv1d_params`ã§ã¯ã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºãŒ1ã§ã‚ã‚‹ (ãã‚Œã‚†ãˆå…¨çµåˆå±¤`torch.nn.Linear`ã¨å‹•ä½œãŒåŒã˜ã§ã‚ã‚‹) ã“ã¨ã‚’å‰æã¨ã—ã¾ã™ã€‚
é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®é †ã§ã€æŒ‡å®šã•ã‚ŒãŸ1æ¬¡å…ƒã®DRAMãƒãƒƒãƒ•ã‚¡ã«ä¸¦ã¹ã¦ã‚†ãã¾ã™ã€‚
IPã‚³ã‚¢å´ã®æœŸå¾…é€šã‚Šã«ãƒ‡ãƒ¼ã‚¿ãŒé…ç½®ã•ã‚Œã‚‹ã‚ˆã†ã«ã€ç´°å¿ƒã®æ³¨æ„ã‚’æ‰•ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‚‰2ã¤ã®é–¢æ•°ã¯ã€é«˜ä½åˆæˆã®å®Ÿè£…ã«ãŠã‘ã‚‹ã€`ReadLinearParamsNaive`ã‚„`ReadLinearParamsOpt1`ã¨é©åˆã™ã‚‹ã‚ˆã†ã«ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

`write_batchnorm1d_params`ã¯ã€`torch.nn.BatchNorm1d`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
IPã‚³ã‚¢å´ã§ã¯ã€`ReadBatchNorm1dParamsNaive`ã‚„`ReadBatchNorm1dParamsOpt1`ã«ç¤ºã™ã‚ˆã†ã«ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã®é †ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸¦ã¶ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®åˆ†æ•£ã¨é‡ã¿ã‹ã‚‰ã€ã‚¹ã‚±ãƒ¼ãƒ«ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™ (è¨ˆç®—å¼ã«ã¤ã„ã¦ã¯å…ˆè¿°)ã€‚

`write_conv_batchnorm1d_params`ã¨`write_linear_batchnorm1d_params`ã¯ã€å…¨çµåˆå±¤ (`torch.nn.Conv1d`ã€`torch.nn.Linear`) ã¨ãƒãƒƒãƒæ­£è¦åŒ–å±¤ (`torch.nn.BatchNorm1d`) ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€æŒ‡å®šã•ã‚ŒãŸDRAMãƒãƒƒãƒ•ã‚¡ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
å…¨çµåˆå±¤ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã€ãã‚Œã‹ã‚‰ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ã€ãƒã‚¤ã‚¢ã‚¹ã€å¹³å‡ã‚’ã€ã“ã®é †ã§ä¸¦ã¹ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
IPã‚³ã‚¢å´ã®`ReadBlockParamsNaive`ã€`ReadBlockParamsOpt1`ã€`ReadBlockParamsOpt2`ã¨å¯¾å¿œã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã§ã™ãŒã€ãã®ã¾ã¾DRAMãƒãƒƒãƒ•ã‚¡ (`pynq.buffer.PynqBuffer`) ã«ä»£å…¥ã§ãã¾ã™ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç„¡äº‹ã«ã‚³ãƒ”ãƒ¼ã§ããŸã®ã§ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚
IPã‚³ã‚¢ã®ãƒˆãƒƒãƒ—é–¢æ•°`PointNetClsTop`ã¯æ¬¡ã®ã‚ˆã†ã«å®£è¨€ã•ã‚Œã¦ã„ã¾ã—ãŸ (`float*`ã®ä»£ã‚ã‚Šã«`ap_uint<64>*`ã‚‚ã‚ã‚Š)ã€‚
```C++
void PointNetClsTop(const int op_mode,
                    const float* point_cloud,
                    const int num_points,
                    float* out_logits,
                    const float* feat_params1,
                    const float* feat_params2,
                    const float* feat_params3,
                    const float* feat_params4,
                    const float* feat_params5,
                    const float* cls_params1,
                    const float* cls_params2,
                    const float* cls_params3)
{
#pragma HLS INTERFACE m_axi port=point_cloud offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=out_logits offset=slave bundle=gmem0
#pragma HLS INTERFACE m_axi port=feat_params1 offset=slave bundle=gmem0
// ...
#pragma HLS INTERFACE m_axi port=cls_params3 offset=slave bundle=gmem0

#pragma HLS INTERFACE s_axilite port=op_mode bundle=control
#pragma HLS INTERFACE s_axilite port=point_cloud bundle=control
#pragma HLS INTERFACE s_axilite port=num_points bundle=control
#pragma HLS INTERFACE s_axilite port=out_logits bundle=control
#pragma HLS INTERFACE s_axilite port=feat_params1 bundle=control
// ...
#pragma HLS INTERFACE s_axilite port=cls_params3 bundle=control
#pragma HLS INTERFACE s_axilite port=return bundle=control
}
```

`op_mode`ã¨`num_points`ã‚’é™¤ãã€DRAMãƒãƒƒãƒ•ã‚¡ç”¨ã®å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã€`#pragma HLS INTERFACE m_axi`ã¨`#pragma HLS INTERFACE s_axilite`ã®è¨˜è¿°ãŒã¿ã‚‰ã‚Œã¾ã™ã€‚
ã“ã®2ã¤ã®HLSãƒ—ãƒ©ã‚°ãƒã‚’ä»˜ä¸ã™ã‚‹ã¨ã€å„ãƒãƒ¼ãƒˆã«å¯¾ã—ã¦ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒä½œæˆã•ã‚Œã¾ã™ã€‚
ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯64ãƒ“ãƒƒãƒˆã§ã™ãŒã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã®ãƒ‡ãƒ¼ã‚¿å¹…ã¯32ãƒ“ãƒƒãƒˆãªã®ã§ã€ä¸Šä½32ãƒ“ãƒƒãƒˆã¨ä¸‹ä½32ãƒ“ãƒƒãƒˆç”¨ã«ã€2ã¤ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚
ä¾‹ãˆã°ã€`point_cloud`ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã¯ã€`point_cloud_1` (ä¸‹ä½32ãƒ“ãƒƒãƒˆ) ã¨ã€`point_cloud_2` (ä¸Šä½32ãƒ“ãƒƒãƒˆ) ã®ã€2ã¤ã§ã™ã€‚
DRAMãƒãƒƒãƒ•ã‚¡ã®ç‰©ç†ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨­å®šã™ã‚Œã°ã€ãƒãƒ¼ãƒˆã¨DRAMãƒãƒƒãƒ•ã‚¡ã¨ãŒç´ã¥ã‘ã‚‰ã‚Œã€FPGAå´ã‹ã‚‰ãƒãƒƒãƒ•ã‚¡ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã¨ã€æ™®é€šã«å€¤ã‚’ä»£å…¥ã—ã¦ã„ã‚‹ã‚ˆã†ã«ã¿ãˆã¾ã™ãŒã€å®Ÿéš›ã«ã¯ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/Oã§å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚
è¨€ã„æ›ãˆã‚‹ã¨ã€å„åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã«ã¯å°‚ç”¨ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ãŠã‚Šã€ãã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã«å¯¾ã—ã¦èª­ã¿æ›¸ãã—ã¦ã„ã¾ã™ã€‚
åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«ã¯ã€å…ˆã»ã©ã®`self.registers`ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

`op_mode`ã¨`num_points`ã«ã¤ã„ã¦ã‚‚ã€`#pragma HLS INTERFACE s_axilite`ã®è¨˜è¿°ãŒã‚ã‚‹ã®ã§ã€ã“ã®2ã¤ (å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã¨ç‚¹ã®å€‹æ•°) ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚

ã“ã“ã¾ã§æ¸ˆã‚“ã ã‚‰ã€`sync_to_device`ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚ŠDRAMãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’åŒæœŸã•ã›ã¦ã€FPGAå´ã‹ã‚‰æ­£ã—ãèª­ã‚ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

æœ€å¾Œã«ã€å‹•ä½œãƒ¢ãƒ¼ãƒ‰`op_mode`ã‚’**é‡ã¿åˆæœŸåŒ–**ã«è¨­å®šã—ã€åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ã®ã†ã¡`CTRL.AP_START`ã‚’1ã«ã™ã‚‹ã“ã¨ã§ã€IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ã¾ã™ã€‚
é‡ã¿åˆæœŸåŒ–ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€DRAMãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿å‡ºã—ã¦ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ã—ã¾ã™ã€‚
`#pragma HLS INTERFACE s_axilite port=return bundle=control`ã®è¨˜è¿°ãŒã‚ã‚‹ãŠã‹ã’ã§ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰IPã‚³ã‚¢ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®`CTRL`ãƒ¬ã‚¸ã‚¹ã‚¿ãŒç”¨æ„ã•ã‚Œã¾ã™ã€‚
IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ãŸã‚‰ã€`wait_for_ip`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã‚“ã§ã€å‹•ä½œçµ‚äº† (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è»¢é€å®Œäº†) ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚
`wait_for_ip`ãƒ¡ã‚½ãƒƒãƒ‰å†…ã§ã¯ã€`CTRL`ãƒ¬ã‚¸ã‚¹ã‚¿ã®`AP_DONE`ãŒ1ã«ãªã‚‹ã¾ã§ã€ãƒ“ã‚¸ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã—ã¾ã™ã€‚
ä»¥ä¸Šã§åˆæœŸåŒ–ãŒãŠã—ã¾ã„ã§ã™ã€‚

### æ¨è«–

åˆæœŸåŒ–ã«ã¯æ§˜ã€…ãªå·¥ç¨‹ãŒã‚ã£ã¦é¢å€’ã§ã™ãŒã€æ¨è«–ã¯æ¯”è¼ƒçš„ç°¡å˜ã§ã™ã€‚
PyTorchã®é€šå¸¸ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨åŒã˜ãã€`forward`ãƒ¡ã‚½ãƒƒãƒ‰ã«æ¨è«–å‡¦ç†ã‚’è¨˜è¿°ã—ã¾ã™ã€‚
å…¥åŠ›ç‚¹ç¾¤`x`ã¯ã€ã‚µã‚¤ã‚ºãŒ$(B, N, 3)$ã®ãƒãƒƒãƒã§ã‚ã‚‹ã¨ã—ã¾ã™ ($B$ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã€$N$ã¯ç‚¹ã®å€‹æ•°)ã€‚
ä»Šå›ã®IPã‚³ã‚¢ã¯ã€ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚ˆã†ã«ã¯ä½œã£ã¦ã„ãªã„ã®ã§ã€ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã‚’1ã¤ãšã¤å‡¦ç†ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
å‡ºåŠ›`out`ã¯ã€ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹æ•°ã‚’$K$ã¨ã™ã‚‹ã¨ã€ã‚µã‚¤ã‚ºãŒ$(B, K)$ã¨ãªã‚Šã¾ã™ã€‚
ä»Šå›ã¯ModelNet40ã¨ã‚ˆã°ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã†ã®ã§ã€ã‚¯ãƒ©ã‚¹æ•°ã¯$K = 40$ã§ã™ã€‚

æœ€åˆã«ã€ç‚¹ç¾¤ã®ã‚µã‚¤ã‚º$N$ãŒã€ç‚¹ç¾¤ç”¨ã«ç¢ºä¿ã—ã¦ã‚ã‚‹ç¾åœ¨ã®DRAMãƒãƒƒãƒ•ã‚¡ã‚ˆã‚Šã‚‚å¤§ãã‘ã‚Œã°ã€DRAMãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿ã—ç›´ã—ã¾ã™ã€‚
ç¶šã„ã¦ã€ãƒãƒƒãƒå†…ã®å„ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦æ¨è«–å‡¦ç†ã‚’è¡Œã£ã¦ã€ç‰©ä½“ã®å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ­ã‚¸ãƒƒãƒˆ (ã‚¹ã‚³ã‚¢) ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ç‚¹ç¾¤ç”¨ã®DRAMãƒãƒƒãƒ•ã‚¡`buf_point_cloud`ã«ç‚¹ç¾¤ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€FPGAå´ã‹ã‚‰æ­£ã—ãèª­ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ã€ãƒãƒƒãƒ•ã‚¡ã‚’åŒæœŸã—ã¾ã™ã€‚
ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å´ã‹ã‚‰ã¯ã€å…¥å‡ºåŠ›ãƒãƒ¼ãƒˆã®å¹… (32ã‹64ã‹ã©ã†ã‹) ã¯ãã‚Œã»ã©æ„è­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã›ã‚“ã€‚
2ã¤ã®åˆ¶å¾¡ãƒ¬ã‚¸ã‚¹ã‚¿ (å‹•ä½œãƒ¢ãƒ¼ãƒ‰`op_mode`ã¨ç‚¹ã®å€‹æ•°`num_points`) ã¯ã€äºˆã‚è¨­å®šã—ã¦ãŠãã¾ã™ã€‚

`CTRL`ãƒ¬ã‚¸ã‚¹ã‚¿ã®`AP_START`ã‚’1ã«ã™ã‚‹ã“ã¨ã§ã€**æ¨è«–**ãƒ¢ãƒ¼ãƒ‰ã§ã®IPã‚³ã‚¢ã®å‹•ä½œã‚’é–‹å§‹ã—ã¾ã™ã€‚
`wait_for_ip`ãƒ¡ã‚½ãƒƒãƒ‰ã«ã‚ˆã‚Šå‹•ä½œã®çµ‚äº†ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã§ã‚ã‚‹ãƒ­ã‚¸ãƒƒãƒˆã¯ã€IPã‚³ã‚¢å´ã‹ã‚‰DRAMãƒãƒƒãƒ•ã‚¡`buf_out_logits`ã«æ›¸ãè¾¼ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ãŸã†ãˆã§ã€å‡ºåŠ›ç”¨ã®ãƒ†ãƒ³ã‚½ãƒ«`out`ã«æ”¹ã‚ã¦æ›¸ãè¾¼ã¿ã¾ã™ã€‚
ä»¥ä¸ŠãŒæ¨è«–å‡¦ç†ã®èª¬æ˜ã§ã—ãŸã€‚

ã“ã®ã‚ˆã†ã«ã€IPã‚³ã‚¢ã®å®Ÿè£…ã ã‘ã§ãªãã€ãã‚Œã‚’å®Ÿéš›ã«ä½¿ã†ãŸã‚ã®ãƒ‰ãƒ©ã‚¤ãƒã‚‚ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã€æ‰‹é–“ãŒæ›ã‹ã‚Šã¾ã™ã­ã€‚
ä»Šå›ã¯ã€Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸã®ã§ã€FPGAã«é–¢ã™ã‚‹å‡¦ç†ã¯ã€æ¯”è¼ƒçš„å®¹æ˜“ã«è¨˜è¿°ã§ãã¾ã—ãŸã€‚
ã¾ãŸã€CPUãƒ»GPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã‚ˆã†ã«ä½¿ã„ãŸã„ã®ã§ã€PyTorchã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (`torch.nn.Module`) ã¨ã—ã¦ãƒ‰ãƒ©ã‚¤ãƒã‚’ä½œæˆã—ã¾ã—ãŸã€‚
Pythonã®ä»£ã‚ã‚Šã«C++ã‚’ä½¿ã†ã“ã¨ã‚‚ã€ã‚‚ã¡ã‚ã‚“å¯èƒ½ã§ã™ã€‚
ãã®å ´åˆã¯ã€ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ãƒ­ãƒ¼ãƒ‰ ([ä¾‹ãˆã°ã“ã¡ã‚‰](https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/bitstream_loader.cpp))ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒˆI/Oã®æº–å‚™ ([ä¾‹ãˆã°ã“ã¡ã‚‰](https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/mmio.cpp))ã€DRAMãƒãƒƒãƒ•ã‚¡ã®ç¢ºä¿ ([ä¾‹ãˆã°ã“ã¡ã‚‰](https://github.com/sterngerlach/my-lidar-graph-slam-v2/blob/b271f4f13050f2f7aced3feb3c37253f287ee006/src/my_lidar_graph_slam/hw/cma_memory.cpp))ãªã©ã‚’ã€C++ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ (Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãã®ã¾ã¾ç§»æ¤ã—ãŸã®ã‚’è¦šãˆã¦ã„ã¾ã™)ã€‚

# è©•ä¾¡

## æ¨è«–æ™‚é–“ã®æ¯”è¼ƒ

ã‚ˆã†ã‚„ãã€IPã‚³ã‚¢ã‚’ä½¿ã£ãŸè©•ä¾¡ã«å…¥ã‚Šã¾ã—ãŸã€‚
æœ€åˆã«ã€æ¨è«–æ™‚é–“ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ©ç”¨ã—ã¾ã™ (`host/time_zcu104.py`)ã€‚
```Python
def main():
    # Parse the command-line arguments
    args = parse_command_line()

    # Create a PointNet classification model
    model = PointNetCls(num_classes=40)
    # Create an FPGA model
    model_zcu104 = PointNetClsZCU104(model, args.bitstream, args.num_points)

    model.eval()
    model_zcu104.eval()

    # Test the output
    # Create a random input point cloud
    point_cloud = torch.rand(size=(1, args.num_points, 3))
    out_cpu = model(point_cloud)
    out_zcu104 = model_zcu104(point_cloud)

    print(f"Output (CPU):\n{out_cpu}")
    print(f"Output (FPGA):\n{out_zcu104}")

    # Measure the inference times
    times_cpu = []
    times_zcu104 = []

    for _ in range(args.runs):
        # Create a random input point cloud
        point_cloud = torch.rand(size=(1, args.num_points, 3))

        t0 = time.monotonic()
        model(point_cloud)
        elapsed_cpu = (time.monotonic() - t0) * 1e3

        t0 = time.monotonic()
        model_zcu104(point_cloud)
        elapsed_zcu104 = (time.monotonic() - t0) * 1e3

        times_cpu.append(elapsed_cpu)
        times_zcu104.append(elapsed_zcu104)

    time_avg_cpu = np.mean(times_cpu)
    time_std_cpu = np.std(times_cpu)
    time_avg_zcu104 = np.mean(times_zcu104)
    time_std_zcu104 = np.std(times_zcu104)
    speedup_factor = time_avg_cpu / time_avg_zcu104

    print(f"Inference time (CPU): " \
          f"mean: {time_avg_cpu:.3f}ms, " \
          f"std: {time_std_cpu:.3f}ms")
    print(f"Inference time (FPGA): " \
          f"mean: {time_avg_zcu104:.3f}ms, " \
          f"std: {time_std_zcu104:.3f}ms")
    print(f"Speedup: {speedup_factor:.3f}x")
```

ã“ã“ã§ã¯ç²¾åº¦ã¯æ°—ã«ã—ãªã„ã®ã§ã€å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ã¯çœã‹ã‚Œã¦ã„ã¾ã™ã€‚
ä½†ã—ã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«`PointNetCls`ã¨ã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«`PointNetClsZCU104`ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æƒãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã™ã€‚
ã¾ãŸã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¯`eval`ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã•ã›ã¾ã™ã€‚
ãƒãƒƒãƒæ­£è¦åŒ–å±¤ã®æŒ™å‹•ãŒè¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«ãªã‚Šã€ãƒãƒƒãƒæ•°ãŒ1ã®ã¨ãã«ã‚¨ãƒ©ãƒ¼ã¨ãªã‚Šã¾ã™ã€‚
ã¾ãŸã€è¨“ç·´æ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ãªãã€å…¥åŠ›ã®ãƒãƒƒãƒã‹ã‚‰å¹³å‡ã‚„æ¨™æº–åå·®ãŒè¨ˆç®—ã•ã‚Œã‚‹ã®ã§ã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã¨å‡ºåŠ›çµæœãŒåˆã‚ãªããªã‚Šã¾ã™ã€‚
æŒ‡å®šã•ã‚ŒãŸå›æ•°`args.runs`ã ã‘ã€æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬ã‚’è¡Œã„ã€å¹³å‡ã¨æ¨™æº–åå·®ã€ã¾ãŸé«˜é€ŸåŒ–ç‡ã‚’ç®—å‡ºã—ã¾ã™ã€‚
ã¾ãŸæœ€åˆã«ã€åŒæ–¹ã®ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒåˆã£ã¦ã„ã‚‹ã‹ã©ã†ã‹ (å¤§ä½“è¿‘ã„å€¤ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã‹) ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ (æœ¬å½“ã¯ã€IPã‚³ã‚¢ã®ä½œæˆæ™‚ã«ã‚‚ãƒ†ã‚¹ãƒˆã—ã¾ã™)ã€‚

FPGAãƒœãƒ¼ãƒ‰ä¸Šã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
```
> cd advent_2022_point_cloud_classification/host

# ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_naive_150.bit

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt2.bit

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_opt3.bit
```

ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã—ãŸå ´åˆã®å‡ºåŠ›ä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚
```
$ sudo XILINX_XRT=/usr ./time_zcu104.sh ../vivado/bitstream/pointnet_naive_150.bit
Output (CPU):
tensor([[-0.0594, -0.0272,  0.0115, -0.0481, -0.0529,  0.0449, -0.0634, -0.0328,
          0.0348, -0.0071, -0.0228,  0.0412,  0.0128, -0.0175, -0.0086, -0.0023,
         -0.0192, -0.0101, -0.0072,  0.0520, -0.0106, -0.0110,  0.0113,  0.0499,
         -0.0563, -0.0523, -0.0711, -0.0104, -0.0048, -0.0404,  0.0375,  0.0089,
          0.0326, -0.0408, -0.0302, -0.0041,  0.0534, -0.0349,  0.0380, -0.0020]],
       grad_fn=<AddmmBackward0>)
Output (FPGA):
tensor([[-0.0592, -0.0274,  0.0114, -0.0491, -0.0527,  0.0446, -0.0632, -0.0335,
          0.0337, -0.0071, -0.0258,  0.0399,  0.0119, -0.0170, -0.0091, -0.0030,
         -0.0216, -0.0112, -0.0106,  0.0522, -0.0111, -0.0130,  0.0114,  0.0487,
         -0.0571, -0.0523, -0.0714, -0.0103, -0.0058, -0.0389,  0.0383,  0.0068,
          0.0306, -0.0421, -0.0314, -0.0052,  0.0539, -0.0360,  0.0399, -0.0031]])
Inference time (CPU): mean: 369.048ms, std: 1.086ms
Inference time (FPGA): mean: 1071.358ms, std: 0.023ms
Speedup: 0.344x
```

CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã¯`float`ã‚’ä½¿ã„ã¾ã™ãŒã€FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å›ºå®šå°æ•°ç‚¹æ•° (`ap_fixed`) ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å…¥åŠ›ã‚’ä¸ãˆã¦ã‚‚ã€å‡ºåŠ›çµæœã«ã¯å¤šå°‘ã®ãšã‚ŒãŒç”Ÿã˜ã¾ã™ (ã“ã“ã§ã¯ã€å›ºå®šå°æ•°ç‚¹æ•°ã®ãƒ“ãƒƒãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã€æ•´æ•°éƒ¨ã‚’16ãƒ“ãƒƒãƒˆã€å°æ•°éƒ¨ã‚’16ãƒ“ãƒƒãƒˆã«è¨­å®šã—ã¦ã„ã¾ã™)ã€‚
ã—ã‹ã—ã€CPUç‰ˆã¨FPGAç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã§ã€å¤§ä½“ä¼¼ãŸã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ (å°æ•°ç¬¬2ä½ãã‚‰ã„ã¾ã§ã¯åˆã£ã¦ã„ã¾ã™)ã€‚
ã‚¯ãƒ©ã‚¹åˆ†é¡å•é¡Œã§ã‚ã‚Œã°ã€ã“ã‚Œã§å•é¡Œãªã„ã¨æ€ã„ã¾ã™ã€‚
æ¨è«–æ™‚é–“ã‚’ã¿ã‚‹ã¨ã€ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã§ã¯ã€CPUç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚3å€ç¨‹åº¦é…ã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

å„å®Ÿè£…ã«å¯¾ã™ã‚‹æ¨è«–æ™‚é–“ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

| å®Ÿè£… | æ¨è«–æ™‚é–“ã®å¹³å‡ (ms) | æ¨™æº–åå·® (ms) | é«˜é€ŸåŒ–ç‡ (ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢æ¯”) | é«˜é€ŸåŒ–ç‡ (ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…æ¯”) |
| :-- | :-- | :-- | :-- | :-- |
| CPUç‰ˆ | 369.0 | 1.086 | **1.0x** | 2.904x |
| ãƒŠã‚¤ãƒ¼ãƒ– (100MHz) | 1606.4 | 0.041 | 0.230x | 0.667x |
| ãƒŠã‚¤ãƒ¼ãƒ– (150MHz) | 1071.4 | 0.023 | 0.344x | **1.0x** |
| ãƒŠã‚¤ãƒ¼ãƒ– (200MHz) | 872.05 | 0.077 | 0.423x | 1.223x |
| ãƒŠã‚¤ãƒ¼ãƒ– (250MHz) | 665.33 | 0.073 | 0.555x | 1.610x |
| ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz) | 34.60 | 0.027 | 10.66x | 30.97x |
| ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz) | 12.93 | 0.016 | 28.54x | 82.86x |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz) | **10.80** | **0.012** | **34.17x** | **99.20x** |

ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£… (150MHz) ã¯ã€CPUã«æ¯”ã¹ã¦æ€§èƒ½ãŒãŸã£ãŸã®0.344å€ã§ã—ãŸã€‚
ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã®ã¾ã¾ã§ã¯ã€å‹•ä½œå‘¨æ³¢æ•°ã‚’250MHzã¾ã§ä¸Šã’ã¦ã‚‚ã€ä¾ç„¶ã¨ã—ã¦CPUã‚ˆã‚Šã‚‚é…ã„ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã®åˆ©ç”¨ã«ã‚ˆã£ã¦ã€æ¨è«–æ™‚é–“ã¯30.97å€ã‚‚çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦10.66å€é«˜é€Ÿã«ãªã‚Šã¾ã—ãŸã€‚
Vitis HLSã«ã‚ˆã‚Šå‡ºåŠ›ã•ã‚ŒãŸã‚¯ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’ã¿ã‚‹ã¨ã€ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£… (150MHz) ã§ã¯161,945,604 (1.079s)ã€ä¸¦åˆ—åŒ–å¾Œã®å®Ÿè£…ã§ã¯4,462,596 (29.72ms)ã¨ãªã£ã¦ã„ã¾ã™ã€‚
å®Ÿéš›ã«ã¯ã€å‰è€…ã¯1.071sã€å¾Œè€…ã¯34.60msãªã®ã§ã€å¤§ä½“åˆã£ã¦ã„ã‚‹ã¨ã„ãˆã¾ã™ã€‚
ç‰¹å¾´æŠ½å‡ºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã®æ´»ç”¨ã«ã‚ˆã£ã¦ã€æ¨è«–æ™‚é–“ã¯ã•ã‚‰ã«2.68å€çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦28.54å€ã€å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã«æ¯”ã¹ã¦82.86å€ã‚‚é«˜é€Ÿã«ãªã‚Šã¾ã—ãŸã€‚
ã•ã‚‰ã«ãƒãƒ¼ãƒˆå¹…ã‚’32ãƒ“ãƒƒãƒˆã‹ã‚‰64ãƒ“ãƒƒãƒˆã«æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€ä¸»ã«åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚
æ¨è«–æ™‚é–“ã¯1.20å€çŸ­ç¸®ã•ã‚Œã€CPUã«æ¯”ã¹ã¦34.17å€ã€å½“åˆã®ãƒŠã‚¤ãƒ¼ãƒ–ãªå®Ÿè£…ã¨æ¯”ã¹ã‚‹ã¨99.20å€ã®é«˜é€ŸåŒ–ã¨ãªã‚Šã¾ã—ãŸã€‚
ã“ã®ã‚ˆã†ã«ã€å„ç¨®æœ€é©åŒ–ã‚’æ–½ã™ã“ã¨ã§ã€ç€å®Ÿã«é«˜é€ŸåŒ–ã§ãã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ã—ã‹ã‚‚ã€åŸºæœ¬çš„ã«ã¯ã€å„ç¨®HLSãƒ—ãƒ©ã‚°ãƒã‚’æŒ¿å…¥ã™ã‚‹ã ã‘ã‚ˆã„ã®ã§ã€éå¸¸ã«æ¥½ã§ã™ã€‚

## ç²¾åº¦

ã¤ãã«ãƒ¢ãƒ‡ãƒ«ã®åˆ†é¡ç²¾åº¦ã‚’ã¿ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
ã“ã“ã§ã¯ModelNet40ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[ã“ã¡ã‚‰](https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip)ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
å„ã‚µãƒ³ãƒ—ãƒ«ã¯ã€é£›è¡Œæ©Ÿã€è‡ªå‹•è»Šã€ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã€äººé–“ãªã©ã€å˜ä¸€ã®ç‰©ä½“ã‚’è¡¨ã™CADãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã€2048å€‹ã®ç‚¹ã‚’ã‚‚ã¤ç‚¹ç¾¤ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’åˆ©ç”¨ã—ã¾ã™ (`host/test_zcu104.py`)ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ã¤ã„ã¦ã¯ã€GitHubã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
```Python
def test(args: argparse.Namespace,
         model: torch.nn.Module,
         model_zcu104: torch.nn.Module,
         test_loader: torch.utils.data.DataLoader):
    print(f"Testing PointNet ...")

    # model.eval()
    model_zcu104.eval()

    # test_loss_total = 0.0
    # correct = 0
    test_loss_total_zcu104 = 0.0
    correct_zcu104 = 0

    with torch.no_grad():
        for i, batch in enumerate(test_loader):
            if i % 5 == 0:
                print(f"Testing batch {i} ...")

            data, target = batch["points"], batch["label"]

            # out = model(data)
            # pred = out.argmax(dim=1, keepdim=True)
            # loss = F.cross_entropy(out, target)
            # correct += pred.eq(target.view_as(pred)).sum().item()
            # test_loss_total += loss.item() * len(data)

            out_zcu104 = model_zcu104(data)
            pred_zcu104 = out_zcu104.argmax(dim=1, keepdim=True)
            loss_zcu104 = F.cross_entropy(out_zcu104, target)
            correct_zcu104 += pred_zcu104.eq(
                target.view_as(pred_zcu104)).sum().item()
            test_loss_total_zcu104 += loss_zcu104.item() * len(data)

    # test_loss_avg = test_loss_total / len(test_loader.dataset)
    # test_acc = correct * 1e2 / len(test_loader.dataset)
    test_loss_avg_zcu104 = test_loss_total_zcu104 / len(test_loader.dataset)
    test_acc_zcu104 = correct_zcu104 * 1e2 / len(test_loader.dataset)

    # print(f"Test result (CPU): " \
    #       f"loss: {test_loss_avg:.6f}, " \
    #       f"accuracy: {test_acc:.3f}%, " \
    #       f"correct: {correct}")
    print(f"Test result (FPGA): " \
          f"loss: {test_loss_avg_zcu104:.6f}, " \
          f"accuracy: {test_acc_zcu104:.3f}%, " \
          f"correct: {correct_zcu104}, " \
          f"total: {len(test_loader.dataset)}")
```

FPGAãƒœãƒ¼ãƒ‰ä¸Šã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
```
> cd advent_2022_point_cloud_classification/host

# ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã—ãŸ (ãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²ã‚’æ¸ˆã¾ã›ãŸ) å®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ¸ˆã¾ã›ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt2.bit

# å…¥å‡ºåŠ›ã®ãƒãƒ¼ãƒˆå¹…ã‚’64ãƒ“ãƒƒãƒˆã«åºƒã’ãŸå®Ÿè£… (å‹•ä½œå‘¨æ³¢æ•°150MHz)
> sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt3.bit
```

å‡ºåŠ›çµæœã®ä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚
```
> sudo XILINX_XRT=/usr ./test_zcu104.sh ../vivado/bitstream/pointnet_opt1.bit
Testing batch 0 ....
Testing batch 5 ...
...
Testing batch 2445 ...
Testing batch 2450 ...
Testing batch 2455 ...
Testing batch 2460 ...
Testing batch 2465 ...
Test result (FPGA): loss: 0.375841, accuracy: 89.506%, correct: 2209, total: 2468
```

å„å®Ÿè£…ã«å¯¾ã™ã‚‹ç²¾åº¦ã‚’ã¾ã¨ã‚ã¾ã™ã€‚
å…¨éƒ¨ã§2,468å€‹ã®ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚
ãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…ã«é–¢ã—ã¦ã¯ã€æ™‚é–“ãŒæ›ã‹ã‚Šã™ãã‚‹ã®ã§çœç•¥ã—ã¦ã„ã¾ã™ã€‚

| å®Ÿè£… | æ­£è§£æ•° | ç²¾åº¦ |
| :-- | :-- | :-- |
| CPUç‰ˆ | 2209 | 89.506% |
| ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz) | 2209 | 89.506% |
| ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz) | 2209 | 89.506% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz) | 2209 | 89.506% |

ã„ãšã‚Œã®IPã‚³ã‚¢ã‚‚ã€CPUä¸Šã§å‹•ã‹ã—ãŸå ´åˆã¨å…¨ãåŒã˜ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
`float`ã®ä»£ã‚ã‚Šã«å›ºå®šå°æ•°ç‚¹æ•°`ap_fixed`ã‚’ä½¿ã£ã¦ã„ã¾ã™ãŒã€ã„ã¾ã®ã¨ã“ã‚ã¯ç²¾åº¦ä½ä¸‹ã¯ã¿ã‚‰ã‚Œã¾ã›ã‚“ã€‚

## ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»

å„ç¨®IPã‚³ã‚¢ã®ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ã€LUT (ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«)ã€FF (ãƒ•ãƒªãƒƒãƒ—ãƒ•ãƒ­ãƒƒãƒ—)ã€BRAM (BlockRAM)ã€URAM (UltraRAM)ã€DSP (Digital Signal Processor)ã®5ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚

[<img src="point-cloud-classification-images/pointnet-opt3-vivado4.png" width="80%" />](point-cloud-classification-images/pointnet-opt3-vivado4.png)

ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’è¡¨ã«ã¾ã¨ã‚ã¾ã™ã€‚

| å®Ÿè£… | LUT | FF | BRAM (36Kb) | URAM | DSP |
| :-- | :-- | :-- | :-- | :-- | :-- |
| åˆè¨ˆ | 230,400 | 460,800 | 312 | 96 | 1,728 |
| ãƒŠã‚¤ãƒ¼ãƒ– (100MHz) | 22,378 (9.71%) | 11,045 (2.40%) | 149.5 (47.92%) | 2 (2.08%) | 48 (2.78%) |
| ãƒŠã‚¤ãƒ¼ãƒ– (150MHz) | 22,140 (9.61%) | 12,428 (2.70%) | 161.5 (51.76%) | 2 (2.08%) | 48 (2.78%) |
| ãƒŠã‚¤ãƒ¼ãƒ– (200MHz) | 21,344 (9.26%) | 13,616 (2.95%) | 149.5 (47.92%) | 2 (2.08%) | 48 (2.78%) |
| ãƒŠã‚¤ãƒ¼ãƒ– (250MHz) | 20,663 (8.97%) | 14,713 (3.19%) | 149.5 (47.92%) | 2 (2.08%) | 20 (1.16%) |
| ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ (150MHz) | 58,223 (25.27%) | 42,755 (9.28%) | 287.5 (92.15%) | 0 (0.00%) | 768 (44.44%) |
| ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ– (150MHz) | 136,408 (59.20%) | 48,940 (10.62%) | 310.5 (99.52%) | 0 (0.00%) | 808 (46.76%) |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz) | 84,263 (36.57%) | 49,660 (10.78%) | 263.5 (84.46%) | 64 (66.67%) | 808 (46.76%) |

ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã™ã‚‹ã¨ã€è¤‡æ•°ã®ç©å’Œæ¼”ç®—ã‚’ä¸¦åˆ—ã«è¡Œã†å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€DSPã®æ¶ˆè²»ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
ä¸€æ–¹ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’ç”¨ã„ã¦ã‚‚ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ãã‚Œã»ã©å¢—ãˆã¦ã„ã¾ã›ã‚“ (ãŸã ã—ã€BRAMãŒä¸è¶³ã—ã¦ã€LUTã‚’LUTRAMã¨ã—ã¦ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€LUTã®æ¶ˆè²»ã¯å¢—åŠ ã—ã¦ã„ã¾ã™)ã€‚
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã«ã‚ˆã£ã¦ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã®å¢—åŠ ã‚’æŠ‘ãˆã¤ã¤ã€å›è·¯ã®æ€§èƒ½ã‚’æ”¹å–„ã§ãã¾ã™ã€‚
ãƒãƒ¼ãƒˆå¹…ã‚’æ‹¡å¼µã—ã¦ã‚‚ã€URAMä»¥å¤–ã®ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã¯ã‚ã¾ã‚Šå¤‰ã‚ã£ã¦ã„ã¾ã›ã‚“ (BRAMãŒä¸è¶³ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã®ã§ã€ã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ã®ä¸€éƒ¨ã‚’URAMã§å®Ÿè£…ã—ã¦ã„ã¾ã™)ã€‚

ä»Šå›ã¯20ä¸‡å††ç¨‹åº¦ã™ã‚‹FPGAãƒœãƒ¼ãƒ‰ã€Xilinx ZCU104 Evaluation Kitã‚’ä½¿ã„ã¾ã—ãŸã€‚
ã“ã®ãƒœãƒ¼ãƒ‰ã®FPGAãƒãƒƒãƒ— (XCZU7EV-2FFVC1156) ã«ã¯ã€BRAMã ã‘ã§ãªãURAMã‚‚æä¾›ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æ¯”è¼ƒçš„å¤§ããªã‚ªãƒ³ãƒãƒƒãƒ—ãƒãƒƒãƒ•ã‚¡ (æ•°MBç¨‹åº¦) ã‚’ä½œæˆã§ãã¾ã™ã€‚
URAM (UltraRAM) ã¯BRAM (BlockRAM) ã«æ¯”ã¹ã¦å€‹æ•°ãŒå°‘ãªã„ã§ã™ãŒ (BRAMãŒ312å€‹ã«å¯¾ã—ã¦URAMã¯96å€‹)ã€1å€‹ã‚ãŸã‚Šã®å®¹é‡ã¯å¤§ãã„ã®ã§ã€ç²—ç²’åº¦ã ã¨ã„ãˆã¾ã™ã€‚
ä½ä¾¡æ ¼ã®FPGAãƒœãƒ¼ãƒ‰ã ã¨ã€URAMãŒæä¾›ã•ã‚Œã¦ã„ãªã„ã®ã§ã€BRAMã‚’å¤§åˆ‡ã«ä½¿ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
å€‹äººçš„ã«ã¯ã€BRAMãŒä¸€ç•ªæœ€åˆã«æ¯æ¸‡ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ (FPGAã«æ…£ã‚Œã¦ã„ãªã„åˆå¿ƒè€…ãªã®ã§ã€ã†ã¾ãå®Ÿè£…ã§ãã¾ã›ã‚“)ã€‚

## å€¤ã®ãƒ“ãƒƒãƒˆå¹…å‰Šæ¸›

ã„ã¾ã¾ã§ã¯ã€å±¤ã®å…¥å‡ºåŠ›ã‚„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¾ã™ã‚‹ã®ã«ã€32ãƒ“ãƒƒãƒˆã®å›ºå®šå°æ•°ç‚¹æ•° (æ•´æ•°éƒ¨ã¨å°æ•°éƒ¨ãŒ16ãƒ“ãƒƒãƒˆãšã¤) ã‚’ä½¿ã£ã¦ã„ã¾ã—ãŸã€‚
ç²¾åº¦ã‚’ã‚ã‚‹ç¨‹åº¦ä¿ã£ãŸã¾ã¾ã€ãƒ“ãƒƒãƒˆæ•° (ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²») ã‚’æŠ‘ãˆã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã‹ã€‚
ã“ã“ã§ã¯ã€ä»¥ä¸‹ã®ãƒ“ãƒƒãƒˆæ•°ã®çµ„ã¿åˆã‚ã›ã§ã€IPã‚³ã‚¢ (å‹•ä½œå‘¨æ³¢æ•°150MHz) ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
IPã‚³ã‚¢ã¯ã€ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ€§ã‚’æ´»ç”¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚’æ–½ã—ã€ã•ã‚‰ã«ãƒãƒ¼ãƒˆå¹…ã‚’æ‹¡å¼µã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ“ãƒƒãƒˆæ•°ã¯ä½•ã¨ãªãæ±ºã‚ã¾ã—ãŸã€‚
ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ–¹ã¯ã€å±¤ã®å…¥å‡ºåŠ›ã«æ¯”ã¹ã¦å€¤åŸŸãŒç‹­ã„ã®ã§ã€ã‚ˆã‚Šãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

| åå‰ | å±¤ã®å…¥å‡ºåŠ› (`value_t`) | ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (`param_t`) |
| :-- | :-- | :-- |
| 28-28 | 28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨14 + å°æ•°éƒ¨14) | 28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨18) |
| 28-24 | 28ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨14 + å°æ•°éƒ¨14) | 24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨8 + å°æ•°éƒ¨16) |
| 24-24 | 24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12) | 24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨8 + å°æ•°éƒ¨16) |
| 24-20 | 24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12) | 20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨6 + å°æ•°éƒ¨14) |
| 24-16 | 24ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨12 + å°æ•°éƒ¨12) | 16ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨4 + å°æ•°éƒ¨12) |
| 20-20 | 20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨10) | 20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨6 + å°æ•°éƒ¨14) |
| 20-16 | 20ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨10 + å°æ•°éƒ¨10) | 16ãƒ“ãƒƒãƒˆ (æ•´æ•°éƒ¨4 + å°æ•°éƒ¨12) |

å„å®Ÿè£…ã«ãŠã‘ã‚‹ç²¾åº¦ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

| å®Ÿè£… | æ­£è§£æ•° | ç²¾åº¦ |
| :-- | :-- | :-- |
| CPUç‰ˆ | 2209 | 89.506% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz) | 2209 | 89.506% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-28) | 2206 | 89.384% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-24) | 2206 | 89.384% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-24) | 2200 | 89.141% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-20) | 550 | 22.285% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-16) | 121 | 4.903% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€20-20) | 448 | 18.152% |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€20-16) | 122 | 4.903% |

ã¾ãŸã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’ä»¥ä¸‹ã«ã¾ã¨ã‚ã¾ã™ã€‚

| å®Ÿè£… | LUT | FF | BRAM (36Kb) | URAM | DSP |
| :-- | :-- | :-- | :-- | :-- | :-- |
| åˆè¨ˆ | 230,400 | 460,800 | 312 | 96 | 1,728 |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHz) | 84,263 (36.57%) | 49,660 (10.78%) | 263.5 (84.46%) | 64 (66.67%) | 808 (46.76%) |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-28) | 74,342 (32.27%) | 47,267 (10.26%) | 261.5 (83.81%) | 64 (66.67%) | 808 (46.76%) |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€28-24) | 63,749 (27.67%) | 39,139 (8.49%) | 257 (82.37%) | 64 (66.67%) | 404 (23.38%) |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-24) | 59,970 (26.03%) | 36,240 (7.86%) | 257 (82.37%) | 64 (66.67%) | 404 (23.38%) |
| ãƒãƒ¼ãƒˆå¹…æ‹¡å¼µ (150MHzã€24-20) | 75,997 (32.98%) | 40,762 (8.85%) | 259 (83.01%) | 64 (66.67%) | 202 (11.69%) |

ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã—ã¦ã‚‚ã€æ¨è«–æ™‚é–“ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
ãƒ“ãƒƒãƒˆæ•°ã®å‰Šæ¸›ã«å¿œã˜ã¦ã€å®Ÿè£…ã‚’å°‘ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ã€‚

ä¸Šè¨˜ã®çµæœã‚’ã¿ã‚‹ã¨ã€é‡ã¿ã®ãƒ“ãƒƒãƒˆæ•°ã‚’24ãƒ“ãƒƒãƒˆã‹ã‚‰20ãƒ“ãƒƒãƒˆã«å‰Šæ¸›ã—ãŸé€”ç«¯ã«ã€åˆ†é¡ç²¾åº¦ãŒä¸€æ°—ã«ä½ä¸‹ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ (ã“ã“ã¾ã§ã®æ€¥æ¿€ãªä½ä¸‹ã«ã¯é©šãã¾ã—ãŸ)ã€‚
å±¤ã®å…¥å‡ºåŠ›ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã„ãšã‚Œã‚‚24ãƒ“ãƒƒãƒˆã«è¨­å®šã—ãŸIPã‚³ã‚¢ãŒã€æœ€ã‚‚ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒã‚ˆã„ã¨ã„ãˆã¾ã™ã€‚
ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’ã¿ã‚‹ã¨ã€ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§å›è·¯ã®è¤‡é›‘ã•ãŒå¾ã€…ã«ä¸‹ãŒã£ã¦ã‚†ãã€ãã‚Œã«ä¼´ã£ã¦LUTã‚„FFã®ä½¿ç”¨é‡ãŒæ¼¸æ¸›ã—ã¦ã„ã¾ã™ã€‚
28ãƒ“ãƒƒãƒˆã‹ã‚‰24ãƒ“ãƒƒãƒˆã«è½ã¨ã™ã¨ã€ç©å’Œæ¼”ç®—ã«å¿…è¦ãªDSPãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ãŒåŠæ¸›ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
24ãƒ“ãƒƒãƒˆã‹ã‚‰20ãƒ“ãƒƒãƒˆã«ã™ã‚‹ã¨ã€DSPã®ä½¿ç”¨é‡ã¯ã•ã‚‰ã«åŠæ¸›ã—ã¦ã„ã¾ã™ (ãã®åˆ†LUTã¨FFãŒå¢—åŠ ã—ã¦ã„ã¾ã™)ã€‚
BRAMã‚„URAMã«ã¤ã„ã¦ã¯ã€ãƒ“ãƒƒãƒˆæ•°ã‚’ã‚‚ã†å°‘ã—æ¸›ã‚‰ã•ãªã„ã¨ã€æ¶ˆè²»é‡ãŒæ¸›ã‚‰ãªã„ã‚ˆã†ã§ã™ (ã‚ªãƒ³ãƒãƒƒãƒ—ãƒ¡ãƒ¢ãƒªã®ä¸è¶³ãŒé ­ç—›ã®ç¨®ã«ãªã‚Šã¾ã™)ã€‚

# ãŠã‚ã‚Šã«

ä»Šå›ã¯ã€FPGAã‚’ç”¨ã„ã¦ã€ç‚¹ç¾¤ã®åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’é«˜é€ŸåŒ–ã—ã¾ã—ãŸã€‚
åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯ã€è»½é‡ã‹ã¤ã‚·ãƒ³ãƒ—ãƒ«ãªPointNetã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚
FPGAã®ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡ç•¥åŒ–ã—ã€ã¾ãŸè¨ˆç®—é †åºã‚’å¤‰æ›´ã—ã¾ã—ãŸã€‚
ç¶šã„ã¦ã€Xilinxç¤¾ã®é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«Vitis HLS 2022.1ã‚’ä½¿ã£ã¦ã€PointNetç”¨ã®ã‚«ã‚¹ã‚¿ãƒ IPã‚³ã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã€å±¤ã®è¨ˆç®—ã®ä¸¦åˆ—åŒ– (ãƒ«ãƒ¼ãƒ—ã®ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨é…åˆ—ã®åˆ†å‰²)ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãªã©ã‚’ä½¿ã£ã¦ã€IPã‚³ã‚¢ã®å®Ÿè£…ã‚’å°‘ã—ãšã¤æ”¹å–„ã—ã¦ã„ãã¾ã—ãŸã€‚

IPã‚³ã‚¢ã‚’ä»–ã®IPã‚³ã‚¢ã¨ç¹‹ãåˆã‚ã›ã¦ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œæˆã—ã€Xilinx Vivado 2022.1ã«ã‚ˆã‚Šè«–ç†åˆæˆãƒ»é…ç½®é…ç·šã‚’è¡Œã£ã¦ã€FPGAã«æ›¸ãè¾¼ã¿å¯èƒ½ãªãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆã—ã¾ã—ãŸã€‚
ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦é«˜é€Ÿã«æ¨è«–ã™ã‚‹ãŸã‚ã®ãƒ‰ãƒ©ã‚¤ãƒã‚’ã€Pynqãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã‚Šè¨˜è¿°ã—ã¾ã—ãŸã€‚
ModelNet40ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€Xilinx ZCU104 Evaluation Kitä¸Šã§ã€æ¨è«–æ™‚é–“ã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã€åˆ†é¡ç²¾åº¦ã®3ã¤ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸã€‚
ã¾ãŸã€è¤‡æ•°ã®ãƒœãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€å„ç¨®æœ€é©åŒ–ã«ã‚ˆã‚‹åŠ¹æœã‚’èª¿ã¹ã¾ã—ãŸã€‚
ãƒ“ãƒƒãƒˆæ•°ã‚’å‰Šæ¸›ã—ã€ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚‚è©¦ã¿ã¾ã—ãŸã€‚

é«˜ä½åˆæˆãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã“ã¨ã§ã€Verilog HDLãªã—ã§ã€C/C++ã ã‘ã§ã€é«˜åŠ¹ç‡ãªIPã‚³ã‚¢ã‚’ä½œæˆã§ãã¾ã—ãŸã€‚
ã—ã‹ã—ãã‚Œã§ã‚‚ã€PyTorchãªã©ã®æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã®ã¨æ¯”ã¹ã¦ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®è¨˜è¿°é‡ã¯ä½•å€ã‚‚å¤šããªã‚Šã¾ã—ãŸã€‚
å†…éƒ¨ã§è¡Œã‚ã‚Œã¦ã„ã‚‹å‡¦ç†ã®æµã‚Œã‚’ã‚ˆãè¦³å¯Ÿã—ã¦ã€å…¨ã¦ç†è§£ã—ãªã„ã¨ã€ãã‚Œã‚’é«˜é€ŸåŒ–ã™ã‚‹IPã‚³ã‚¢ã‚‚ä½œæˆã§ãã¾ã›ã‚“ã€‚
ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ãªã©ã€è€ƒãˆãªãã¦ã¯ãªã‚‰ãªã„äº‹æŸ„ã‚‚å¤šã„ã§ã™ã€‚
ä½œæ¥­å·¥ç¨‹ãŒå¤šãã¦å¤§å¤‰ã§ã™ãŒã€è‡ªä½œã®IPã‚³ã‚¢ãŒæ­£ã—ãå‹•ä½œã—ãŸ (ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å®Ÿè£…ã¨åŒã˜ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚ŒãŸ) ã¨ãã‚„ã€å®Ÿè£…ã‚’é«˜é€ŸåŒ–ã§ããŸã¨ãã®æ­“ã³ã¯ã€ãã®ã¶ã‚“å¤§ãã„ã¨æ€ã„ã¾ã™ã€‚
æœ‰é›£ã†ã”ã–ã„ã¾ã—ãŸã€‚

GPUã£ã¦ä¾¿åˆ©ã ãªã‚ã¨æ€ã†ã“ã¨ã—ãã‚Šã§ã™ã€‚
